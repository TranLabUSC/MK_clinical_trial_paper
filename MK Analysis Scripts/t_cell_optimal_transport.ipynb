{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efdf6fae-d039-4de7-a615-a40fba7b5b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Activated_CD4 - control\n",
      "Processing Activated_CD4 - short_term\n",
      "Processing Activated_CD4 - long_term\n",
      "Processing Effector_CD8 - control\n",
      "Processing Effector_CD8 - short_term\n",
      "Processing Effector_CD8 - long_term\n",
      "Processing Effector_Memory_CD8 - control\n",
      "Processing Effector_Memory_CD8 - short_term\n",
      "Processing Effector_Memory_CD8 - long_term\n",
      "Processing Exhausted_T - control\n",
      "Processing Exhausted_T - short_term\n",
      "Processing Exhausted_T - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Gamma_Delta_T - control\n",
      "Processing Gamma_Delta_T - short_term\n",
      "Processing Gamma_Delta_T - long_term\n",
      "Processing Active_CD4 - control\n",
      "Processing Active_CD4 - short_term\n",
      "Processing Active_CD4 - long_term\n",
      "Processing Naive_CD4 - control\n",
      "Processing Naive_CD4 - short_term\n",
      "Processing Naive_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD4 - control\n",
      "Processing Memory_CD4 - short_term\n",
      "Processing Memory_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD8 - control\n",
      "Processing Memory_CD8 - short_term\n",
      "Processing Memory_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Anergic_CD8 - control\n",
      "Processing Anergic_CD8 - short_term\n",
      "Processing Anergic_CD8 - long_term\n",
      "Processing Naive_CD8 - control\n",
      "Processing Naive_CD8 - short_term\n",
      "Processing Naive_CD8 - long_term\n",
      "Processing Hyperactivated_CD8 - control\n",
      "Processing Hyperactivated_CD8 - short_term\n",
      "Processing Hyperactivated_CD8 - long_term\n",
      "Processing Proliferating_Effector - control\n",
      "Processing Proliferating_Effector - short_term\n",
      "Processing Proliferating_Effector - long_term\n",
      "Processing CD8 - control\n",
      "Processing CD8 - short_term\n",
      "Processing CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization generation completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ot\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration Parameters\n",
    "# ---------------------------\n",
    "\n",
    "# Base directory where the CSV files are stored\n",
    "base_input_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/new/temp\"\n",
    "\n",
    "# Base directory to save the plots\n",
    "base_output_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/new/temp\"\n",
    "\n",
    "# Subpopulations (should match the names used in R)\n",
    "subpopulations = [\n",
    "    \"Activated_CD4\",\n",
    "    \"Effector_CD8\",\n",
    "    \"Effector_Memory_CD8\",\n",
    "    \"Exhausted_T\",\n",
    "    \"Gamma_Delta_T\",\n",
    "    \"Active_CD4\",\n",
    "    \"Naive_CD4\",\n",
    "    \"Memory_CD4\",\n",
    "    \"Memory_CD8\",\n",
    "    \"Anergic_CD8\",\n",
    "    \"Naive_CD8\",\n",
    "    \"Hyperactivated_CD8\",\n",
    "    \"Proliferating_Effector\",\n",
    "    \"CD8\"\n",
    "]\n",
    "\n",
    "# Cohorts\n",
    "cohorts = [\"control\", \"short_term\", \"long_term\"]\n",
    "\n",
    "# Ordered timepoints\n",
    "all_timepoints = [\"Pre\", \"C1\", \"C2\", \"C4\", \"C6\", \"C9\", \"C18\", \"C36\"]\n",
    "\n",
    "# Color mapping for cohorts\n",
    "cohort_colors = {\n",
    "    'control': 'yellow',\n",
    "    'short_term': 'blue',\n",
    "    'long_term': 'red'\n",
    "}\n",
    "\n",
    "# Desired arrow length for unit vectors\n",
    "arrow_length = 0.5  # Adjust as needed\n",
    "\n",
    "# ---------------------------\n",
    "# Visualization Function\n",
    "# ---------------------------\n",
    "\n",
    "def optimal_transport_visualization(subpop_name, cohort_name):\n",
    "    input_folder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "    output_folder = os.path.join(base_output_dir, subpop_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Get list of available timepoint folders\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    timepoint_folders = sorted([f for f in os.listdir(input_folder) if f.startswith(\"Timepoint_\")])\n",
    "    available_timepoints = [tp.split(\"_\")[1] for tp in timepoint_folders]\n",
    "    \n",
    "    # Ensure timepoints are in the correct order\n",
    "    cohort_timepoints = [tp for tp in all_timepoints if tp in available_timepoints]\n",
    "    \n",
    "    # Check if there are any timepoints to process\n",
    "    if not cohort_timepoints:\n",
    "        print(f\"No timepoints available for {subpop_name} in {cohort_name} cohort.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize lists to store axes limits\n",
    "    all_x_coords = []\n",
    "    all_y_coords = []\n",
    "    \n",
    "    # First pass to collect coordinates for consistent axis limits\n",
    "    for tp in cohort_timepoints:\n",
    "        source_folder = os.path.join(input_folder, f\"Timepoint_{tp}\")\n",
    "        source_cells_file = os.path.join(source_folder, 'source_cells.csv')\n",
    "        gray_cells_file = os.path.join(source_folder, 'gray_cells.csv')\n",
    "        \n",
    "        # Check if source files exist\n",
    "        if not (os.path.exists(source_cells_file) and os.path.exists(gray_cells_file)):\n",
    "            print(f\"Missing files for timepoint {tp} in {cohort_name} cohort. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        source_cells = pd.read_csv(source_cells_file)\n",
    "        gray_cells = pd.read_csv(gray_cells_file)\n",
    "        \n",
    "        if source_cells.empty:\n",
    "            print(f\"No source cells for timepoint {tp} in {cohort_name} cohort.\")\n",
    "            continue\n",
    "        \n",
    "        # Collect coordinates (flipped)\n",
    "        all_x_coords.extend(gray_cells['UMAP_2'])\n",
    "        all_x_coords.extend(source_cells['UMAP_2'])\n",
    "        all_y_coords.extend(gray_cells['UMAP_1'])\n",
    "        all_y_coords.extend(source_cells['UMAP_1'])\n",
    "    \n",
    "    # Check if any coordinates were collected\n",
    "    if not all_x_coords or not all_y_coords:\n",
    "        print(f\"No coordinates found for {subpop_name} in {cohort_name} cohort.\")\n",
    "        return\n",
    "    \n",
    "    # Determine consistent axis limits\n",
    "    x_min, x_max = min(all_x_coords), max(all_x_coords)\n",
    "    y_min, y_max = min(all_y_coords), max(all_y_coords)\n",
    "    \n",
    "    # Start a PDF to save all plots in one file\n",
    "    output_file = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots.pdf\")\n",
    "    with PdfPages(output_file) as pdf:\n",
    "        # Create a figure with subplots arranged in one row\n",
    "        num_timepoints = len(cohort_timepoints)\n",
    "        fig, axes = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        \n",
    "        # Ensure axes is iterable\n",
    "        if num_timepoints == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for idx, ax in enumerate(axes):\n",
    "            source_tp = cohort_timepoints[idx]\n",
    "            source_folder = os.path.join(input_folder, f\"Timepoint_{source_tp}\")\n",
    "            \n",
    "            source_cells_file = os.path.join(source_folder, 'source_cells.csv')\n",
    "            gray_cells_file = os.path.join(source_folder, 'gray_cells.csv')\n",
    "            \n",
    "            # Check if source files exist\n",
    "            if not (os.path.exists(source_cells_file) and os.path.exists(gray_cells_file)):\n",
    "                print(f\"Missing files for timepoint {source_tp}. Skipping subplot.\")\n",
    "                ax.axis('off')\n",
    "                continue  # Skip if any source file is missing\n",
    "            \n",
    "            source_cells = pd.read_csv(source_cells_file)\n",
    "            gray_cells = pd.read_csv(gray_cells_file)\n",
    "            \n",
    "            if source_cells.empty:\n",
    "                print(f\"No source cells for timepoint {source_tp}. Skipping subplot.\")\n",
    "                ax.axis('off')\n",
    "                continue\n",
    "            \n",
    "            # Flip UMAP coordinates by swapping UMAP_1 and UMAP_2\n",
    "            gray_x = gray_cells['UMAP_2']\n",
    "            gray_y = gray_cells['UMAP_1']\n",
    "            source_x = source_cells['UMAP_2']\n",
    "            source_y = source_cells['UMAP_1']\n",
    "            \n",
    "            # Plot gray background cells\n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5, label='Background')\n",
    "            # Plot source cells\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1, label='Highlighted')\n",
    "            \n",
    "            # For all but the last timepoint, compute optimal transport and plot arrows\n",
    "            if idx < len(cohort_timepoints) - 1:\n",
    "                target_tp = cohort_timepoints[idx + 1]\n",
    "                target_folder = os.path.join(input_folder, f\"Timepoint_{target_tp}\")\n",
    "                \n",
    "                target_cells_file = os.path.join(target_folder, 'source_cells.csv')  # Target highlighted cells\n",
    "                \n",
    "                # Check if target_cells_file exists\n",
    "                if not os.path.exists(target_cells_file):\n",
    "                    print(f\"Missing target cells for timepoint {target_tp}. No arrows will be plotted for {source_tp}.\")\n",
    "                    ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "                    continue  # Skip if target file is missing\n",
    "                \n",
    "                target_cells = pd.read_csv(target_cells_file)\n",
    "                \n",
    "                if target_cells.empty:\n",
    "                    print(f\"No target cells for timepoint {target_tp}. No arrows will be plotted for {source_tp}.\")\n",
    "                    ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "                    continue\n",
    "                \n",
    "                # Extract UMAP coordinates (flipped)\n",
    "                source_coords = source_cells[['UMAP_2', 'UMAP_1']].values  # [x, y]\n",
    "                target_coords = target_cells[['UMAP_2', 'UMAP_1']].values  # [x, y]\n",
    "                \n",
    "                # Compute cost matrix (Euclidean distance)\n",
    "                cost_matrix = ot.dist(source_coords, target_coords, metric='euclidean')\n",
    "                \n",
    "                # Uniform weights for source and target\n",
    "                a = np.ones((source_coords.shape[0],)) / source_coords.shape[0]\n",
    "                b = np.ones((target_coords.shape[0],)) / target_coords.shape[0]\n",
    "                \n",
    "                # Compute optimal transport plan\n",
    "                transport_plan = ot.emd(a, b, cost_matrix)\n",
    "                \n",
    "                # Assign each source cell to the target cell with the highest transport probability\n",
    "                target_indices = np.argmax(transport_plan, axis=1)\n",
    "                \n",
    "                # Compute displacement vectors from source to assigned target\n",
    "                displacement_vectors = target_coords[target_indices] - source_coords\n",
    "                \n",
    "                # Normalize vectors to unit length\n",
    "                norms = np.linalg.norm(displacement_vectors, axis=1, keepdims=True)\n",
    "                norms[norms == 0] = 1  # Avoid division by zero\n",
    "                unit_vectors = displacement_vectors / norms\n",
    "                \n",
    "                # Scale unit vectors to desired length\n",
    "                scaled_vectors = unit_vectors * arrow_length\n",
    "                \n",
    "                # Draw unit length arrows\n",
    "                for j in range(len(source_coords)):\n",
    "                    ax.arrow(source_coords[j, 0], source_coords[j, 1],\n",
    "                             scaled_vectors[j, 0], scaled_vectors[j, 1],\n",
    "                             color='black', alpha=0.7, head_width=0.05, head_length=0.05, \n",
    "                             length_includes_head=True, linewidth=0.5)\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min-1, x_max+1)\n",
    "            ax.set_ylim(y_min-1, y_max+1)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        # Add a global title\n",
    "        fig.suptitle(f\"{subpop_name} - {cohort_name}\", fontsize=16)\n",
    "        \n",
    "        # Adjust layout to accommodate the global title\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        \n",
    "        # Save the figure to PDF\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution Loop\n",
    "# ---------------------------\n",
    "\n",
    "# Loop over subpopulations and cohorts\n",
    "for subpop_name in subpopulations:\n",
    "    for cohort_name in cohorts:\n",
    "        print(f\"Processing {subpop_name} - {cohort_name}\")\n",
    "        input_subfolder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "        if not os.path.exists(input_subfolder):\n",
    "            print(f\"Input subfolder does not exist: {input_subfolder}. Skipping.\")\n",
    "            continue  # Skip if input subfolder does not exist\n",
    "        optimal_transport_visualization(subpop_name, cohort_name)\n",
    "\n",
    "print(\"Visualization generation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7f7f0ca-f51c-4591-8b6c-9d5a4ac32927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Activated_CD4 - control\n",
      "Processing Activated_CD4 - short_term\n",
      "Processing Activated_CD4 - long_term\n",
      "Processing Effector_CD8 - control\n",
      "Processing Effector_CD8 - short_term\n",
      "Processing Effector_CD8 - long_term\n",
      "Processing Effector_Memory_CD8 - control\n",
      "Processing Effector_Memory_CD8 - short_term\n",
      "Processing Effector_Memory_CD8 - long_term\n",
      "Processing Exhausted_T - control\n",
      "Processing Exhausted_T - short_term\n",
      "Processing Exhausted_T - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Gamma_Delta_T - control\n",
      "Processing Gamma_Delta_T - short_term\n",
      "Processing Gamma_Delta_T - long_term\n",
      "Processing Active_CD4 - control\n",
      "Processing Active_CD4 - short_term\n",
      "Processing Active_CD4 - long_term\n",
      "Processing Naive_CD4 - control\n",
      "Processing Naive_CD4 - short_term\n",
      "Processing Naive_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD4 - control\n",
      "Processing Memory_CD4 - short_term\n",
      "Processing Memory_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD8 - control\n",
      "Processing Memory_CD8 - short_term\n",
      "Processing Memory_CD8 - long_term\n",
      "Processing Anergic_CD8 - control\n",
      "Processing Anergic_CD8 - short_term\n",
      "Processing Anergic_CD8 - long_term\n",
      "Processing Naive_CD8 - control\n",
      "Processing Naive_CD8 - short_term\n",
      "Processing Naive_CD8 - long_term\n",
      "Processing Hyperactivated_CD8 - control\n",
      "Processing Hyperactivated_CD8 - short_term\n",
      "Processing Hyperactivated_CD8 - long_term\n",
      "Processing Proliferating_Effector - control\n",
      "Processing Proliferating_Effector - short_term\n",
      "Processing Proliferating_Effector - long_term\n",
      "Processing CD8 - control\n",
      "Processing CD8 - short_term\n",
      "Processing CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization generation completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ot\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration Parameters\n",
    "# ---------------------------\n",
    "\n",
    "# Base directory where the CSV files are stored\n",
    "base_input_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/new/temp\"  # Replace with your actual path\n",
    "\n",
    "# Base directory to save the plots\n",
    "base_output_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/new/temp\"  # Replace with your desired output path\n",
    "\n",
    "# Subpopulations (should match the names used in R)\n",
    "subpopulations = [\n",
    "    \"Activated_CD4\",\n",
    "    \"Effector_CD8\",\n",
    "    \"Effector_Memory_CD8\",\n",
    "    \"Exhausted_T\",\n",
    "    \"Gamma_Delta_T\",\n",
    "    \"Active_CD4\",\n",
    "    \"Naive_CD4\",\n",
    "    \"Memory_CD4\",\n",
    "    \"Memory_CD8\",\n",
    "    \"Anergic_CD8\",\n",
    "    \"Naive_CD8\",\n",
    "    \"Hyperactivated_CD8\",\n",
    "    \"Proliferating_Effector\",\n",
    "    \"CD8\"\n",
    "]\n",
    "\n",
    "# Cohorts\n",
    "cohorts = [\"control\", \"short_term\", \"long_term\"]\n",
    "\n",
    "# Ordered timepoints\n",
    "all_timepoints = [\"Pre\", \"C1\", \"C2\", \"C4\", \"C6\", \"C9\", \"C18\", \"C36\"]\n",
    "\n",
    "# Color mapping for cohorts\n",
    "cohort_colors = {\n",
    "    'control': 'yellow',\n",
    "    'short_term': 'blue',\n",
    "    'long_term': 'red'\n",
    "}\n",
    "\n",
    "# Desired minimum and maximum arrow lengths for visualization\n",
    "min_arrow_length = 0.3  # Adjust as needed\n",
    "max_arrow_length = 2  # Adjust as needed\n",
    "\n",
    "# ---------------------------\n",
    "# Visualization Function\n",
    "# ---------------------------\n",
    "\n",
    "def optimal_transport_visualization(subpop_name, cohort_name):\n",
    "    input_folder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "    output_folder = os.path.join(base_output_dir, subpop_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Get list of available timepoint folders\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    timepoint_folders = sorted([f for f in os.listdir(input_folder) if f.startswith(\"Timepoint_\")])\n",
    "    available_timepoints = [tp.split(\"_\")[1] for tp in timepoint_folders]\n",
    "    \n",
    "    # Ensure timepoints are in the correct order\n",
    "    cohort_timepoints = [tp for tp in all_timepoints if tp in available_timepoints]\n",
    "    \n",
    "    # Check if there are any timepoints to process\n",
    "    if not cohort_timepoints:\n",
    "        print(f\"No timepoints available for {subpop_name} in {cohort_name} cohort.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize lists to store axes limits\n",
    "    all_x_coords = []\n",
    "    all_y_coords = []\n",
    "    \n",
    "    # First pass to collect coordinates for consistent axis limits\n",
    "    for tp in cohort_timepoints:\n",
    "        source_folder = os.path.join(input_folder, f\"Timepoint_{tp}\")\n",
    "        source_cells_file = os.path.join(source_folder, 'source_cells.csv')\n",
    "        gray_cells_file = os.path.join(source_folder, 'gray_cells.csv')\n",
    "        \n",
    "        # Check if source files exist\n",
    "        if not (os.path.exists(source_cells_file) and os.path.exists(gray_cells_file)):\n",
    "            print(f\"Missing files for timepoint {tp} in {cohort_name} cohort. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        source_cells = pd.read_csv(source_cells_file)\n",
    "        gray_cells = pd.read_csv(gray_cells_file)\n",
    "        \n",
    "        if source_cells.empty:\n",
    "            print(f\"No source cells for timepoint {tp} in {cohort_name} cohort.\")\n",
    "            continue\n",
    "        \n",
    "        # Collect coordinates (flipped)\n",
    "        all_x_coords.extend(gray_cells['UMAP_2'])\n",
    "        all_x_coords.extend(source_cells['UMAP_2'])\n",
    "        all_y_coords.extend(gray_cells['UMAP_1'])\n",
    "        all_y_coords.extend(source_cells['UMAP_1'])\n",
    "    \n",
    "    # Check if any coordinates were collected\n",
    "    if not all_x_coords or not all_y_coords:\n",
    "        print(f\"No coordinates found for {subpop_name} in {cohort_name} cohort.\")\n",
    "        return\n",
    "    \n",
    "    # Determine consistent axis limits\n",
    "    x_min, x_max = min(all_x_coords), max(all_x_coords)\n",
    "    y_min, y_max = min(all_y_coords), max(all_y_coords)\n",
    "    \n",
    "    # Start a PDF to save all plots in one file\n",
    "    output_file = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_differential_arrow_lengths.pdf\")\n",
    "    with PdfPages(output_file) as pdf:\n",
    "        # Create a figure with subplots arranged in one row\n",
    "        num_timepoints = len(cohort_timepoints)\n",
    "        fig, axes = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        \n",
    "        # Ensure axes is iterable\n",
    "        if num_timepoints == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for idx, ax in enumerate(axes):\n",
    "            source_tp = cohort_timepoints[idx]\n",
    "            source_folder = os.path.join(input_folder, f\"Timepoint_{source_tp}\")\n",
    "            \n",
    "            source_cells_file = os.path.join(source_folder, 'source_cells.csv')\n",
    "            gray_cells_file = os.path.join(source_folder, 'gray_cells.csv')\n",
    "            \n",
    "            # Check if source files exist\n",
    "            if not (os.path.exists(source_cells_file) and os.path.exists(gray_cells_file)):\n",
    "                print(f\"Missing files for timepoint {source_tp}. Skipping subplot.\")\n",
    "                ax.axis('off')\n",
    "                continue  # Skip if any source file is missing\n",
    "            \n",
    "            source_cells = pd.read_csv(source_cells_file)\n",
    "            gray_cells = pd.read_csv(gray_cells_file)\n",
    "            \n",
    "            if source_cells.empty:\n",
    "                print(f\"No source cells for timepoint {source_tp}. Skipping subplot.\")\n",
    "                ax.axis('off')\n",
    "                continue\n",
    "            \n",
    "            # Flip UMAP coordinates by swapping UMAP_1 and UMAP_2\n",
    "            gray_x = gray_cells['UMAP_2']\n",
    "            gray_y = gray_cells['UMAP_1']\n",
    "            source_x = source_cells['UMAP_2']\n",
    "            source_y = source_cells['UMAP_1']\n",
    "            \n",
    "            # Plot gray background cells with reduced size\n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5, label='Background')  # Reduced size\n",
    "            \n",
    "            # Plot source cells with reduced size\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1, label='Highlighted')  # Reduced size\n",
    "            \n",
    "            # For all but the last timepoint, compute optimal transport and plot arrows\n",
    "            if idx < len(cohort_timepoints) - 1:\n",
    "                target_tp = cohort_timepoints[idx + 1]\n",
    "                target_folder = os.path.join(input_folder, f\"Timepoint_{target_tp}\")\n",
    "                \n",
    "                target_cells_file = os.path.join(target_folder, 'source_cells.csv')  # Target highlighted cells\n",
    "                \n",
    "                # Check if target_cells_file exists\n",
    "                if not os.path.exists(target_cells_file):\n",
    "                    print(f\"Missing target cells for timepoint {target_tp}. No arrows will be plotted for {source_tp}.\")\n",
    "                    ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "                    continue  # Skip if target file is missing\n",
    "                \n",
    "                target_cells = pd.read_csv(target_cells_file)\n",
    "                \n",
    "                if target_cells.empty:\n",
    "                    print(f\"No target cells for timepoint {target_tp}. No arrows will be plotted for {source_tp}.\")\n",
    "                    ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "                    continue\n",
    "                \n",
    "                # Extract UMAP coordinates (flipped)\n",
    "                source_coords = source_cells[['UMAP_2', 'UMAP_1']].values  # [x, y]\n",
    "                target_coords = target_cells[['UMAP_2', 'UMAP_1']].values  # [x, y]\n",
    "                \n",
    "                # Compute cost matrix (Euclidean distance)\n",
    "                cost_matrix = ot.dist(source_coords, target_coords, metric='euclidean')\n",
    "                \n",
    "                # Uniform weights for source and target\n",
    "                a = np.ones((source_coords.shape[0],)) / source_coords.shape[0]\n",
    "                b = np.ones((target_coords.shape[0],)) / target_coords.shape[0]\n",
    "                \n",
    "                # Compute optimal transport plan with increased numItermax\n",
    "                try:\n",
    "                    transport_plan = ot.emd(a, b, cost_matrix, numItermax=100000)\n",
    "                except Exception as e:\n",
    "                    print(f\"Optimal transport computation failed for timepoint {source_tp} to {target_tp}: {e}\")\n",
    "                    ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "                    continue\n",
    "                \n",
    "                # Assign each source cell to the target cell with the highest transport probability\n",
    "                target_indices = np.argmax(transport_plan, axis=1)\n",
    "                \n",
    "                # Compute displacement vectors from source to assigned target\n",
    "                displacement_vectors = target_coords[target_indices] - source_coords\n",
    "                \n",
    "                # Compute norms (lengths) of displacement vectors\n",
    "                norms = np.linalg.norm(displacement_vectors, axis=1)\n",
    "                \n",
    "                # Map norms to desired arrow length range [min_arrow_length, max_arrow_length]\n",
    "                min_norm = np.min(norms)\n",
    "                max_norm = np.max(norms)\n",
    "                \n",
    "                if max_norm - min_norm > 0:\n",
    "                    arrow_lengths = ((norms - min_norm) / (max_norm - min_norm)) * (max_arrow_length - min_arrow_length) + min_arrow_length\n",
    "                else:\n",
    "                    arrow_lengths = np.full_like(norms, min_arrow_length)\n",
    "                \n",
    "                # Compute unit vectors\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    unit_vectors = displacement_vectors / norms[:, np.newaxis]\n",
    "                    unit_vectors[~np.isfinite(unit_vectors)] = 0  # Handle divisions by zero\n",
    "                \n",
    "                # Compute scaled vectors\n",
    "                scaled_vectors = unit_vectors * arrow_lengths[:, np.newaxis]\n",
    "                \n",
    "                # Draw arrows with lengths representing distances\n",
    "                for j in range(len(source_coords)):\n",
    "                    ax.arrow(source_coords[j, 0], source_coords[j, 1],\n",
    "                             scaled_vectors[j, 0], scaled_vectors[j, 1],\n",
    "                             color='black', alpha=0.7, head_width=0.05, head_length=0.05, \n",
    "                             length_includes_head=True, linewidth=0.5)\n",
    "                \n",
    "            # Set titles and limits\n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        # Add a global title\n",
    "        fig.suptitle(f\"{subpop_name} - {cohort_name}\", fontsize=16)\n",
    "        \n",
    "        # Adjust layout to accommodate the global title\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        \n",
    "        # Save the figure to PDF\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution Loop\n",
    "# ---------------------------\n",
    "\n",
    "# Loop over subpopulations and cohorts\n",
    "for subpop_name in subpopulations:\n",
    "    for cohort_name in cohorts:\n",
    "        print(f\"Processing {subpop_name} - {cohort_name}\")\n",
    "        input_subfolder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "        if not os.path.exists(input_subfolder):\n",
    "            print(f\"Input subfolder does not exist: {input_subfolder}. Skipping.\")\n",
    "            continue  # Skip if input subfolder does not exist\n",
    "        optimal_transport_visualization(subpop_name, cohort_name)\n",
    "\n",
    "print(\"Visualization generation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1156e99-97fb-40b0-909d-c8bfeabcb16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Activated_CD4 - control\n",
      "Minimum number of source cells across timepoints: 143\n",
      "Minimum number of gray cells across timepoints: 2165\n",
      "Processing Activated_CD4 - short_term\n",
      "Minimum number of source cells across timepoints: 445\n",
      "Minimum number of gray cells across timepoints: 1315\n",
      "Processing Activated_CD4 - long_term\n",
      "Minimum number of source cells across timepoints: 206\n",
      "Minimum number of gray cells across timepoints: 1474\n",
      "Processing Effector_CD8 - control\n",
      "Minimum number of source cells across timepoints: 449\n",
      "Minimum number of gray cells across timepoints: 1939\n",
      "Processing Effector_CD8 - short_term\n",
      "Minimum number of source cells across timepoints: 285\n",
      "Minimum number of gray cells across timepoints: 1479\n",
      "Processing Effector_CD8 - long_term\n",
      "Minimum number of source cells across timepoints: 571\n",
      "Minimum number of gray cells across timepoints: 1428\n",
      "Processing Effector_Memory_CD8 - control\n",
      "Minimum number of source cells across timepoints: 339\n",
      "Minimum number of gray cells across timepoints: 2049\n",
      "Processing Effector_Memory_CD8 - short_term\n",
      "Minimum number of source cells across timepoints: 214\n",
      "Minimum number of gray cells across timepoints: 1541\n",
      "Processing Effector_Memory_CD8 - long_term\n",
      "Minimum number of source cells across timepoints: 432\n",
      "Minimum number of gray cells across timepoints: 1519\n",
      "Processing Exhausted_T - control\n",
      "Minimum number of source cells across timepoints: 707\n",
      "Minimum number of gray cells across timepoints: 1681\n",
      "Processing Exhausted_T - short_term\n",
      "Minimum number of source cells across timepoints: 429\n",
      "Minimum number of gray cells across timepoints: 1411\n",
      "Processing Exhausted_T - long_term\n",
      "Minimum number of source cells across timepoints: 665\n",
      "Minimum number of gray cells across timepoints: 1312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Gamma_Delta_T - control\n",
      "Minimum number of source cells across timepoints: 248\n",
      "Minimum number of gray cells across timepoints: 2140\n",
      "Processing Gamma_Delta_T - short_term\n",
      "Minimum number of source cells across timepoints: 102\n",
      "Minimum number of gray cells across timepoints: 1639\n",
      "Processing Gamma_Delta_T - long_term\n",
      "Minimum number of source cells across timepoints: 438\n",
      "Minimum number of gray cells across timepoints: 1561\n",
      "Processing Active_CD4 - control\n",
      "Minimum number of source cells across timepoints: 327\n",
      "Minimum number of gray cells across timepoints: 2061\n",
      "Processing Active_CD4 - short_term\n",
      "Minimum number of source cells across timepoints: 310\n",
      "Minimum number of gray cells across timepoints: 1530\n",
      "Processing Active_CD4 - long_term\n",
      "Minimum number of source cells across timepoints: 357\n",
      "Minimum number of gray cells across timepoints: 1591\n",
      "Processing Naive_CD4 - control\n",
      "Minimum number of source cells across timepoints: 454\n",
      "Minimum number of gray cells across timepoints: 1934\n",
      "Processing Naive_CD4 - short_term\n",
      "Minimum number of source cells across timepoints: 246\n",
      "Minimum number of gray cells across timepoints: 1488\n",
      "Processing Naive_CD4 - long_term\n",
      "Minimum number of source cells across timepoints: 482\n",
      "Minimum number of gray cells across timepoints: 1451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD4 - control\n",
      "Minimum number of source cells across timepoints: 7\n",
      "Minimum number of gray cells across timepoints: 2377\n",
      "Processing Memory_CD4 - short_term\n",
      "Minimum number of source cells across timepoints: 10\n",
      "Minimum number of gray cells across timepoints: 1760\n",
      "Processing Memory_CD4 - long_term\n",
      "Minimum number of source cells across timepoints: 171\n",
      "Minimum number of gray cells across timepoints: 1768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD8 - control\n",
      "Minimum number of source cells across timepoints: 404\n",
      "Minimum number of gray cells across timepoints: 1982\n",
      "Processing Memory_CD8 - short_term\n",
      "Minimum number of source cells across timepoints: 308\n",
      "Minimum number of gray cells across timepoints: 1370\n",
      "Processing Memory_CD8 - long_term\n",
      "Minimum number of source cells across timepoints: 536\n",
      "Minimum number of gray cells across timepoints: 1463\n",
      "Processing Anergic_CD8 - control\n",
      "Minimum number of source cells across timepoints: 300\n",
      "Minimum number of gray cells across timepoints: 2076\n",
      "Processing Anergic_CD8 - short_term\n",
      "Minimum number of source cells across timepoints: 224\n",
      "Minimum number of gray cells across timepoints: 1475\n",
      "Processing Anergic_CD8 - long_term\n",
      "Minimum number of source cells across timepoints: 504\n",
      "Minimum number of gray cells across timepoints: 1487\n",
      "Processing Naive_CD8 - control\n",
      "Minimum number of source cells across timepoints: 161\n",
      "Minimum number of gray cells across timepoints: 2227\n",
      "Processing Naive_CD8 - short_term\n",
      "Minimum number of source cells across timepoints: 110\n",
      "Minimum number of gray cells across timepoints: 1724\n",
      "Processing Naive_CD8 - long_term\n",
      "Minimum number of source cells across timepoints: 129\n",
      "Minimum number of gray cells across timepoints: 1870\n",
      "Processing Hyperactivated_CD8 - control\n",
      "Minimum number of source cells across timepoints: 184\n",
      "Minimum number of gray cells across timepoints: 2220\n",
      "Processing Hyperactivated_CD8 - short_term\n",
      "Minimum number of source cells across timepoints: 56\n",
      "Minimum number of gray cells across timepoints: 1706\n",
      "Processing Hyperactivated_CD8 - long_term\n",
      "Minimum number of source cells across timepoints: 60\n",
      "Minimum number of gray cells across timepoints: 1942\n",
      "Processing Proliferating_Effector - control\n",
      "Minimum number of source cells across timepoints: 269\n",
      "Minimum number of gray cells across timepoints: 2119\n",
      "Processing Proliferating_Effector - short_term\n",
      "Minimum number of source cells across timepoints: 124\n",
      "Minimum number of gray cells across timepoints: 1703\n",
      "Processing Proliferating_Effector - long_term\n",
      "Minimum number of source cells across timepoints: 460\n",
      "Minimum number of gray cells across timepoints: 1539\n",
      "Processing CD8 - control\n",
      "Minimum number of source cells across timepoints: 532\n",
      "Minimum number of gray cells across timepoints: 1856\n",
      "Processing CD8 - short_term\n",
      "Minimum number of source cells across timepoints: 414\n",
      "Minimum number of gray cells across timepoints: 1426\n",
      "Processing CD8 - long_term\n",
      "Minimum number of source cells across timepoints: 632\n",
      "Minimum number of gray cells across timepoints: 1329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization generation completed.\n"
     ]
    }
   ],
   "source": [
    "# equal number of cells at each timepoint\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ot\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration Parameters\n",
    "# ---------------------------\n",
    "\n",
    "# Base directory where the CSV files are stored\n",
    "base_input_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/new/temp\"  # Replace with your actual path\n",
    "\n",
    "# Base directory to save the plots\n",
    "base_output_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/new/temp\"  # Replace with your desired output path\n",
    "\n",
    "# Subpopulations (should match the names used in R)\n",
    "subpopulations = [\n",
    "    \"Activated_CD4\",\n",
    "    \"Effector_CD8\",\n",
    "    \"Effector_Memory_CD8\",\n",
    "    \"Exhausted_T\",\n",
    "    \"Gamma_Delta_T\",\n",
    "    \"Active_CD4\",\n",
    "    \"Naive_CD4\",\n",
    "    \"Memory_CD4\",\n",
    "    \"Memory_CD8\",\n",
    "    \"Anergic_CD8\",\n",
    "    \"Naive_CD8\",\n",
    "    \"Hyperactivated_CD8\",\n",
    "    \"Proliferating_Effector\",\n",
    "    \"CD8\"\n",
    "]\n",
    "\n",
    "# Cohorts\n",
    "cohorts = [\"control\", \"short_term\", \"long_term\"]\n",
    "\n",
    "# Ordered timepoints\n",
    "all_timepoints = [\"Pre\", \"C1\", \"C2\", \"C4\", \"C6\", \"C9\", \"C18\", \"C36\"]\n",
    "\n",
    "# Color mapping for cohorts\n",
    "cohort_colors = {\n",
    "    'control': 'yellow',\n",
    "    'short_term': 'blue',\n",
    "    'long_term': 'red'\n",
    "}\n",
    "\n",
    "# Desired minimum and maximum arrow lengths for visualization\n",
    "min_arrow_length = 0.3  # Adjust as needed\n",
    "max_arrow_length = 2  # Adjust as needed\n",
    "\n",
    "# ---------------------------\n",
    "# Visualization Function\n",
    "# ---------------------------\n",
    "\n",
    "def optimal_transport_visualization(subpop_name, cohort_name):\n",
    "    input_folder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "    output_folder = os.path.join(base_output_dir, subpop_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Get list of available timepoint folders\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    timepoint_folders = sorted([f for f in os.listdir(input_folder) if f.startswith(\"Timepoint_\")])\n",
    "    available_timepoints = [tp.split(\"_\")[1] for tp in timepoint_folders]\n",
    "    \n",
    "    # Ensure timepoints are in the correct order\n",
    "    cohort_timepoints = [tp for tp in all_timepoints if tp in available_timepoints]\n",
    "    \n",
    "    # Check if there are any timepoints to process\n",
    "    if not cohort_timepoints:\n",
    "        print(f\"No timepoints available for {subpop_name} in {cohort_name} cohort.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize lists to store axes limits\n",
    "    all_x_coords = []\n",
    "    all_y_coords = []\n",
    "    \n",
    "    # Initialize a dictionary to store cell counts for each timepoint\n",
    "    cell_counts = {'source': {}, 'gray': {}}\n",
    "    \n",
    "    # First pass to collect coordinates for consistent axis limits and cell counts\n",
    "    for tp in cohort_timepoints:\n",
    "        source_folder = os.path.join(input_folder, f\"Timepoint_{tp}\")\n",
    "        source_cells_file = os.path.join(source_folder, 'source_cells.csv')\n",
    "        gray_cells_file = os.path.join(source_folder, 'gray_cells.csv')\n",
    "        \n",
    "        # Check if source files exist\n",
    "        if not (os.path.exists(source_cells_file) and os.path.exists(gray_cells_file)):\n",
    "            print(f\"Missing files for timepoint {tp} in {cohort_name} cohort. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        source_cells = pd.read_csv(source_cells_file)\n",
    "        gray_cells = pd.read_csv(gray_cells_file)\n",
    "        \n",
    "        if source_cells.empty:\n",
    "            print(f\"No source cells for timepoint {tp} in {cohort_name} cohort.\")\n",
    "            continue\n",
    "        \n",
    "        # Collect coordinates (flipped)\n",
    "        all_x_coords.extend(gray_cells['UMAP_2'])\n",
    "        all_x_coords.extend(source_cells['UMAP_2'])\n",
    "        all_y_coords.extend(gray_cells['UMAP_1'])\n",
    "        all_y_coords.extend(source_cells['UMAP_1'])\n",
    "        \n",
    "        # Store cell counts\n",
    "        cell_counts['source'][tp] = len(source_cells)\n",
    "        cell_counts['gray'][tp] = len(gray_cells)\n",
    "    \n",
    "    # Determine the minimum number of source and gray cells across all timepoints\n",
    "    if cell_counts['source']:\n",
    "        min_source_cells = min(cell_counts['source'].values())\n",
    "    else:\n",
    "        min_source_cells = 0\n",
    "    \n",
    "    if cell_counts['gray']:\n",
    "        min_gray_cells = min(cell_counts['gray'].values())\n",
    "    else:\n",
    "        min_gray_cells = 0\n",
    "    \n",
    "    # Handle cases where no cells are found\n",
    "    if min_source_cells == 0 or min_gray_cells == 0:\n",
    "        print(f\"Insufficient cells for uniform sampling in {subpop_name} - {cohort_name}. Skipping visualization.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Minimum number of source cells across timepoints: {min_source_cells}\")\n",
    "    print(f\"Minimum number of gray cells across timepoints: {min_gray_cells}\")\n",
    "    \n",
    "    # Determine consistent axis limits\n",
    "    x_min, x_max = min(all_x_coords), max(all_x_coords)\n",
    "    y_min, y_max = min(all_y_coords), max(all_y_coords)\n",
    "    \n",
    "    # Start a PDF to save all plots in one file\n",
    "    output_file = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_differential_arrow_lengths_equal_cells.pdf\")\n",
    "    with PdfPages(output_file) as pdf:\n",
    "        # Create a figure with subplots arranged in one row\n",
    "        num_timepoints = len(cohort_timepoints)\n",
    "        fig, axes = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        \n",
    "        # Ensure axes is iterable\n",
    "        if num_timepoints == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for idx, ax in enumerate(axes):\n",
    "            source_tp = cohort_timepoints[idx]\n",
    "            source_folder = os.path.join(input_folder, f\"Timepoint_{source_tp}\")\n",
    "            \n",
    "            source_cells_file = os.path.join(source_folder, 'source_cells.csv')\n",
    "            gray_cells_file = os.path.join(source_folder, 'gray_cells.csv')\n",
    "            \n",
    "            # Check if source files exist\n",
    "            if not (os.path.exists(source_cells_file) and os.path.exists(gray_cells_file)):\n",
    "                print(f\"Missing files for timepoint {source_tp}. Skipping subplot.\")\n",
    "                ax.axis('off')\n",
    "                continue  # Skip if any source file is missing\n",
    "            \n",
    "            source_cells = pd.read_csv(source_cells_file)\n",
    "            gray_cells = pd.read_csv(gray_cells_file)\n",
    "            \n",
    "            if source_cells.empty:\n",
    "                print(f\"No source cells for timepoint {source_tp}. Skipping subplot.\")\n",
    "                ax.axis('off')\n",
    "                continue\n",
    "            \n",
    "            # Sample source_cells and gray_cells to have uniform counts\n",
    "            if len(source_cells) >= min_source_cells:\n",
    "                sampled_source_cells = source_cells.sample(n=min_source_cells, random_state=42)\n",
    "            else:\n",
    "                print(f\"Not enough source cells in {source_tp} for uniform sampling. Skipping subplot.\")\n",
    "                ax.axis('off')\n",
    "                continue\n",
    "            \n",
    "            if len(gray_cells) >= min_gray_cells:\n",
    "                sampled_gray_cells = gray_cells.sample(n=min_gray_cells, random_state=42)\n",
    "            else:\n",
    "                print(f\"Not enough gray cells in {source_tp} for uniform sampling. Skipping subplot.\")\n",
    "                ax.axis('off')\n",
    "                continue\n",
    "            \n",
    "            # Flip UMAP coordinates by swapping UMAP_1 and UMAP_2\n",
    "            gray_x = sampled_gray_cells['UMAP_2']\n",
    "            gray_y = sampled_gray_cells['UMAP_1']\n",
    "            source_x = sampled_source_cells['UMAP_2']\n",
    "            source_y = sampled_source_cells['UMAP_1']\n",
    "            \n",
    "            # Plot gray background cells with reduced size\n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5, label='Background')  # Reduced size\n",
    "            \n",
    "            # Plot source cells with reduced size\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1, label='Highlighted')  # Reduced size\n",
    "            \n",
    "            # For all but the last timepoint, compute optimal transport and plot arrows\n",
    "            if idx < len(cohort_timepoints) - 1:\n",
    "                target_tp = cohort_timepoints[idx + 1]\n",
    "                target_folder = os.path.join(input_folder, f\"Timepoint_{target_tp}\")\n",
    "                \n",
    "                target_cells_file = os.path.join(target_folder, 'source_cells.csv')  # Target highlighted cells\n",
    "                \n",
    "                # Check if target_cells_file exists\n",
    "                if not os.path.exists(target_cells_file):\n",
    "                    print(f\"Missing target cells for timepoint {target_tp}. No arrows will be plotted for {source_tp}.\")\n",
    "                    ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "                    continue  # Skip if target file is missing\n",
    "                \n",
    "                target_cells = pd.read_csv(target_cells_file)\n",
    "                \n",
    "                if target_cells.empty:\n",
    "                    print(f\"No target cells for timepoint {target_tp}. No arrows will be plotted for {source_tp}.\")\n",
    "                    ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "                    continue\n",
    "                \n",
    "                # Extract UMAP coordinates (flipped)\n",
    "                source_coords = source_cells[['UMAP_2', 'UMAP_1']].values  # [x, y]\n",
    "                target_coords = target_cells[['UMAP_2', 'UMAP_1']].values  # [x, y]\n",
    "                \n",
    "                # Compute cost matrix (Euclidean distance)\n",
    "                cost_matrix = ot.dist(source_coords, target_coords, metric='euclidean')\n",
    "                \n",
    "                # Uniform weights for source and target\n",
    "                a = np.ones((source_coords.shape[0],)) / source_coords.shape[0]\n",
    "                b = np.ones((target_coords.shape[0],)) / target_coords.shape[0]\n",
    "                \n",
    "                # Compute optimal transport plan with increased numItermax\n",
    "                try:\n",
    "                    transport_plan = ot.emd(a, b, cost_matrix, numItermax=100000)\n",
    "                except Exception as e:\n",
    "                    print(f\"Optimal transport computation failed for timepoint {source_tp} to {target_tp}: {e}\")\n",
    "                    ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "                    continue\n",
    "                \n",
    "                # Assign each source cell to the target cell with the highest transport probability\n",
    "                target_indices = np.argmax(transport_plan, axis=1)\n",
    "                \n",
    "                # Compute displacement vectors from source to assigned target\n",
    "                displacement_vectors = target_coords[target_indices] - source_coords\n",
    "                \n",
    "                # Compute norms (lengths) of displacement vectors\n",
    "                norms = np.linalg.norm(displacement_vectors, axis=1)\n",
    "                \n",
    "                # Map norms to desired arrow length range [min_arrow_length, max_arrow_length]\n",
    "                min_norm = np.min(norms)\n",
    "                max_norm = np.max(norms)\n",
    "                \n",
    "                if max_norm - min_norm > 0:\n",
    "                    arrow_lengths = ((norms - min_norm) / (max_norm - min_norm)) * (max_arrow_length - min_arrow_length) + min_arrow_length\n",
    "                else:\n",
    "                    arrow_lengths = np.full_like(norms, min_arrow_length)\n",
    "                \n",
    "                # Compute unit vectors\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    unit_vectors = displacement_vectors / norms[:, np.newaxis]\n",
    "                    unit_vectors[~np.isfinite(unit_vectors)] = 0  # Handle divisions by zero\n",
    "                \n",
    "                # Compute scaled vectors\n",
    "                scaled_vectors = unit_vectors * arrow_lengths[:, np.newaxis]\n",
    "                \n",
    "                # Draw arrows with lengths representing distances\n",
    "                for j in range(len(source_coords)):\n",
    "                    ax.arrow(source_coords[j, 0], source_coords[j, 1],\n",
    "                             scaled_vectors[j, 0], scaled_vectors[j, 1],\n",
    "                             color='black', alpha=0.7, head_width=0.05, head_length=0.05, \n",
    "                             length_includes_head=True, linewidth=0.5)\n",
    "                \n",
    "            # Set titles and limits\n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        # Add a global title\n",
    "        fig.suptitle(f\"{subpop_name} - {cohort_name}\", fontsize=16)\n",
    "        \n",
    "        # Adjust layout to accommodate the global title\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        \n",
    "        # Save the figure to PDF\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution Loop\n",
    "# ---------------------------\n",
    "\n",
    "# Loop over subpopulations and cohorts\n",
    "for subpop_name in subpopulations:\n",
    "    for cohort_name in cohorts:\n",
    "        print(f\"Processing {subpop_name} - {cohort_name}\")\n",
    "        input_subfolder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "        if not os.path.exists(input_subfolder):\n",
    "            print(f\"Input subfolder does not exist: {input_subfolder}. Skipping.\")\n",
    "            continue  # Skip if input subfolder does not exist\n",
    "        optimal_transport_visualization(subpop_name, cohort_name)\n",
    "\n",
    "print(\"Visualization generation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfec787-b660-45b4-8fca-bbb0c52a38b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of cells to sample for single-cell plotting (to reduce overcrowding)\n",
    "# This can be the minimum number found across timepoints, or a fixed number.\n",
    "# Here we will determine a uniform sampling size based on the smallest dataset\n",
    "# found across timepoints, same as before, but only for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f446a51-9c5a-48a1-ae6e-cf1f9846d21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Activated_CD4 - control\n",
      "Processing Activated_CD4 - short_term\n",
      "Processing Activated_CD4 - long_term\n",
      "Processing Effector_CD8 - control\n",
      "Processing Effector_CD8 - short_term\n",
      "Processing Effector_CD8 - long_term\n",
      "Processing Effector_Memory_CD8 - control\n",
      "Processing Effector_Memory_CD8 - short_term\n",
      "Processing Effector_Memory_CD8 - long_term\n",
      "Processing Exhausted_T - control\n",
      "Processing Exhausted_T - short_term\n",
      "Processing Exhausted_T - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Gamma_Delta_T - control\n",
      "Processing Gamma_Delta_T - short_term\n",
      "Processing Gamma_Delta_T - long_term\n",
      "Processing Active_CD4 - control\n",
      "Processing Active_CD4 - short_term\n",
      "Processing Active_CD4 - long_term\n",
      "Processing Naive_CD4 - control\n",
      "Processing Naive_CD4 - short_term\n",
      "Processing Naive_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD4 - control\n",
      "Processing Memory_CD4 - short_term\n",
      "Processing Memory_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD8 - control\n",
      "Processing Memory_CD8 - short_term\n",
      "Processing Memory_CD8 - long_term\n",
      "Processing Anergic_CD8 - control\n",
      "Processing Anergic_CD8 - short_term\n",
      "Processing Anergic_CD8 - long_term\n",
      "Processing Naive_CD8 - control\n",
      "Processing Naive_CD8 - short_term\n",
      "Processing Naive_CD8 - long_term\n",
      "Processing Hyperactivated_CD8 - control\n",
      "Processing Hyperactivated_CD8 - short_term\n",
      "Processing Hyperactivated_CD8 - long_term\n",
      "Processing Proliferating_Effector - control\n",
      "Processing Proliferating_Effector - short_term\n",
      "Processing Proliferating_Effector - long_term\n",
      "Processing CD8 - control\n",
      "Processing CD8 - short_term\n",
      "Processing CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization generation completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ot\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration Parameters\n",
    "# ---------------------------\n",
    "\n",
    "# Base directory where the CSV files are stored\n",
    "base_input_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/new/temp\"  # Replace with your actual path\n",
    "\n",
    "# Base directory to save the plots\n",
    "base_output_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/new/temp\"  # Replace with your desired output path\n",
    "\n",
    "# Subpopulations (should match the names used in R)\n",
    "subpopulations = [\n",
    "    \"Activated_CD4\",\n",
    "    \"Effector_CD8\",\n",
    "    \"Effector_Memory_CD8\",\n",
    "    \"Exhausted_T\",\n",
    "    \"Gamma_Delta_T\",\n",
    "    \"Active_CD4\",\n",
    "    \"Naive_CD4\",\n",
    "    \"Memory_CD4\",\n",
    "    \"Memory_CD8\",\n",
    "    \"Anergic_CD8\",\n",
    "    \"Naive_CD8\",\n",
    "    \"Hyperactivated_CD8\",\n",
    "    \"Proliferating_Effector\",\n",
    "    \"CD8\"\n",
    "]\n",
    "\n",
    "# Cohorts\n",
    "cohorts = [\"control\", \"short_term\", \"long_term\"]\n",
    "\n",
    "# Ordered timepoints\n",
    "all_timepoints = [\"Pre\", \"C1\", \"C2\", \"C4\", \"C6\", \"C9\", \"C18\", \"C36\"]\n",
    "\n",
    "# Color mapping for cohorts\n",
    "cohort_colors = {\n",
    "    'control': 'yellow',\n",
    "    'short_term': 'blue',\n",
    "    'long_term': 'red'\n",
    "}\n",
    "\n",
    "# Desired minimum and maximum arrow lengths for visualization\n",
    "min_arrow_length = 0.3  # Adjust as needed\n",
    "max_arrow_length = 2    # Adjust as needed\n",
    "\n",
    "# Number of cells to sample for single-cell plotting (to reduce overcrowding)\n",
    "# This can be the minimum number found across timepoints, or a fixed number.\n",
    "# Here we will determine a uniform sampling size based on the smallest dataset\n",
    "# found across timepoints, same as before, but only for plotting.\n",
    "max_plot_cells = None  # will determine at runtime\n",
    "\n",
    "def optimal_transport_visualization(subpop_name, cohort_name):\n",
    "    input_folder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "    output_folder = os.path.join(base_output_dir, subpop_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    timepoint_folders = sorted([f for f in os.listdir(input_folder) if f.startswith(\"Timepoint_\")])\n",
    "    available_timepoints = [tp.split(\"_\")[1] for tp in timepoint_folders]\n",
    "    cohort_timepoints = [tp for tp in all_timepoints if tp in available_timepoints]\n",
    "    \n",
    "    if not cohort_timepoints:\n",
    "        print(f\"No timepoints available for {subpop_name} in {cohort_name} cohort.\")\n",
    "        return\n",
    "\n",
    "    # We'll store coordinates and also find global axis limits\n",
    "    all_x_coords = []\n",
    "    all_y_coords = []\n",
    "    \n",
    "    # Load all source and gray cells (full sets) for axis limits\n",
    "    # We also determine the smallest number of source and gray cells across timepoints\n",
    "    # for single-cell plotting (downsampling).\n",
    "    cell_counts_source = []\n",
    "    cell_counts_gray = []\n",
    "    \n",
    "    full_data = {}  # store full data for each timepoint\n",
    "    \n",
    "    for tp in cohort_timepoints:\n",
    "        source_folder = os.path.join(input_folder, f\"Timepoint_{tp}\")\n",
    "        source_cells_file = os.path.join(source_folder, 'source_cells.csv')\n",
    "        gray_cells_file = os.path.join(source_folder, 'gray_cells.csv')\n",
    "        \n",
    "        if not (os.path.exists(source_cells_file) and os.path.exists(gray_cells_file)):\n",
    "            print(f\"Missing files for {tp}. Skipping this timepoint.\")\n",
    "            continue\n",
    "        \n",
    "        source_cells = pd.read_csv(source_cells_file)\n",
    "        gray_cells = pd.read_csv(gray_cells_file)\n",
    "        \n",
    "        if source_cells.empty:\n",
    "            print(f\"No source cells for {tp}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Store full data\n",
    "        full_data[tp] = {\n",
    "            'source': source_cells,\n",
    "            'gray': gray_cells\n",
    "        }\n",
    "        \n",
    "        # For axis limits\n",
    "        all_x_coords.extend(gray_cells['UMAP_2'])\n",
    "        all_x_coords.extend(source_cells['UMAP_2'])\n",
    "        all_y_coords.extend(gray_cells['UMAP_1'])\n",
    "        all_y_coords.extend(source_cells['UMAP_1'])\n",
    "        \n",
    "        cell_counts_source.append(len(source_cells))\n",
    "        cell_counts_gray.append(len(gray_cells))\n",
    "    \n",
    "    if not full_data:\n",
    "        print(\"No valid timepoints with data.\")\n",
    "        return\n",
    "    \n",
    "    x_min, x_max = min(all_x_coords), max(all_x_coords)\n",
    "    y_min, y_max = min(all_y_coords), max(all_y_coords)\n",
    "    \n",
    "    # Determine number of cells to plot for single-cell arrows (downsample for plotting only)\n",
    "    # We use the minimum across all timepoints to keep it consistent.\n",
    "    min_source_cells = min(cell_counts_source) if cell_counts_source else 0\n",
    "    min_gray_cells = min(cell_counts_gray) if cell_counts_gray else 0\n",
    "    if min_source_cells == 0 or min_gray_cells == 0:\n",
    "        print(\"Insufficient cells. Skipping visualization.\")\n",
    "        return\n",
    "    \n",
    "    # We'll do OT and cluster arrow computations using the full sets,\n",
    "    # but only plot a subsample of source & gray cells (and their arrows) in the single-cell arrow PDF.\n",
    "    \n",
    "    # We'll store results for both single-cell (subsampled) plotting and cluster-level arrows (full).\n",
    "    timepoint_results = {}\n",
    "    \n",
    "    # We need to compute OT from each timepoint to the next (except the last one)\n",
    "    for i, source_tp in enumerate(cohort_timepoints):\n",
    "        source_data = full_data[source_tp]\n",
    "        source_cells = source_data['source']\n",
    "        \n",
    "        # Store initial data for plotting\n",
    "        # Downsample for plotting single-cell arrows only\n",
    "        sampled_source_cells = source_cells.sample(n=min_source_cells, random_state=42)\n",
    "        sampled_gray_cells = full_data[source_tp]['gray'].sample(n=min_gray_cells, random_state=42)\n",
    "        \n",
    "        timepoint_results[source_tp] = {\n",
    "            'sampled_source': sampled_source_cells,\n",
    "            'sampled_gray': sampled_gray_cells\n",
    "        }\n",
    "        \n",
    "        if i < len(cohort_timepoints) - 1:\n",
    "            target_tp = cohort_timepoints[i+1]\n",
    "            target_data = full_data[target_tp]\n",
    "            target_cells = target_data['source']\n",
    "            \n",
    "            if target_cells.empty:\n",
    "                # No target cells, no OT\n",
    "                continue\n",
    "            \n",
    "            # Compute OT on full sets, not sampled\n",
    "            full_source_coords = source_cells[['UMAP_2', 'UMAP_1']].values\n",
    "            full_target_coords = target_cells[['UMAP_2', 'UMAP_1']].values\n",
    "            \n",
    "            # Uniform distributions\n",
    "            a = np.ones((full_source_coords.shape[0],)) / full_source_coords.shape[0]\n",
    "            b = np.ones((full_target_coords.shape[0],)) / full_target_coords.shape[0]\n",
    "            \n",
    "            # Compute cost matrix\n",
    "            cost_matrix = ot.dist(full_source_coords, full_target_coords, metric='euclidean')\n",
    "            \n",
    "            try:\n",
    "                transport_plan = ot.emd(a, b, cost_matrix, numItermax=100000)\n",
    "            except Exception as e:\n",
    "                print(f\"OT computation failed for {source_tp} to {target_tp}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Assign each full source cell to a target cell\n",
    "            full_target_indices = np.argmax(transport_plan, axis=1)\n",
    "            displacement_vectors_full = full_target_coords[full_target_indices] - full_source_coords\n",
    "            \n",
    "            # Compute norms\n",
    "            norms_full = np.linalg.norm(displacement_vectors_full, axis=1)\n",
    "            \n",
    "            # Scale arrow lengths for cluster-level arrows\n",
    "            min_norm_full = np.min(norms_full)\n",
    "            max_norm_full = np.max(norms_full)\n",
    "            \n",
    "            if max_norm_full - min_norm_full > 0:\n",
    "                arrow_lengths_full = ((norms_full - min_norm_full) / (max_norm_full - min_norm_full)) * (max_arrow_length - min_arrow_length) + min_arrow_length\n",
    "            else:\n",
    "                arrow_lengths_full = np.full_like(norms_full, min_arrow_length)\n",
    "            \n",
    "            # For single-cell plotting, we also need displacement vectors of sampled sets\n",
    "            # Assign each sampled source cell to target cell based on the full transport plan\n",
    "            # We must find their indices in the full set\n",
    "            # Matching sampled source cells to full source:\n",
    "            sampled_source_indices = source_cells.index.get_indexer_for(sampled_source_cells.index)\n",
    "            sampled_displacements = displacement_vectors_full[sampled_source_indices, :]\n",
    "            sampled_norms = norms_full[sampled_source_indices]\n",
    "            \n",
    "            # Scale for sampled set plotting\n",
    "            min_norm_sampled = np.min(sampled_norms)\n",
    "            max_norm_sampled = np.max(sampled_norms)\n",
    "            if max_norm_sampled - min_norm_sampled > 0:\n",
    "                arrow_lengths_sampled = ((sampled_norms - min_norm_sampled) / (max_norm_sampled - min_norm_sampled)) * (max_arrow_length - min_arrow_length) + min_arrow_length\n",
    "            else:\n",
    "                arrow_lengths_sampled = np.full_like(sampled_norms, min_arrow_length)\n",
    "            \n",
    "            # Store single-cell arrow info (for plotting)\n",
    "            timepoint_results[source_tp]['single_cell_displacements'] = sampled_displacements\n",
    "            timepoint_results[source_tp]['single_cell_arrow_lengths'] = arrow_lengths_sampled\n",
    "            \n",
    "            clusters_of_interest = {1, 2, 3, 8, 10, 12, 14}\n",
    "\n",
    "            # Compute cluster-level arrows using full data (not sampled)\n",
    "            if 'seurat_clusters' in source_cells.columns:\n",
    "                # Replace 16 and 17 with 14 before computing cluster-level arrows\n",
    "                source_cells['seurat_clusters'] = source_cells['seurat_clusters'].replace({16: 14, 17: 14})\n",
    "                source_clusters_full = source_cells['seurat_clusters'].values\n",
    "                df_cluster_full = pd.DataFrame({\n",
    "                    'cluster': source_clusters_full,\n",
    "                    'sx': full_source_coords[:,0],\n",
    "                    'sy': full_source_coords[:,1],\n",
    "                    'dx': displacement_vectors_full[:,0],\n",
    "                    'dy': displacement_vectors_full[:,1],\n",
    "                    'norm': norms_full\n",
    "                })\n",
    "                \n",
    "                # Compute cluster aggregates\n",
    "                group = df_cluster_full.groupby('cluster')\n",
    "                centroids = group[['sx','sy']].mean()\n",
    "                mean_disp = group[['dx','dy']].mean()\n",
    "            \n",
    "                cluster_norms = np.sqrt(mean_disp['dx']**2 + mean_disp['dy']**2)\n",
    "                cn_min = cluster_norms.min()\n",
    "                cn_max = cluster_norms.max()\n",
    "                \n",
    "                # Ensure cluster_arrow_lengths is a Series\n",
    "                if cn_max - cn_min > 0:\n",
    "                    cluster_arrow_lengths = ((cluster_norms - cn_min) / (cn_max - cn_min)) * (max_arrow_length - min_arrow_length) + min_arrow_length\n",
    "                else:\n",
    "                    cluster_arrow_lengths = pd.Series(np.full_like(cluster_norms, min_arrow_length), index=cluster_norms.index)\n",
    "                \n",
    "                cluster_arrows = []\n",
    "                for clust in centroids.index:\n",
    "                    # Only show aggregated arrow if cluster is in clusters_of_interest\n",
    "                    if clust in clusters_of_interest:\n",
    "                        cx, cy = centroids.loc[clust, ['sx','sy']]\n",
    "                        cdx, cdy = mean_disp.loc[clust, ['dx','dy']]\n",
    "                        cnorm = np.sqrt(cdx**2 + cdy**2)\n",
    "                        if cnorm > 0:\n",
    "                            cdx = cdx / cnorm\n",
    "                            cdy = cdy / cnorm\n",
    "                        else:\n",
    "                            cdx, cdy = 0, 0\n",
    "            \n",
    "                        # Safely access cluster_arrow_lengths\n",
    "                        try:\n",
    "                            length = cluster_arrow_lengths.loc[clust]\n",
    "                        except AttributeError:\n",
    "                            # If this happens, convert cluster_arrow_lengths to Series and retry\n",
    "                            print(\"cluster_arrow_lengths is not a Series. Attempting to convert...\")\n",
    "                            if isinstance(cluster_arrow_lengths, np.ndarray):\n",
    "                                cluster_arrow_lengths = pd.Series(cluster_arrow_lengths, index=cluster_norms.index)\n",
    "                            try:\n",
    "                                length = cluster_arrow_lengths.loc[clust]\n",
    "                            except Exception as e:\n",
    "                                print(f\"Failed to access cluster_arrow_lengths for cluster {clust}: {e}\")\n",
    "                                length = min_arrow_length\n",
    "            \n",
    "                        cdx *= length\n",
    "                        cdy *= length\n",
    "                        cluster_arrows.append((cx, cy, cdx, cdy))\n",
    "                \n",
    "                timepoint_results[source_tp]['cluster_arrows'] = cluster_arrows\n",
    "            else:\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                print(f\"No 'seurat_clusters' column in {source_tp} source cells. Cannot compute cluster arrows.\")\n",
    "\n",
    "            \n",
    "        else:\n",
    "            # Last timepoint has no next timepoint\n",
    "            timepoint_results[source_tp]['single_cell_displacements'] = np.array([]) # no arrows\n",
    "            timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "            timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "            if 'seurat_clusters' in source_cells.columns:\n",
    "                # Still record that cluster info was present\n",
    "                # but no arrows since there's no next timepoint\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "            else:\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "    \n",
    "    # Now plot the original single-cell arrows PDF\n",
    "    output_file_original = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_differential_arrow_lengths_equal_cells.pdf\")\n",
    "    with PdfPages(output_file_original) as pdf_original:\n",
    "        num_timepoints = len(cohort_timepoints)\n",
    "        fig_orig, axes_orig = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        if num_timepoints == 1:\n",
    "            axes_orig = [axes_orig]\n",
    "        \n",
    "        for i, source_tp in enumerate(cohort_timepoints):\n",
    "            ax = axes_orig[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            sampled_source_cells = res['sampled_source']\n",
    "            sampled_gray_cells = res['sampled_gray']\n",
    "            \n",
    "            gray_x = sampled_gray_cells['UMAP_2'].values\n",
    "            gray_y = sampled_gray_cells['UMAP_1'].values\n",
    "            source_x = sampled_source_cells['UMAP_2'].values\n",
    "            source_y = sampled_source_cells['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "            \n",
    "            single_cell_displacements = res['single_cell_displacements']\n",
    "            single_cell_arrow_lengths = res['single_cell_arrow_lengths']\n",
    "            \n",
    "            if len(single_cell_displacements) > 0:\n",
    "                source_coords_sampled = sampled_source_cells[['UMAP_2','UMAP_1']].values\n",
    "                # Compute unit vectors for sampled\n",
    "                sampled_norms = np.linalg.norm(single_cell_displacements, axis=1)\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    unit_vectors = single_cell_displacements / sampled_norms[:, np.newaxis]\n",
    "                    unit_vectors[~np.isfinite(unit_vectors)] = 0\n",
    "                scaled_vectors = unit_vectors * single_cell_arrow_lengths[:, np.newaxis]\n",
    "                \n",
    "                for j in range(len(source_coords_sampled)):\n",
    "                    sx, sy = source_coords_sampled[j]\n",
    "                    dx, dy = scaled_vectors[j]\n",
    "                    ax.arrow(sx, sy, dx, dy,\n",
    "                             color='black', alpha=0.7, head_width=0.05, head_length=0.05,\n",
    "                             length_includes_head=True, linewidth=0.5)\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_orig.suptitle(f\"{subpop_name} - {cohort_name}\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_original.savefig(fig_orig)\n",
    "        plt.close(fig_orig)\n",
    "    \n",
    "    # Now plot aggregated cluster arrows in a new PDF\n",
    "    output_file_cluster = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_aggregated_cluster_arrows_equal_cells.pdf\")\n",
    "    with PdfPages(output_file_cluster) as pdf_cluster:\n",
    "        num_timepoints = len(cohort_timepoints)\n",
    "        fig_clust, axes_clust = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        if num_timepoints == 1:\n",
    "            axes_clust = [axes_clust]\n",
    "        \n",
    "        for i, source_tp in enumerate(cohort_timepoints):\n",
    "            ax = axes_clust[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            \n",
    "            # For cluster-level plot, we still show the same sampled source/gray points as background\n",
    "            gray_x = res['sampled_gray']['UMAP_2'].values\n",
    "            gray_y = res['sampled_gray']['UMAP_1'].values\n",
    "            source_x = res['sampled_source']['UMAP_2'].values\n",
    "            source_y = res['sampled_source']['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "            \n",
    "            cluster_arrows = res.get('cluster_arrows', [])\n",
    "            for (cx, cy, cdx, cdy) in cluster_arrows:\n",
    "                ax.arrow(cx, cy, cdx, cdy,\n",
    "                         color='black', alpha=0.7, head_width=0.1, head_length=0.1,\n",
    "                         length_includes_head=True, linewidth=1.0)\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_clust.suptitle(f\"{subpop_name} - {cohort_name} (Cluster-Level Arrows)\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_cluster.savefig(fig_clust)\n",
    "        plt.close(fig_clust)\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution Loop\n",
    "# ---------------------------\n",
    "\n",
    "for subpop_name in subpopulations:\n",
    "    for cohort_name in cohorts:\n",
    "        print(f\"Processing {subpop_name} - {cohort_name}\")\n",
    "        input_subfolder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "        if not os.path.exists(input_subfolder):\n",
    "            print(f\"Input subfolder does not exist: {input_subfolder}. Skipping.\")\n",
    "            continue\n",
    "        optimal_transport_visualization(subpop_name, cohort_name)\n",
    "\n",
    "print(\"Visualization generation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70624a45-656a-46a0-b990-d722784bd77a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff5a940-802e-4df6-99f4-277887ad3c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4970e10-c88b-4b75-8092-c20912734f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f7bbf8-f0df-4bb5-9a1b-dbab897975fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdb161c-1f2b-47b1-a427-470b29db153b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0381700-7c21-400b-a174-d50b870c548c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2af36c3d-d75c-4a0c-99ee-0385aa1041d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the following version we use PCs to calculate the distance instead of UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "625d40c5-8796-4d82-a9b0-a56751456288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Activated_CD4 - control\n",
      "Processing Activated_CD4 - short_term\n",
      "Processing Activated_CD4 - long_term\n",
      "Processing Effector_CD8 - control\n",
      "Processing Effector_CD8 - short_term\n",
      "Processing Effector_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Effector_Memory_CD8 - control\n",
      "Processing Effector_Memory_CD8 - short_term\n",
      "Processing Effector_Memory_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Exhausted_T - control\n",
      "Processing Exhausted_T - short_term\n",
      "Processing Exhausted_T - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Gamma_Delta_T - control\n",
      "Processing Gamma_Delta_T - short_term\n",
      "Processing Gamma_Delta_T - long_term\n",
      "Processing Active_CD4 - control\n",
      "Processing Active_CD4 - short_term\n",
      "Processing Active_CD4 - long_term\n",
      "Processing Naive_CD4 - control\n",
      "Processing Naive_CD4 - short_term\n",
      "Processing Naive_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD4 - control\n",
      "Processing Memory_CD4 - short_term\n",
      "Processing Memory_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD8 - control\n",
      "Processing Memory_CD8 - short_term\n",
      "Processing Memory_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Anergic_CD8 - control\n",
      "Processing Anergic_CD8 - short_term\n",
      "Processing Anergic_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Naive_CD8 - control\n",
      "Processing Naive_CD8 - short_term\n",
      "Processing Naive_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Hyperactivated_CD8 - control\n",
      "Missing files for Pre. Skipping this timepoint.\n",
      "Missing files for C1. Skipping this timepoint.\n",
      "Missing files for C2. Skipping this timepoint.\n",
      "No valid timepoints with data.\n",
      "Processing Hyperactivated_CD8 - short_term\n",
      "Missing files for Pre. Skipping this timepoint.\n",
      "Missing files for C1. Skipping this timepoint.\n",
      "Missing files for C2. Skipping this timepoint.\n",
      "Missing files for C4. Skipping this timepoint.\n",
      "No valid timepoints with data.\n",
      "Processing Hyperactivated_CD8 - long_term\n",
      "Missing files for Pre. Skipping this timepoint.\n",
      "Missing files for C1. Skipping this timepoint.\n",
      "Missing files for C2. Skipping this timepoint.\n",
      "Missing files for C4. Skipping this timepoint.\n",
      "Missing files for C6. Skipping this timepoint.\n",
      "Missing files for C9. Skipping this timepoint.\n",
      "Missing files for C18. Skipping this timepoint.\n",
      "Missing files for C36. Skipping this timepoint.\n",
      "No valid timepoints with data.\n",
      "Processing Proliferating_Effector - control\n",
      "Processing Proliferating_Effector - short_term\n",
      "Processing Proliferating_Effector - long_term\n",
      "Processing CD8 - control\n",
      "Processing CD8 - short_term\n",
      "Processing CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization generation completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ot\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration Parameters\n",
    "# ---------------------------\n",
    "\n",
    "# Base directory where the CSV files are stored\n",
    "base_input_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/new/temp\"  # Replace with your actual path\n",
    "\n",
    "# Base directory to save the plots\n",
    "base_output_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/new/temp\"  # Replace with your desired output path\n",
    "\n",
    "# Subpopulations (should match the names used in R)\n",
    "subpopulations = [\n",
    "    \"Activated_CD4\",\n",
    "    \"Effector_CD8\",\n",
    "    \"Effector_Memory_CD8\",\n",
    "    \"Exhausted_T\",\n",
    "    \"Gamma_Delta_T\",\n",
    "    \"Active_CD4\",\n",
    "    \"Naive_CD4\",\n",
    "    \"Memory_CD4\",\n",
    "    \"Memory_CD8\",\n",
    "    \"Anergic_CD8\",\n",
    "    \"Naive_CD8\",\n",
    "    \"Hyperactivated_CD8\",\n",
    "    \"Proliferating_Effector\",\n",
    "    \"CD8\"\n",
    "]\n",
    "\n",
    "# Cohorts\n",
    "cohorts = [\"control\", \"short_term\", \"long_term\"]\n",
    "\n",
    "# Ordered timepoints\n",
    "all_timepoints = [\"Pre\", \"C1\", \"C2\", \"C4\", \"C6\", \"C9\", \"C18\", \"C36\"]\n",
    "\n",
    "# Color mapping for cohorts\n",
    "cohort_colors = {\n",
    "    'control': 'yellow',\n",
    "    'short_term': 'blue',\n",
    "    'long_term': 'red'\n",
    "}\n",
    "\n",
    "# Desired minimum and maximum arrow lengths for visualization\n",
    "min_arrow_length = 0.3  # Adjust as needed\n",
    "max_arrow_length = 2    # Adjust as needed\n",
    "\n",
    "# Number of cells to sample for single-cell plotting (to reduce overcrowding)\n",
    "# This can be the minimum number found across timepoints, or a fixed number.\n",
    "# Here we will determine a uniform sampling size based on the smallest dataset\n",
    "# found across timepoints, same as before, but only for plotting.\n",
    "max_plot_cells = None  # will determine at runtime\n",
    "\n",
    "def optimal_transport_visualization(subpop_name, cohort_name):\n",
    "    input_folder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "    output_folder = os.path.join(base_output_dir, subpop_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    timepoint_folders = sorted([f for f in os.listdir(input_folder) if f.startswith(\"Timepoint_\")])\n",
    "    available_timepoints = [tp.split(\"_\")[1] for tp in timepoint_folders]\n",
    "    cohort_timepoints = [tp for tp in all_timepoints if tp in available_timepoints]\n",
    "    \n",
    "    if not cohort_timepoints:\n",
    "        print(f\"No timepoints available for {subpop_name} in {cohort_name} cohort.\")\n",
    "        return\n",
    "\n",
    "    # We'll store coordinates and also find global axis limits\n",
    "    all_x_coords = []\n",
    "    all_y_coords = []\n",
    "    \n",
    "    # Load all source and gray cells (full sets) for axis limits\n",
    "    # We also determine the smallest number of source and gray cells across timepoints\n",
    "    # for single-cell plotting (downsampling).\n",
    "    cell_counts_source = []\n",
    "    cell_counts_gray = []\n",
    "    \n",
    "    full_data = {}  # store full data for each timepoint\n",
    "    \n",
    "    for tp in cohort_timepoints:\n",
    "        source_folder = os.path.join(input_folder, f\"Timepoint_{tp}\")\n",
    "        source_cells_file = os.path.join(source_folder, 'source_cells_new.csv')\n",
    "        gray_cells_file = os.path.join(source_folder, 'gray_cells_new.csv')\n",
    "        \n",
    "        if not (os.path.exists(source_cells_file) and os.path.exists(gray_cells_file)):\n",
    "            print(f\"Missing files for {tp}. Skipping this timepoint.\")\n",
    "            continue\n",
    "        \n",
    "        source_cells = pd.read_csv(source_cells_file)\n",
    "        gray_cells = pd.read_csv(gray_cells_file)\n",
    "        \n",
    "        if source_cells.empty:\n",
    "            print(f\"No source cells for {tp}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Store full data\n",
    "        full_data[tp] = {\n",
    "            'source': source_cells,\n",
    "            'gray': gray_cells\n",
    "        }\n",
    "        \n",
    "        # For axis limits\n",
    "        all_x_coords.extend(gray_cells['UMAP_2'])\n",
    "        all_x_coords.extend(source_cells['UMAP_2'])\n",
    "        all_y_coords.extend(gray_cells['UMAP_1'])\n",
    "        all_y_coords.extend(source_cells['UMAP_1'])\n",
    "        \n",
    "        cell_counts_source.append(len(source_cells))\n",
    "        cell_counts_gray.append(len(gray_cells))\n",
    "    \n",
    "    if not full_data:\n",
    "        print(\"No valid timepoints with data.\")\n",
    "        return\n",
    "    \n",
    "    x_min, x_max = min(all_x_coords), max(all_x_coords)\n",
    "    y_min, y_max = min(all_y_coords), max(all_y_coords)\n",
    "    \n",
    "    # Determine number of cells to plot for single-cell arrows (downsample for plotting only)\n",
    "    # We use the minimum across all timepoints to keep it consistent.\n",
    "    min_source_cells = min(cell_counts_source) if cell_counts_source else 0\n",
    "    min_gray_cells = min(cell_counts_gray) if cell_counts_gray else 0\n",
    "    if min_source_cells == 0 or min_gray_cells == 0:\n",
    "        print(\"Insufficient cells. Skipping visualization.\")\n",
    "        return\n",
    "    \n",
    "    # We'll do OT and cluster arrow computations using the full sets,\n",
    "    # but only plot a subsample of source & gray cells (and their arrows) in the single-cell arrow PDF.\n",
    "    \n",
    "    # We'll store results for both single-cell (subsampled) plotting and cluster-level arrows (full).\n",
    "    timepoint_results = {}\n",
    "    \n",
    "    # We need to compute OT from each timepoint to the next (except the last one)\n",
    "    for i, source_tp in enumerate(cohort_timepoints):\n",
    "        source_data = full_data[source_tp]\n",
    "        source_cells = source_data['source']\n",
    "        \n",
    "        # Store initial data for plotting\n",
    "        # Downsample for plotting single-cell arrows only\n",
    "        sampled_source_cells = source_cells.sample(n=min_source_cells, random_state=42)\n",
    "        sampled_gray_cells = full_data[source_tp]['gray'].sample(n=min_gray_cells, random_state=42)\n",
    "        \n",
    "        timepoint_results[source_tp] = {\n",
    "            'sampled_source': sampled_source_cells,\n",
    "            'sampled_gray': sampled_gray_cells\n",
    "        }\n",
    "        \n",
    "        if i < len(cohort_timepoints) - 1:\n",
    "            target_tp = cohort_timepoints[i+1]\n",
    "            target_data = full_data[target_tp]\n",
    "            target_cells = target_data['source']\n",
    "            \n",
    "            if target_cells.empty:\n",
    "                # No target cells, no OT\n",
    "                continue\n",
    "            \n",
    "            pc_cols = [c for c in source_cells.columns if c.startswith('PC_')]\n",
    "            full_source_coords = source_cells[pc_cols].values\n",
    "            full_target_coords = target_cells[pc_cols].values\n",
    "            \n",
    "            full_source_coords_pc = source_cells[pc_cols].values\n",
    "            full_target_coords_pc = target_cells[pc_cols].values\n",
    "            \n",
    "            # Also store UMAP coordinates for plotting and displacement vectors\n",
    "            full_source_coords_umap = source_cells[['UMAP_2', 'UMAP_1']].values\n",
    "            full_target_coords_umap = target_cells[['UMAP_2', 'UMAP_1']].values\n",
    "            \n",
    "            # Uniform distributions\n",
    "            a = np.ones((full_source_coords.shape[0],)) / full_source_coords.shape[0]\n",
    "            b = np.ones((full_target_coords.shape[0],)) / full_target_coords.shape[0]\n",
    "            \n",
    "            # Compute cost matrix\n",
    "            cost_matrix = ot.dist(full_source_coords_pc, full_target_coords_pc, metric='euclidean')\n",
    "            \n",
    "            try:\n",
    "                transport_plan = ot.emd(a, b, cost_matrix, numItermax=100000)\n",
    "            except Exception as e:\n",
    "                print(f\"OT computation failed for {source_tp} to {target_tp}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Assign each full source cell to a target cell\n",
    "            full_target_indices = np.argmax(transport_plan, axis=1)\n",
    "            displacement_vectors_full = full_target_coords_umap[full_target_indices] - full_source_coords_umap\n",
    "            \n",
    "            # Compute norms\n",
    "            norms_full = np.linalg.norm(displacement_vectors_full, axis=1)\n",
    "            \n",
    "            # Scale arrow lengths for cluster-level arrows\n",
    "            min_norm_full = np.min(norms_full)\n",
    "            max_norm_full = np.max(norms_full)\n",
    "            \n",
    "            if max_norm_full - min_norm_full > 0:\n",
    "                arrow_lengths_full = ((norms_full - min_norm_full) / (max_norm_full - min_norm_full)) * (max_arrow_length - min_arrow_length) + min_arrow_length\n",
    "            else:\n",
    "                arrow_lengths_full = np.full_like(norms_full, min_arrow_length)\n",
    "            \n",
    "            # For single-cell plotting, we also need displacement vectors of sampled sets\n",
    "            # Assign each sampled source cell to target cell based on the full transport plan\n",
    "            # We must find their indices in the full set\n",
    "            # Matching sampled source cells to full source:\n",
    "            sampled_source_indices = source_cells.index.get_indexer_for(sampled_source_cells.index)\n",
    "            sampled_displacements = displacement_vectors_full[sampled_source_indices, :]\n",
    "            sampled_norms = norms_full[sampled_source_indices]\n",
    "            \n",
    "            # Scale for sampled set plotting\n",
    "            min_norm_sampled = np.min(sampled_norms)\n",
    "            max_norm_sampled = np.max(sampled_norms)\n",
    "            if max_norm_sampled - min_norm_sampled > 0:\n",
    "                arrow_lengths_sampled = ((sampled_norms - min_norm_sampled) / (max_norm_sampled - min_norm_sampled)) * (max_arrow_length - min_arrow_length) + min_arrow_length\n",
    "            else:\n",
    "                arrow_lengths_sampled = np.full_like(sampled_norms, min_arrow_length)\n",
    "            \n",
    "            # Store single-cell arrow info (for plotting)\n",
    "            timepoint_results[source_tp]['single_cell_displacements'] = sampled_displacements\n",
    "            timepoint_results[source_tp]['single_cell_arrow_lengths'] = arrow_lengths_sampled\n",
    "            \n",
    "            clusters_of_interest = {1, 2, 3, 8, 10, 12, 14}\n",
    "\n",
    "            # Compute cluster-level arrows using full data (not sampled)\n",
    "            if 'seurat_clusters' in source_cells.columns:\n",
    "                # Replace 16 and 17 with 14 before computing cluster-level arrows\n",
    "                source_cells['seurat_clusters'] = source_cells['seurat_clusters'].replace({16: 14, 17: 14})\n",
    "                source_clusters_full = source_cells['seurat_clusters'].values\n",
    "                df_cluster_full = pd.DataFrame({\n",
    "                    'cluster': source_clusters_full,\n",
    "                    'sx': full_source_coords_umap[:,0],\n",
    "                    'sy': full_source_coords_umap[:,1],\n",
    "                    'dx': displacement_vectors_full[:,0],\n",
    "                    'dy': displacement_vectors_full[:,1],\n",
    "                    'norm': norms_full\n",
    "                })\n",
    "                \n",
    "                # Combine source and gray cells\n",
    "                all_cells_combined = pd.concat([source_cells, gray_cells])\n",
    "                \n",
    "                # Replace 16 and 17 with 14 in the combined set as well\n",
    "                all_cells_combined['seurat_clusters'] = all_cells_combined['seurat_clusters'].replace({16:14,17:14})\n",
    "                \n",
    "                # Extract UMAP coordinates for all cells\n",
    "                all_clusters = all_cells_combined['seurat_clusters'].values\n",
    "                all_coords_umap = all_cells_combined[['UMAP_2', 'UMAP_1']].values\n",
    "                \n",
    "                # Create a DataFrame for centroid calculation\n",
    "                df_all = pd.DataFrame({\n",
    "                    'cluster': all_clusters,\n",
    "                    'sx': all_coords_umap[:,0],\n",
    "                    'sy': all_coords_umap[:,1]\n",
    "                })\n",
    "                \n",
    "                # Compute centroids from all cells (source + gray)\n",
    "                all_group = df_all.groupby('cluster')\n",
    "                # centroids = all_group[['sx','sy']].mean()\n",
    "                centroids = all_group[['sx','sy']].median()\n",
    "\n",
    "                \n",
    "                # Mean displacement vectors still come from source cells only\n",
    "                group = df_cluster_full.groupby('cluster')\n",
    "                mean_disp = group[['dx','dy']].mean()\n",
    "            \n",
    "                cluster_norms = np.sqrt(mean_disp['dx']**2 + mean_disp['dy']**2)\n",
    "                cn_min = cluster_norms.min()\n",
    "                cn_max = cluster_norms.max()\n",
    "                \n",
    "                # Ensure cluster_arrow_lengths is a Series\n",
    "                if cn_max - cn_min > 0:\n",
    "                    cluster_arrow_lengths = ((cluster_norms - cn_min) / (cn_max - cn_min)) * (max_arrow_length - min_arrow_length) + min_arrow_length\n",
    "                else:\n",
    "                    cluster_arrow_lengths = pd.Series(np.full_like(cluster_norms, min_arrow_length), index=cluster_norms.index)\n",
    "                \n",
    "                cluster_arrows = []\n",
    "\n",
    "                # Only consider clusters present in both centroids and mean_disp\n",
    "                common_clusters = centroids.index.intersection(mean_disp.index)\n",
    "                for clust in common_clusters:\n",
    "                    # Only show aggregated arrow if cluster is in clusters_of_interest\n",
    "                    if clust in clusters_of_interest:\n",
    "                        cx, cy = centroids.loc[clust, ['sx','sy']]\n",
    "                        cdx, cdy = mean_disp.loc[clust, ['dx','dy']]\n",
    "                        cnorm = np.sqrt(cdx**2 + cdy**2)\n",
    "                        if cnorm > 0:\n",
    "                            cdx = cdx / cnorm\n",
    "                            cdy = cdy / cnorm\n",
    "                        else:\n",
    "                            cdx, cdy = 0, 0\n",
    "            \n",
    "                        # Safely access cluster_arrow_lengths\n",
    "                        try:\n",
    "                            length = cluster_arrow_lengths.loc[clust]\n",
    "                        except AttributeError:\n",
    "                            # If this happens, convert cluster_arrow_lengths to Series and retry\n",
    "                            print(\"cluster_arrow_lengths is not a Series. Attempting to convert...\")\n",
    "                            if isinstance(cluster_arrow_lengths, np.ndarray):\n",
    "                                cluster_arrow_lengths = pd.Series(cluster_arrow_lengths, index=cluster_norms.index)\n",
    "                            try:\n",
    "                                length = cluster_arrow_lengths.loc[clust]\n",
    "                            except Exception as e:\n",
    "                                print(f\"Failed to access cluster_arrow_lengths for cluster {clust}: {e}\")\n",
    "                                length = min_arrow_length\n",
    "            \n",
    "                        cdx *= length\n",
    "                        cdy *= length\n",
    "                        cluster_arrows.append((cx, cy, cdx, cdy))\n",
    "                \n",
    "                timepoint_results[source_tp]['cluster_arrows'] = cluster_arrows\n",
    "            else:\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                print(f\"No 'seurat_clusters' column in {source_tp} source cells. Cannot compute cluster arrows.\")\n",
    "\n",
    "            \n",
    "        else:\n",
    "            # Last timepoint has no next timepoint\n",
    "            timepoint_results[source_tp]['single_cell_displacements'] = np.array([]) # no arrows\n",
    "            timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "            timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "            if 'seurat_clusters' in source_cells.columns:\n",
    "                # Still record that cluster info was present\n",
    "                # but no arrows since there's no next timepoint\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "            else:\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "    \n",
    "    # Now plot the original single-cell arrows PDF\n",
    "    output_file_original = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_differential_arrow_lengths_equal_cells_using_PCs.pdf\")\n",
    "    with PdfPages(output_file_original) as pdf_original:\n",
    "        num_timepoints = len(cohort_timepoints)\n",
    "        fig_orig, axes_orig = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        if num_timepoints == 1:\n",
    "            axes_orig = [axes_orig]\n",
    "        \n",
    "        for i, source_tp in enumerate(cohort_timepoints):\n",
    "            ax = axes_orig[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            sampled_source_cells = res['sampled_source']\n",
    "            sampled_gray_cells = res['sampled_gray']\n",
    "            \n",
    "            gray_x = sampled_gray_cells['UMAP_2'].values\n",
    "            gray_y = sampled_gray_cells['UMAP_1'].values\n",
    "            source_x = sampled_source_cells['UMAP_2'].values\n",
    "            source_y = sampled_source_cells['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "            \n",
    "            single_cell_displacements = res['single_cell_displacements']\n",
    "            single_cell_arrow_lengths = res['single_cell_arrow_lengths']\n",
    "            \n",
    "            if len(single_cell_displacements) > 0:\n",
    "                source_coords_sampled = sampled_source_cells[['UMAP_2','UMAP_1']].values\n",
    "                # Compute unit vectors for sampled\n",
    "                sampled_norms = np.linalg.norm(single_cell_displacements, axis=1)\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    unit_vectors = single_cell_displacements / sampled_norms[:, np.newaxis]\n",
    "                    unit_vectors[~np.isfinite(unit_vectors)] = 0\n",
    "                scaled_vectors = unit_vectors * single_cell_arrow_lengths[:, np.newaxis]\n",
    "                \n",
    "                for j in range(len(source_coords_sampled)):\n",
    "                    sx, sy = source_coords_sampled[j]\n",
    "                    dx, dy = scaled_vectors[j]\n",
    "                    ax.arrow(sx, sy, dx, dy,\n",
    "                             color='black', alpha=0.7, head_width=0.05, head_length=0.05,\n",
    "                             length_includes_head=True, linewidth=0.5)\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_orig.suptitle(f\"{subpop_name} - {cohort_name}\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_original.savefig(fig_orig)\n",
    "        plt.close(fig_orig)\n",
    "    \n",
    "    # Now plot aggregated cluster arrows in a new PDF\n",
    "    output_file_cluster = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_aggregated_cluster_arrows_equal_cells_using_PCs.pdf\")\n",
    "    with PdfPages(output_file_cluster) as pdf_cluster:\n",
    "        num_timepoints = len(cohort_timepoints)\n",
    "        fig_clust, axes_clust = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        if num_timepoints == 1:\n",
    "            axes_clust = [axes_clust]\n",
    "        \n",
    "        for i, source_tp in enumerate(cohort_timepoints):\n",
    "            ax = axes_clust[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            \n",
    "            # For cluster-level plot, we still show the same sampled source/gray points as background\n",
    "            gray_x = res['sampled_gray']['UMAP_2'].values\n",
    "            gray_y = res['sampled_gray']['UMAP_1'].values\n",
    "            source_x = res['sampled_source']['UMAP_2'].values\n",
    "            source_y = res['sampled_source']['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "\n",
    "            cluster_arrows = res.get('cluster_arrows', [])\n",
    "            for (cx, cy, cdx, cdy) in cluster_arrows:\n",
    "                ax.arrow(cx, cy, cdx, cdy,\n",
    "                         color='black', alpha=0.7, head_width=0.1, head_length=0.1,\n",
    "                         length_includes_head=True, linewidth=1.0)\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_clust.suptitle(f\"{subpop_name} - {cohort_name} (Cluster-Level Arrows)\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_cluster.savefig(fig_clust)\n",
    "        plt.close(fig_clust)\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution Loop\n",
    "# ---------------------------\n",
    "\n",
    "for subpop_name in subpopulations:\n",
    "    for cohort_name in cohorts:\n",
    "        print(f\"Processing {subpop_name} - {cohort_name}\")\n",
    "        input_subfolder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "        if not os.path.exists(input_subfolder):\n",
    "            print(f\"Input subfolder does not exist: {input_subfolder}. Skipping.\")\n",
    "            continue\n",
    "        optimal_transport_visualization(subpop_name, cohort_name)\n",
    "\n",
    "print(\"Visualization generation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "642c8a17-1ed6-4fe2-b085-7618ee915696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Activated_CD4 - control\n",
      "Processing Activated_CD4 - short_term\n",
      "Processing Activated_CD4 - long_term\n",
      "Processing Effector_CD8 - control\n",
      "Processing Effector_CD8 - short_term\n",
      "Processing Effector_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Effector_Memory_CD8 - control\n",
      "Processing Effector_Memory_CD8 - short_term\n",
      "Processing Effector_Memory_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Exhausted_T - control\n",
      "Processing Exhausted_T - short_term\n",
      "Processing Exhausted_T - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Gamma_Delta_T - control\n",
      "Processing Gamma_Delta_T - short_term\n",
      "Processing Gamma_Delta_T - long_term\n",
      "Processing Active_CD4 - control\n",
      "Processing Active_CD4 - short_term\n",
      "Processing Active_CD4 - long_term\n",
      "Processing Naive_CD4 - control\n",
      "Processing Naive_CD4 - short_term\n",
      "Processing Naive_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD4 - control\n",
      "Processing Memory_CD4 - short_term\n",
      "Processing Memory_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD8 - control\n",
      "Processing Memory_CD8 - short_term\n",
      "Processing Memory_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Anergic_CD8 - control\n",
      "Processing Anergic_CD8 - short_term\n",
      "Processing Anergic_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Naive_CD8 - control\n",
      "Processing Naive_CD8 - short_term\n",
      "Processing Naive_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Hyperactivated_CD8 - control\n",
      "Missing files for Pre. Skipping this timepoint.\n",
      "Missing files for C1. Skipping this timepoint.\n",
      "Missing files for C2. Skipping this timepoint.\n",
      "No valid timepoints with data.\n",
      "Processing Hyperactivated_CD8 - short_term\n",
      "Missing files for Pre. Skipping this timepoint.\n",
      "Missing files for C1. Skipping this timepoint.\n",
      "Missing files for C2. Skipping this timepoint.\n",
      "Missing files for C4. Skipping this timepoint.\n",
      "No valid timepoints with data.\n",
      "Processing Hyperactivated_CD8 - long_term\n",
      "Missing files for Pre. Skipping this timepoint.\n",
      "Missing files for C1. Skipping this timepoint.\n",
      "Missing files for C2. Skipping this timepoint.\n",
      "Missing files for C4. Skipping this timepoint.\n",
      "Missing files for C6. Skipping this timepoint.\n",
      "Missing files for C9. Skipping this timepoint.\n",
      "Missing files for C18. Skipping this timepoint.\n",
      "Missing files for C36. Skipping this timepoint.\n",
      "No valid timepoints with data.\n",
      "Processing Proliferating_Effector - control\n",
      "Processing Proliferating_Effector - short_term\n",
      "Processing Proliferating_Effector - long_term\n",
      "Processing CD8 - control\n",
      "Processing CD8 - short_term\n",
      "Processing CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization generation completed.\n"
     ]
    }
   ],
   "source": [
    "# remove previous, if this works\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ot\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration Parameters\n",
    "# ---------------------------\n",
    "\n",
    "# Base directory where the CSV files are stored\n",
    "base_input_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/new/temp\"  # Replace with your actual path\n",
    "\n",
    "# Base directory to save the plots\n",
    "base_output_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/new/temp\"  # Replace with your desired output path\n",
    "\n",
    "# Subpopulations (should match the names used in R)\n",
    "subpopulations = [\n",
    "    \"Activated_CD4\",\n",
    "    \"Effector_CD8\",\n",
    "    \"Effector_Memory_CD8\",\n",
    "    \"Exhausted_T\",\n",
    "    \"Gamma_Delta_T\",\n",
    "    \"Active_CD4\",\n",
    "    \"Naive_CD4\",\n",
    "    \"Memory_CD4\",\n",
    "    \"Memory_CD8\",\n",
    "    \"Anergic_CD8\",\n",
    "    \"Naive_CD8\",\n",
    "    \"Hyperactivated_CD8\",\n",
    "    \"Proliferating_Effector\",\n",
    "    \"CD8\"\n",
    "]\n",
    "\n",
    "# Cohorts\n",
    "cohorts = [\"control\", \"short_term\", \"long_term\"]\n",
    "\n",
    "# Ordered timepoints\n",
    "all_timepoints = [\"Pre\", \"C1\", \"C2\", \"C4\", \"C6\", \"C9\", \"C18\", \"C36\"]\n",
    "\n",
    "# Color mapping for cohorts\n",
    "cohort_colors = {\n",
    "    'control': 'yellow',\n",
    "    'short_term': 'blue',\n",
    "    'long_term': 'red'\n",
    "}\n",
    "\n",
    "# Desired minimum and maximum arrow lengths for visualization\n",
    "min_arrow_length = 0.3  # Adjust as needed\n",
    "max_arrow_length = 2    # Adjust as needed\n",
    "\n",
    "# Number of cells to sample for single-cell plotting (to reduce overcrowding)\n",
    "max_plot_cells = None  # will determine at runtime\n",
    "\n",
    "def optimal_transport_visualization(subpop_name, cohort_name):\n",
    "    input_folder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "    output_folder = os.path.join(base_output_dir, subpop_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    timepoint_folders = sorted([f for f in os.listdir(input_folder) if f.startswith(\"Timepoint_\")])\n",
    "    available_timepoints = [tp.split(\"_\")[1] for tp in timepoint_folders]\n",
    "    cohort_timepoints = [tp for tp in all_timepoints if tp in available_timepoints]\n",
    "    \n",
    "    if not cohort_timepoints:\n",
    "        print(f\"No timepoints available for {subpop_name} in {cohort_name} cohort.\")\n",
    "        return\n",
    "\n",
    "    # We'll store coordinates and also find global axis limits\n",
    "    all_x_coords = []\n",
    "    all_y_coords = []\n",
    "    \n",
    "    cell_counts_source = []\n",
    "    cell_counts_gray = []\n",
    "    full_data = {}  # store full data for each timepoint\n",
    "    \n",
    "    for tp in cohort_timepoints:\n",
    "        source_folder = os.path.join(input_folder, f\"Timepoint_{tp}\")\n",
    "        source_cells_file = os.path.join(source_folder, 'source_cells_new.csv')\n",
    "        gray_cells_file = os.path.join(source_folder, 'gray_cells_new.csv')\n",
    "        \n",
    "        if not (os.path.exists(source_cells_file) and os.path.exists(gray_cells_file)):\n",
    "            print(f\"Missing files for {tp}. Skipping this timepoint.\")\n",
    "            continue\n",
    "        \n",
    "        source_cells = pd.read_csv(source_cells_file)\n",
    "        gray_cells = pd.read_csv(gray_cells_file)\n",
    "        \n",
    "        if source_cells.empty:\n",
    "            print(f\"No source cells for {tp}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Store full data\n",
    "        full_data[tp] = {\n",
    "            'source': source_cells,\n",
    "            'gray': gray_cells\n",
    "        }\n",
    "        \n",
    "        # For axis limits\n",
    "        all_x_coords.extend(gray_cells['UMAP_2'])\n",
    "        all_x_coords.extend(source_cells['UMAP_2'])\n",
    "        all_y_coords.extend(gray_cells['UMAP_1'])\n",
    "        all_y_coords.extend(source_cells['UMAP_1'])\n",
    "        \n",
    "        cell_counts_source.append(len(source_cells))\n",
    "        cell_counts_gray.append(len(gray_cells))\n",
    "    \n",
    "    if not full_data:\n",
    "        print(\"No valid timepoints with data.\")\n",
    "        return\n",
    "    \n",
    "    x_min, x_max = min(all_x_coords), max(all_x_coords)\n",
    "    y_min, y_max = min(all_y_coords), max(all_y_coords)\n",
    "    \n",
    "    # Determine number of cells to plot for single-cell arrows (downsample for plotting only)\n",
    "    min_source_cells = min(cell_counts_source) if cell_counts_source else 0\n",
    "    min_gray_cells = min(cell_counts_gray) if cell_counts_gray else 0\n",
    "    if min_source_cells == 0 or min_gray_cells == 0:\n",
    "        print(\"Insufficient cells. Skipping visualization.\")\n",
    "        return\n",
    "    \n",
    "    timepoint_results = {}\n",
    "    \n",
    "    # Compute OT from each timepoint to the next (except the last one)\n",
    "    for i, source_tp in enumerate(cohort_timepoints):\n",
    "        source_data = full_data[source_tp]\n",
    "        source_cells = source_data['source']\n",
    "        \n",
    "        # Downsample for plotting single-cell arrows only\n",
    "        sampled_source_cells = source_cells.sample(n=min_source_cells, random_state=42)\n",
    "        sampled_gray_cells = full_data[source_tp]['gray'].sample(n=min_gray_cells, random_state=42)\n",
    "        \n",
    "        timepoint_results[source_tp] = {\n",
    "            'sampled_source': sampled_source_cells,\n",
    "            'sampled_gray': sampled_gray_cells\n",
    "        }\n",
    "        \n",
    "        if i < len(cohort_timepoints) - 1:\n",
    "            target_tp = cohort_timepoints[i+1]\n",
    "            target_data = full_data[target_tp]\n",
    "            target_cells = target_data['source']\n",
    "            \n",
    "            if target_cells.empty:\n",
    "                continue\n",
    "            \n",
    "            pc_cols = [c for c in source_cells.columns if c.startswith('PC_')]\n",
    "            full_source_coords_pc = source_cells[pc_cols].values\n",
    "            full_target_coords_pc = target_cells[pc_cols].values\n",
    "            \n",
    "            # Also store UMAP coordinates\n",
    "            full_source_coords_umap = source_cells[['UMAP_2', 'UMAP_1']].values\n",
    "            full_target_coords_umap = target_cells[['UMAP_2', 'UMAP_1']].values\n",
    "            \n",
    "            # Uniform distributions\n",
    "            a = np.ones((full_source_coords_pc.shape[0],)) / full_source_coords_pc.shape[0]\n",
    "            b = np.ones((full_target_coords_pc.shape[0],)) / full_target_coords_pc.shape[0]\n",
    "            \n",
    "            # Compute cost matrix\n",
    "            cost_matrix = ot.dist(full_source_coords_pc, full_target_coords_pc, metric='euclidean')\n",
    "            \n",
    "            try:\n",
    "                transport_plan = ot.emd(a, b, cost_matrix, numItermax=100000)\n",
    "            except Exception as e:\n",
    "                print(f\"OT computation failed for {source_tp} to {target_tp}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            full_target_indices = np.argmax(transport_plan, axis=1)\n",
    "            displacement_vectors_full = full_target_coords_umap[full_target_indices] - full_source_coords_umap\n",
    "            \n",
    "            # Compute norms\n",
    "            norms_full = np.linalg.norm(displacement_vectors_full, axis=1)\n",
    "            min_norm_full = np.min(norms_full)\n",
    "            max_norm_full = np.max(norms_full)\n",
    "            \n",
    "            if max_norm_full - min_norm_full > 0:\n",
    "                arrow_lengths_full = ((norms_full - min_norm_full) / (max_norm_full - min_norm_full)) * (max_arrow_length - min_arrow_length) + min_arrow_length\n",
    "            else:\n",
    "                arrow_lengths_full = np.full_like(norms_full, min_arrow_length)\n",
    "            \n",
    "            # For single-cell plotting\n",
    "            sampled_source_indices = source_cells.index.get_indexer_for(sampled_source_cells.index)\n",
    "            sampled_displacements = displacement_vectors_full[sampled_source_indices, :]\n",
    "            sampled_norms = norms_full[sampled_source_indices]\n",
    "            \n",
    "            min_norm_sampled = np.min(sampled_norms)\n",
    "            max_norm_sampled = np.max(sampled_norms)\n",
    "            if max_norm_sampled - min_norm_sampled > 0:\n",
    "                arrow_lengths_sampled = ((sampled_norms - min_norm_sampled) / (max_norm_sampled - min_norm_sampled)) * (max_arrow_length - min_arrow_length) + min_arrow_length\n",
    "            else:\n",
    "                arrow_lengths_sampled = np.full_like(sampled_norms, min_arrow_length)\n",
    "            \n",
    "            timepoint_results[source_tp]['single_cell_displacements'] = sampled_displacements\n",
    "            timepoint_results[source_tp]['single_cell_arrow_lengths'] = arrow_lengths_sampled\n",
    "            \n",
    "            clusters_of_interest = {1, 2, 3, 8, 10, 12, 14}\n",
    "\n",
    "            # Compute cluster-level arrows using full data\n",
    "            if 'seurat_clusters' in source_cells.columns:\n",
    "                source_cells['seurat_clusters'] = source_cells['seurat_clusters'].replace({16: 14, 17: 14})\n",
    "                source_clusters_full = source_cells['seurat_clusters'].values\n",
    "                df_cluster_full = pd.DataFrame({\n",
    "                    'cluster': source_clusters_full,\n",
    "                    'sx': full_source_coords_umap[:,0],\n",
    "                    'sy': full_source_coords_umap[:,1],\n",
    "                    'dx': displacement_vectors_full[:,0],\n",
    "                    'dy': displacement_vectors_full[:,1],\n",
    "                    'norm': norms_full\n",
    "                })\n",
    "                \n",
    "                gray_cells = full_data[source_tp]['gray']\n",
    "                all_cells_combined = pd.concat([source_cells, gray_cells])\n",
    "                all_cells_combined['seurat_clusters'] = all_cells_combined['seurat_clusters'].replace({16:14,17:14})\n",
    "                \n",
    "                all_clusters = all_cells_combined['seurat_clusters'].values\n",
    "                all_coords_umap = all_cells_combined[['UMAP_2', 'UMAP_1']].values\n",
    "                \n",
    "                df_all = pd.DataFrame({\n",
    "                    'cluster': all_clusters,\n",
    "                    'sx': all_coords_umap[:,0],\n",
    "                    'sy': all_coords_umap[:,1]\n",
    "                })\n",
    "                \n",
    "                # Compute centroids from all cells (source + gray)\n",
    "                all_group = df_all.groupby('cluster')\n",
    "                centroids = all_group[['sx','sy']].median()\n",
    "                \n",
    "                # Mean displacement from source cells only\n",
    "                group = df_cluster_full.groupby('cluster')\n",
    "                mean_disp = group[['dx','dy']].mean()\n",
    "            \n",
    "                cluster_norms = np.sqrt(mean_disp['dx']**2 + mean_disp['dy']**2)\n",
    "                cn_min = cluster_norms.min()\n",
    "                cn_max = cluster_norms.max()\n",
    "                \n",
    "                if cn_max - cn_min > 0:\n",
    "                    cluster_arrow_lengths = ((cluster_norms - cn_min) / (cn_max - cn_min)) * (max_arrow_length - min_arrow_length) + min_arrow_length\n",
    "                else:\n",
    "                    cluster_arrow_lengths = pd.Series(np.full_like(cluster_norms, min_arrow_length), index=cluster_norms.index)\n",
    "                \n",
    "                cluster_arrows = []\n",
    "                common_clusters = centroids.index.intersection(mean_disp.index)\n",
    "                for clust in common_clusters:\n",
    "                    if clust in clusters_of_interest:\n",
    "                        cx, cy = centroids.loc[clust, ['sx','sy']]\n",
    "                        cdx, cdy = mean_disp.loc[clust, ['dx','dy']]\n",
    "                        cnorm = np.sqrt(cdx**2 + cdy**2)\n",
    "                        if cnorm > 0:\n",
    "                            cdx = cdx / cnorm\n",
    "                            cdy = cdy / cnorm\n",
    "                        else:\n",
    "                            cdx, cdy = 0, 0\n",
    "                        \n",
    "                        try:\n",
    "                            length = cluster_arrow_lengths.loc[clust]\n",
    "                        except AttributeError:\n",
    "                            if isinstance(cluster_arrow_lengths, np.ndarray):\n",
    "                                cluster_arrow_lengths = pd.Series(cluster_arrow_lengths, index=cluster_norms.index)\n",
    "                            try:\n",
    "                                length = cluster_arrow_lengths.loc[clust]\n",
    "                            except Exception as e:\n",
    "                                print(f\"Failed to access cluster_arrow_lengths for cluster {clust}: {e}\")\n",
    "                                length = min_arrow_length\n",
    "            \n",
    "                        cdx *= length\n",
    "                        cdy *= length\n",
    "                        # Store cluster ID alongside arrow coordinates for line width calculation later\n",
    "                        cluster_arrows.append((clust, cx, cy, cdx, cdy))\n",
    "                \n",
    "                timepoint_results[source_tp]['cluster_arrows'] = cluster_arrows\n",
    "\n",
    "                if len(cluster_arrows) > 0:\n",
    "                    # Instead of global centroid of all cells, use the median of source cells only:\n",
    "                    source_median_x = source_cells['UMAP_2'].median()\n",
    "                    source_median_y = source_cells['UMAP_1'].median()\n",
    "\n",
    "                    # Sum displacement vectors of all cluster arrows\n",
    "                    total_dx = sum([arrow[3] for arrow in cluster_arrows])\n",
    "                    total_dy = sum([arrow[4] for arrow in cluster_arrows])\n",
    "\n",
    "                    # Store the aggregated arrow\n",
    "                    timepoint_results[source_tp]['aggregated_arrow'] = (source_median_x, source_median_y, total_dx, total_dy)\n",
    "                else:\n",
    "                    # If no cluster arrows, no aggregated arrow\n",
    "                    timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "            else:\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "                print(f\"No 'seurat_clusters' column in {source_tp} source cells. Cannot compute cluster arrows.\")\n",
    "\n",
    "            \n",
    "        else:\n",
    "            # Last timepoint has no next timepoint\n",
    "            timepoint_results[source_tp]['single_cell_displacements'] = np.array([])\n",
    "            timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "            timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "            timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "            if 'seurat_clusters' in source_cells.columns:\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "            else:\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "    \n",
    "    # Plot single-cell arrows PDF\n",
    "    output_file_original = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_differential_arrow_lengths_equal_cells_using_PCs.pdf\")\n",
    "    with PdfPages(output_file_original) as pdf_original:\n",
    "        num_timepoints = len(cohort_timepoints)\n",
    "        fig_orig, axes_orig = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        if num_timepoints == 1:\n",
    "            axes_orig = [axes_orig]\n",
    "        \n",
    "        for i, source_tp in enumerate(cohort_timepoints):\n",
    "            ax = axes_orig[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            sampled_source_cells = res['sampled_source']\n",
    "            sampled_gray_cells = res['sampled_gray']\n",
    "            \n",
    "            gray_x = sampled_gray_cells['UMAP_2'].values\n",
    "            gray_y = sampled_gray_cells['UMAP_1'].values\n",
    "            source_x = sampled_source_cells['UMAP_2'].values\n",
    "            source_y = sampled_source_cells['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "            \n",
    "            single_cell_displacements = res['single_cell_displacements']\n",
    "            single_cell_arrow_lengths = res['single_cell_arrow_lengths']\n",
    "            \n",
    "            if len(single_cell_displacements) > 0:\n",
    "                source_coords_sampled = sampled_source_cells[['UMAP_2','UMAP_1']].values\n",
    "                sampled_norms = np.linalg.norm(single_cell_displacements, axis=1)\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    unit_vectors = single_cell_displacements / sampled_norms[:, np.newaxis]\n",
    "                    unit_vectors[~np.isfinite(unit_vectors)] = 0\n",
    "                scaled_vectors = unit_vectors * single_cell_arrow_lengths[:, np.newaxis]\n",
    "                \n",
    "                for j in range(len(source_coords_sampled)):\n",
    "                    sx, sy = source_coords_sampled[j]\n",
    "                    dx, dy = scaled_vectors[j]\n",
    "                    ax.arrow(sx, sy, dx, dy,\n",
    "                             color='black', alpha=0.7, head_width=0.05, head_length=0.05,\n",
    "                             length_includes_head=True, linewidth=0.5)\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_orig.suptitle(f\"{subpop_name} - {cohort_name}\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_original.savefig(fig_orig)\n",
    "        plt.close(fig_orig)\n",
    "    \n",
    "    # Plot aggregated cluster arrows in a new PDF\n",
    "    output_file_cluster = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_aggregated_cluster_arrows_equal_cells_using_PCs.pdf\")\n",
    "    color_mapping_file = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/Publication_Material/T_Cell_cluster_colors.csv\"  # Update with actual path\n",
    "    color_mapping_df = pd.read_csv(color_mapping_file)\n",
    "    cluster_colors_map = dict(zip(color_mapping_df['Cluster'], color_mapping_df['Color']))\n",
    "    with PdfPages(output_file_cluster) as pdf_cluster:\n",
    "        num_timepoints = len(cohort_timepoints)\n",
    "        fig_clust, axes_clust = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        if num_timepoints == 1:\n",
    "            axes_clust = [axes_clust]\n",
    "        \n",
    "        for i, source_tp in enumerate(cohort_timepoints):\n",
    "            ax = axes_clust[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            \n",
    "            gray_x = res['sampled_gray']['UMAP_2'].values\n",
    "            gray_y = res['sampled_gray']['UMAP_1'].values\n",
    "            source_x = res['sampled_source']['UMAP_2'].values\n",
    "            source_y = res['sampled_source']['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "            \n",
    "            cluster_arrows = res.get('cluster_arrows', [])\n",
    "            \n",
    "            # Compute source cluster proportions for line widths\n",
    "            source_cells = full_data[source_tp]['source']\n",
    "            source_cluster_counts = source_cells['seurat_clusters'].value_counts()\n",
    "            total_source_cells_current = len(source_cells)\n",
    "            \n",
    "            # Now each entry in cluster_arrows is (clust, cx, cy, cdx, cdy)\n",
    "            for (clust, cx, cy, cdx, cdy) in cluster_arrows:\n",
    "                # Get the color for this cluster, default to black if not found\n",
    "                arrow_color = cluster_colors_map.get(clust, 'black')\n",
    "                proportion = source_cluster_counts.get(clust, 0) / total_source_cells_current\n",
    "                line_width = 1.0 + 8.0 * proportion  # adjust scaling as needed\n",
    "\n",
    "                # Draw boundary arrow\n",
    "                boundary_color = 'black'\n",
    "                ax.arrow(cx, cy, cdx, cdy,\n",
    "                         color=boundary_color,\n",
    "                         alpha=1,\n",
    "                         head_width=0.2 + 0.5 * proportion,\n",
    "                         head_length=0.1 + 0.1 * proportion,\n",
    "                         length_includes_head=True,\n",
    "                         linewidth=line_width + 2)  # Thicker for boundary\n",
    "\n",
    "                # Draw main arrow on top\n",
    "                ax.arrow(cx, cy, cdx, cdy,\n",
    "                         color=arrow_color, alpha=1,\n",
    "                         head_width=0.2+0.5*proportion, head_length=0.1+0.1*proportion,   # bigger arrowhead\n",
    "                         length_includes_head=True, linewidth=line_width)\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_clust.suptitle(f\"{subpop_name} - {cohort_name} (Cluster-Level Arrows)\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_cluster.savefig(fig_clust)\n",
    "        plt.close(fig_clust)\n",
    "\n",
    "    # Plot single aggregated arrow in a new PDF\n",
    "    output_file_single_agg = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_aggregated_single_arrow_equal_cells_using_PCs.pdf\")\n",
    "    with PdfPages(output_file_single_agg) as pdf_agg:\n",
    "        num_timepoints = len(cohort_timepoints)\n",
    "        fig_agg, axes_agg = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        if num_timepoints == 1:\n",
    "            axes_agg = [axes_agg]\n",
    "\n",
    "        for i, source_tp in enumerate(cohort_timepoints):\n",
    "            ax = axes_agg[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "\n",
    "            gray_x = res['sampled_gray']['UMAP_2'].values\n",
    "            gray_y = res['sampled_gray']['UMAP_1'].values\n",
    "            source_x = res['sampled_source']['UMAP_2'].values\n",
    "            source_y = res['sampled_source']['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "\n",
    "            aggregated_arrow = res.get('aggregated_arrow', None)\n",
    "            if aggregated_arrow is not None:\n",
    "                global_cx, global_cy, total_dx, total_dy = aggregated_arrow\n",
    "                # Just draw one large arrow\n",
    "                # Use a fixed linewidth and arrowhead for clarity\n",
    "                ax.arrow(global_cx, global_cy, total_dx, total_dy,\n",
    "                         color='black', alpha=0.9,\n",
    "                         head_width=0.3, head_length=0.3,\n",
    "                         length_includes_head=True, linewidth=2.0)\n",
    "\n",
    "            ax.set_title(f\"Timepoint: {source_tp} (Single Aggregated Arrow)\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        fig_agg.suptitle(f\"{subpop_name} - {cohort_name} (Single Aggregated Arrow)\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_agg.savefig(fig_agg)\n",
    "        plt.close(fig_agg)\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution Loop\n",
    "# ---------------------------\n",
    "\n",
    "for subpop_name in subpopulations:\n",
    "    for cohort_name in cohorts:\n",
    "        print(f\"Processing {subpop_name} - {cohort_name}\")\n",
    "        input_subfolder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "        if not os.path.exists(input_subfolder):\n",
    "            print(f\"Input subfolder does not exist: {input_subfolder}. Skipping.\")\n",
    "            continue\n",
    "        optimal_transport_visualization(subpop_name, cohort_name)\n",
    "\n",
    "print(\"Visualization generation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cc2dd2-1469-4b71-877c-21371c2209a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7529d0bd-89e2-4546-923a-bdb271e0106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increasing the arrow lengths to actual displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06ba9b5f-2b07-4c99-8656-3152301cb507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Activated_CD4 - control\n",
      "Processing Activated_CD4 - short_term\n",
      "Processing Activated_CD4 - long_term\n",
      "Processing Effector_CD8 - control\n",
      "Processing Effector_CD8 - short_term\n",
      "Processing Effector_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Effector_Memory_CD8 - control\n",
      "Processing Effector_Memory_CD8 - short_term\n",
      "Processing Effector_Memory_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Exhausted_T - control\n",
      "Processing Exhausted_T - short_term\n",
      "Processing Exhausted_T - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Gamma_Delta_T - control\n",
      "Processing Gamma_Delta_T - short_term\n",
      "Processing Gamma_Delta_T - long_term\n",
      "Processing Active_CD4 - control\n",
      "Processing Active_CD4 - short_term\n",
      "Processing Active_CD4 - long_term\n",
      "Processing Naive_CD4 - control\n",
      "Processing Naive_CD4 - short_term\n",
      "Processing Naive_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD4 - control\n",
      "Processing Memory_CD4 - short_term\n",
      "Processing Memory_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD8 - control\n",
      "Processing Memory_CD8 - short_term\n",
      "Processing Memory_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Anergic_CD8 - control\n",
      "Processing Anergic_CD8 - short_term\n",
      "Processing Anergic_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Naive_CD8 - control\n",
      "Processing Naive_CD8 - short_term\n",
      "Processing Naive_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Hyperactivated_CD8 - control\n",
      "Missing files for Pre. Skipping this timepoint.\n",
      "Missing files for C1. Skipping this timepoint.\n",
      "Missing files for C2. Skipping this timepoint.\n",
      "No valid timepoints with data.\n",
      "Processing Hyperactivated_CD8 - short_term\n",
      "Missing files for Pre. Skipping this timepoint.\n",
      "Missing files for C1. Skipping this timepoint.\n",
      "Missing files for C2. Skipping this timepoint.\n",
      "Missing files for C4. Skipping this timepoint.\n",
      "No valid timepoints with data.\n",
      "Processing Hyperactivated_CD8 - long_term\n",
      "Missing files for Pre. Skipping this timepoint.\n",
      "Missing files for C1. Skipping this timepoint.\n",
      "Missing files for C2. Skipping this timepoint.\n",
      "Missing files for C4. Skipping this timepoint.\n",
      "Missing files for C6. Skipping this timepoint.\n",
      "Missing files for C9. Skipping this timepoint.\n",
      "Missing files for C18. Skipping this timepoint.\n",
      "Missing files for C36. Skipping this timepoint.\n",
      "No valid timepoints with data.\n",
      "Processing Proliferating_Effector - control\n",
      "Processing Proliferating_Effector - short_term\n",
      "Processing Proliferating_Effector - long_term\n",
      "Processing CD8 - control\n",
      "Processing CD8 - short_term\n",
      "Processing CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization generation completed.\n"
     ]
    }
   ],
   "source": [
    "# remove previous, if this works\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ot\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration Parameters\n",
    "# ---------------------------\n",
    "\n",
    "# Base directory where the CSV files are stored\n",
    "base_input_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/new/temp\"  # Replace with your actual path\n",
    "\n",
    "# Base directory to save the plots\n",
    "base_output_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/new/temp\"  # Replace with your desired output path\n",
    "\n",
    "# Subpopulations (should match the names used in R)\n",
    "subpopulations = [\n",
    "    \"Activated_CD4\",\n",
    "    \"Effector_CD8\",\n",
    "    \"Effector_Memory_CD8\",\n",
    "    \"Exhausted_T\",\n",
    "    \"Gamma_Delta_T\",\n",
    "    \"Active_CD4\",\n",
    "    \"Naive_CD4\",\n",
    "    \"Memory_CD4\",\n",
    "    \"Memory_CD8\",\n",
    "    \"Anergic_CD8\",\n",
    "    \"Naive_CD8\",\n",
    "    \"Hyperactivated_CD8\",\n",
    "    \"Proliferating_Effector\",\n",
    "    \"CD8\"\n",
    "]\n",
    "\n",
    "# Cohorts\n",
    "cohorts = [\"control\", \"short_term\", \"long_term\"]\n",
    "\n",
    "# Ordered timepoints\n",
    "all_timepoints = [\"Pre\", \"C1\", \"C2\", \"C4\", \"C6\", \"C9\", \"C18\", \"C36\"]\n",
    "\n",
    "# Color mapping for cohorts\n",
    "cohort_colors = {\n",
    "    'control': 'yellow',\n",
    "    'short_term': 'blue',\n",
    "    'long_term': 'red'\n",
    "}\n",
    "\n",
    "# Desired minimum and maximum arrow lengths for visualization\n",
    "# min_arrow_length = 0.3  # Adjust as needed\n",
    "# max_arrow_length = 2    # Adjust as needed\n",
    "\n",
    "# Number of cells to sample for single-cell plotting (to reduce overcrowding)\n",
    "max_plot_cells = None  # will determine at runtime\n",
    "\n",
    "def optimal_transport_visualization(subpop_name, cohort_name):\n",
    "    input_folder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "    output_folder = os.path.join(base_output_dir, subpop_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    timepoint_folders = sorted([f for f in os.listdir(input_folder) if f.startswith(\"Timepoint_\")])\n",
    "    available_timepoints = [tp.split(\"_\")[1] for tp in timepoint_folders]\n",
    "    cohort_timepoints = [tp for tp in all_timepoints if tp in available_timepoints]\n",
    "    \n",
    "    if not cohort_timepoints:\n",
    "        print(f\"No timepoints available for {subpop_name} in {cohort_name} cohort.\")\n",
    "        return\n",
    "\n",
    "    # We'll store coordinates and also find global axis limits\n",
    "    all_x_coords = []\n",
    "    all_y_coords = []\n",
    "    \n",
    "    cell_counts_source = []\n",
    "    cell_counts_gray = []\n",
    "    full_data = {}  # store full data for each timepoint\n",
    "    \n",
    "    for tp in cohort_timepoints:\n",
    "        source_folder = os.path.join(input_folder, f\"Timepoint_{tp}\")\n",
    "        source_cells_file = os.path.join(source_folder, 'source_cells_new.csv')\n",
    "        gray_cells_file = os.path.join(source_folder, 'gray_cells_new.csv')\n",
    "        \n",
    "        if not (os.path.exists(source_cells_file) and os.path.exists(gray_cells_file)):\n",
    "            print(f\"Missing files for {tp}. Skipping this timepoint.\")\n",
    "            continue\n",
    "        \n",
    "        source_cells = pd.read_csv(source_cells_file)\n",
    "        gray_cells = pd.read_csv(gray_cells_file)\n",
    "        \n",
    "        if source_cells.empty:\n",
    "            print(f\"No source cells for {tp}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Store full data\n",
    "        full_data[tp] = {\n",
    "            'source': source_cells,\n",
    "            'gray': gray_cells\n",
    "        }\n",
    "        \n",
    "        # For axis limits\n",
    "        all_x_coords.extend(gray_cells['UMAP_2'])\n",
    "        all_x_coords.extend(source_cells['UMAP_2'])\n",
    "        all_y_coords.extend(gray_cells['UMAP_1'])\n",
    "        all_y_coords.extend(source_cells['UMAP_1'])\n",
    "        \n",
    "        cell_counts_source.append(len(source_cells))\n",
    "        cell_counts_gray.append(len(gray_cells))\n",
    "    \n",
    "    if not full_data:\n",
    "        print(\"No valid timepoints with data.\")\n",
    "        return\n",
    "    \n",
    "    x_min, x_max = min(all_x_coords), max(all_x_coords)\n",
    "    y_min, y_max = min(all_y_coords), max(all_y_coords)\n",
    "    \n",
    "    # Determine number of cells to plot for single-cell arrows (downsample for plotting only)\n",
    "    min_source_cells = min(cell_counts_source) if cell_counts_source else 0\n",
    "    min_gray_cells = min(cell_counts_gray) if cell_counts_gray else 0\n",
    "    if min_source_cells == 0 or min_gray_cells == 0:\n",
    "        print(\"Insufficient cells. Skipping visualization.\")\n",
    "        return\n",
    "    \n",
    "    timepoint_results = {}\n",
    "    \n",
    "    # Compute OT from each timepoint to the next (except the last one)\n",
    "    for i, source_tp in enumerate(cohort_timepoints):\n",
    "        source_data = full_data[source_tp]\n",
    "        source_cells = source_data['source']\n",
    "        \n",
    "        # Downsample for plotting single-cell arrows only\n",
    "        sampled_source_cells = source_cells.sample(n=min_source_cells, random_state=42)\n",
    "        sampled_gray_cells = full_data[source_tp]['gray'].sample(n=min_gray_cells, random_state=42)\n",
    "        \n",
    "        timepoint_results[source_tp] = {\n",
    "            'sampled_source': sampled_source_cells,\n",
    "            'sampled_gray': sampled_gray_cells\n",
    "        }\n",
    "        \n",
    "        if i < len(cohort_timepoints) - 1:\n",
    "            target_tp = cohort_timepoints[i+1]\n",
    "            target_data = full_data[target_tp]\n",
    "            target_cells = target_data['source']\n",
    "            \n",
    "            if target_cells.empty:\n",
    "                continue\n",
    "            \n",
    "            pc_cols = [c for c in source_cells.columns if c.startswith('PC_')]\n",
    "            full_source_coords_pc = source_cells[pc_cols].values\n",
    "            full_target_coords_pc = target_cells[pc_cols].values\n",
    "            \n",
    "            # Also store UMAP coordinates\n",
    "            full_source_coords_umap = source_cells[['UMAP_2', 'UMAP_1']].values\n",
    "            full_target_coords_umap = target_cells[['UMAP_2', 'UMAP_1']].values\n",
    "            \n",
    "            # Uniform distributions\n",
    "            a = np.ones((full_source_coords_pc.shape[0],)) / full_source_coords_pc.shape[0]\n",
    "            b = np.ones((full_target_coords_pc.shape[0],)) / full_target_coords_pc.shape[0]\n",
    "            \n",
    "            # Compute cost matrix\n",
    "            cost_matrix = ot.dist(full_source_coords_pc, full_target_coords_pc, metric='euclidean')\n",
    "            \n",
    "            try:\n",
    "                transport_plan = ot.emd(a, b, cost_matrix, numItermax=100000)\n",
    "            except Exception as e:\n",
    "                print(f\"OT computation failed for {source_tp} to {target_tp}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            full_target_indices = np.argmax(transport_plan, axis=1)\n",
    "            displacement_vectors_full = full_target_coords_umap[full_target_indices] - full_source_coords_umap\n",
    "            \n",
    "            # Compute norms\n",
    "            norms_full = np.linalg.norm(displacement_vectors_full, axis=1)\n",
    "            arrow_lengths_full = norms_full\n",
    "            # min_norm_full = np.min(norms_full)\n",
    "            # max_norm_full = np.max(norms_full)\n",
    "            \n",
    "            # if max_norm_full - min_norm_full > 0:\n",
    "            #     arrow_lengths_full = ((norms_full - min_norm_full) / (max_norm_full - min_norm_full)) * (max_arrow_length - min_arrow_length) + min_arrow_length\n",
    "            # else:\n",
    "            #     arrow_lengths_full = np.full_like(norms_full, min_arrow_length)\n",
    "            \n",
    "            # For single-cell plotting\n",
    "            sampled_source_indices = source_cells.index.get_indexer_for(sampled_source_cells.index)\n",
    "            sampled_displacements = displacement_vectors_full[sampled_source_indices, :]\n",
    "            sampled_norms = norms_full[sampled_source_indices]\n",
    "            arrow_lengths_sampled = sampled_norms\n",
    "            \n",
    "            # min_norm_sampled = np.min(sampled_norms)\n",
    "            # max_norm_sampled = np.max(sampled_norms)\n",
    "            # if max_norm_sampled - min_norm_sampled > 0:\n",
    "            #     arrow_lengths_sampled = ((sampled_norms - min_norm_sampled) / (max_norm_sampled - min_norm_sampled)) * (max_arrow_length - min_arrow_length) + min_arrow_length\n",
    "            # else:\n",
    "            #     arrow_lengths_sampled = np.full_like(sampled_norms, min_arrow_length)\n",
    "            \n",
    "            timepoint_results[source_tp]['single_cell_displacements'] = sampled_displacements\n",
    "            timepoint_results[source_tp]['single_cell_arrow_lengths'] = arrow_lengths_sampled\n",
    "            \n",
    "            clusters_of_interest = {1, 2, 3, 8, 10, 12, 14}\n",
    "\n",
    "            # Compute cluster-level arrows using full data\n",
    "            if 'seurat_clusters' in source_cells.columns:\n",
    "                source_cells['seurat_clusters'] = source_cells['seurat_clusters'].replace({16: 14, 17: 14})\n",
    "                source_clusters_full = source_cells['seurat_clusters'].values\n",
    "                df_cluster_full = pd.DataFrame({\n",
    "                    'cluster': source_clusters_full,\n",
    "                    'sx': full_source_coords_umap[:,0],\n",
    "                    'sy': full_source_coords_umap[:,1],\n",
    "                    'dx': displacement_vectors_full[:,0],\n",
    "                    'dy': displacement_vectors_full[:,1],\n",
    "                    'norm': norms_full\n",
    "                })\n",
    "                \n",
    "                gray_cells = full_data[source_tp]['gray']\n",
    "                all_cells_combined = pd.concat([source_cells, gray_cells])\n",
    "                all_cells_combined['seurat_clusters'] = all_cells_combined['seurat_clusters'].replace({16:14,17:14})\n",
    "                \n",
    "                all_clusters = all_cells_combined['seurat_clusters'].values\n",
    "                all_coords_umap = all_cells_combined[['UMAP_2', 'UMAP_1']].values\n",
    "                \n",
    "                df_all = pd.DataFrame({\n",
    "                    'cluster': all_clusters,\n",
    "                    'sx': all_coords_umap[:,0],\n",
    "                    'sy': all_coords_umap[:,1]\n",
    "                })\n",
    "                \n",
    "                # Compute centroids from all cells (source + gray)\n",
    "                all_group = df_all.groupby('cluster')\n",
    "                centroids = all_group[['sx','sy']].median()\n",
    "                \n",
    "                # Mean displacement from source cells only\n",
    "                group = df_cluster_full.groupby('cluster')\n",
    "                mean_disp = group[['dx','dy']].mean()\n",
    "            \n",
    "                cluster_norms = np.sqrt(mean_disp['dx']**2 + mean_disp['dy']**2)\n",
    "                cluster_arrow_lengths = cluster_norms\n",
    "                # cn_min = cluster_norms.min()\n",
    "                # cn_max = cluster_norms.max()\n",
    "                \n",
    "                # if cn_max - cn_min > 0:\n",
    "                #     cluster_arrow_lengths = ((cluster_norms - cn_min) / (cn_max - cn_min)) * (max_arrow_length - min_arrow_length) + min_arrow_length\n",
    "                # else:\n",
    "                #     cluster_arrow_lengths = pd.Series(np.full_like(cluster_norms, min_arrow_length), index=cluster_norms.index)\n",
    "                \n",
    "                cluster_arrows = []\n",
    "                common_clusters = centroids.index.intersection(mean_disp.index)\n",
    "                for clust in common_clusters:\n",
    "                    if clust in clusters_of_interest:\n",
    "                        cx, cy = centroids.loc[clust, ['sx','sy']]\n",
    "                        cdx, cdy = mean_disp.loc[clust, ['dx','dy']]\n",
    "                        cnorm = np.sqrt(cdx**2 + cdy**2)\n",
    "                        if cnorm > 0:\n",
    "                            cdx = cdx / cnorm\n",
    "                            cdy = cdy / cnorm\n",
    "                        else:\n",
    "                            cdx, cdy = 0, 0\n",
    "                        \n",
    "                        try:\n",
    "                            length = cluster_arrow_lengths.loc[clust]\n",
    "                        except AttributeError:\n",
    "                            if isinstance(cluster_arrow_lengths, np.ndarray):\n",
    "                                cluster_arrow_lengths = pd.Series(cluster_arrow_lengths, index=cluster_norms.index)\n",
    "                            try:\n",
    "                                length = cluster_arrow_lengths.loc[clust]\n",
    "                            except Exception as e:\n",
    "                                print(f\"Failed to access cluster_arrow_lengths for cluster {clust}: {e}\")\n",
    "                                length = min_arrow_length\n",
    "            \n",
    "                        cdx *= length\n",
    "                        cdy *= length\n",
    "                        # Store cluster ID alongside arrow coordinates for line width calculation later\n",
    "                        cluster_arrows.append((clust, cx, cy, cdx, cdy))\n",
    "                \n",
    "                timepoint_results[source_tp]['cluster_arrows'] = cluster_arrows\n",
    "\n",
    "                if len(cluster_arrows) > 0:\n",
    "                    # Instead of global centroid of all cells, use the median of source cells only:\n",
    "                    source_median_x = source_cells['UMAP_2'].median()\n",
    "                    source_median_y = source_cells['UMAP_1'].median()\n",
    "\n",
    "                    # Sum displacement vectors of all cluster arrows\n",
    "                    total_dx = sum([arrow[3] for arrow in cluster_arrows])\n",
    "                    total_dy = sum([arrow[4] for arrow in cluster_arrows])\n",
    "\n",
    "                    # Store the aggregated arrow\n",
    "                    timepoint_results[source_tp]['aggregated_arrow'] = (source_median_x, source_median_y, total_dx, total_dy)\n",
    "                else:\n",
    "                    # If no cluster arrows, no aggregated arrow\n",
    "                    timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "            else:\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "                print(f\"No 'seurat_clusters' column in {source_tp} source cells. Cannot compute cluster arrows.\")\n",
    "\n",
    "            \n",
    "        else:\n",
    "            # Last timepoint has no next timepoint\n",
    "            timepoint_results[source_tp]['single_cell_displacements'] = np.array([])\n",
    "            timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "            timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "            timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "            if 'seurat_clusters' in source_cells.columns:\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "            else:\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "    \n",
    "    # Plot single-cell arrows PDF\n",
    "    output_file_original = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_differential_arrow_lengths_equal_cells_using_PCs.pdf\")\n",
    "    with PdfPages(output_file_original) as pdf_original:\n",
    "        num_timepoints = len(cohort_timepoints)\n",
    "        fig_orig, axes_orig = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        if num_timepoints == 1:\n",
    "            axes_orig = [axes_orig]\n",
    "        \n",
    "        for i, source_tp in enumerate(cohort_timepoints):\n",
    "            ax = axes_orig[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            sampled_source_cells = res['sampled_source']\n",
    "            sampled_gray_cells = res['sampled_gray']\n",
    "            \n",
    "            gray_x = sampled_gray_cells['UMAP_2'].values\n",
    "            gray_y = sampled_gray_cells['UMAP_1'].values\n",
    "            source_x = sampled_source_cells['UMAP_2'].values\n",
    "            source_y = sampled_source_cells['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "            \n",
    "            single_cell_displacements = res['single_cell_displacements']\n",
    "            single_cell_arrow_lengths = res['single_cell_arrow_lengths']\n",
    "            \n",
    "            if len(single_cell_displacements) > 0:\n",
    "                source_coords_sampled = sampled_source_cells[['UMAP_2','UMAP_1']].values\n",
    "                sampled_norms = np.linalg.norm(single_cell_displacements, axis=1)\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    unit_vectors = single_cell_displacements / sampled_norms[:, np.newaxis]\n",
    "                    unit_vectors[~np.isfinite(unit_vectors)] = 0\n",
    "                scaled_vectors = unit_vectors * single_cell_arrow_lengths[:, np.newaxis]\n",
    "                \n",
    "                for j in range(len(source_coords_sampled)):\n",
    "                    sx, sy = source_coords_sampled[j]\n",
    "                    dx, dy = scaled_vectors[j]\n",
    "                    ax.arrow(sx, sy, dx, dy,\n",
    "                             color='black', alpha=0.7, head_width=0.05, head_length=0.05,\n",
    "                             length_includes_head=True, linewidth=0.5)\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_orig.suptitle(f\"{subpop_name} - {cohort_name}\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_original.savefig(fig_orig)\n",
    "        plt.close(fig_orig)\n",
    "    \n",
    "    # Plot aggregated cluster arrows in a new PDF\n",
    "    output_file_cluster = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_aggregated_cluster_arrows_equal_cells_using_PCs.pdf\")\n",
    "    color_mapping_file = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/Publication_Material/T_Cell_cluster_colors.csv\"  # Update with actual path\n",
    "    color_mapping_df = pd.read_csv(color_mapping_file)\n",
    "    cluster_colors_map = dict(zip(color_mapping_df['Cluster'], color_mapping_df['Color']))\n",
    "    with PdfPages(output_file_cluster) as pdf_cluster:\n",
    "        num_timepoints = len(cohort_timepoints)\n",
    "        fig_clust, axes_clust = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        if num_timepoints == 1:\n",
    "            axes_clust = [axes_clust]\n",
    "        \n",
    "        for i, source_tp in enumerate(cohort_timepoints):\n",
    "            ax = axes_clust[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            \n",
    "            gray_x = res['sampled_gray']['UMAP_2'].values\n",
    "            gray_y = res['sampled_gray']['UMAP_1'].values\n",
    "            source_x = res['sampled_source']['UMAP_2'].values\n",
    "            source_y = res['sampled_source']['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "            \n",
    "            cluster_arrows = res.get('cluster_arrows', [])\n",
    "            \n",
    "            # Compute source cluster proportions for line widths\n",
    "            source_cells = full_data[source_tp]['source']\n",
    "            source_cluster_counts = source_cells['seurat_clusters'].value_counts()\n",
    "            total_source_cells_current = len(source_cells)\n",
    "            \n",
    "            # Now each entry in cluster_arrows is (clust, cx, cy, cdx, cdy)\n",
    "            for (clust, cx, cy, cdx, cdy) in cluster_arrows:\n",
    "                # Get the color for this cluster, default to black if not found\n",
    "                arrow_color = cluster_colors_map.get(clust, 'black')\n",
    "                proportion = source_cluster_counts.get(clust, 0) / total_source_cells_current\n",
    "                line_width = 1.0 + 8.0 * proportion  # adjust scaling as needed\n",
    "\n",
    "                # Draw boundary arrow\n",
    "                boundary_color = 'black'\n",
    "                ax.arrow(cx, cy, cdx, cdy,\n",
    "                         color=boundary_color,\n",
    "                         alpha=1,\n",
    "                         head_width=0.2 + 0.5 * proportion,\n",
    "                         head_length=0.1 + 0.1 * proportion,\n",
    "                         length_includes_head=True,\n",
    "                         linewidth=line_width + 2)  # Thicker for boundary\n",
    "\n",
    "                # Draw main arrow on top\n",
    "                ax.arrow(cx, cy, cdx, cdy,\n",
    "                         color=arrow_color, alpha=1,\n",
    "                         head_width=0.2+0.5*proportion, head_length=0.1+0.1*proportion,   # bigger arrowhead\n",
    "                         length_includes_head=True, linewidth=line_width)\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_clust.suptitle(f\"{subpop_name} - {cohort_name} (Cluster-Level Arrows)\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_cluster.savefig(fig_clust)\n",
    "        plt.close(fig_clust)\n",
    "\n",
    "    # Plot single aggregated arrow in a new PDF\n",
    "    output_file_single_agg = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_aggregated_single_arrow_equal_cells_using_PCs.pdf\")\n",
    "    with PdfPages(output_file_single_agg) as pdf_agg:\n",
    "        num_timepoints = len(cohort_timepoints)\n",
    "        fig_agg, axes_agg = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        if num_timepoints == 1:\n",
    "            axes_agg = [axes_agg]\n",
    "\n",
    "        for i, source_tp in enumerate(cohort_timepoints):\n",
    "            ax = axes_agg[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "\n",
    "            gray_x = res['sampled_gray']['UMAP_2'].values\n",
    "            gray_y = res['sampled_gray']['UMAP_1'].values\n",
    "            source_x = res['sampled_source']['UMAP_2'].values\n",
    "            source_y = res['sampled_source']['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "\n",
    "            aggregated_arrow = res.get('aggregated_arrow', None)\n",
    "            if aggregated_arrow is not None:\n",
    "                global_cx, global_cy, total_dx, total_dy = aggregated_arrow\n",
    "                # Just draw one large arrow\n",
    "                # Use a fixed linewidth and arrowhead for clarity\n",
    "                ax.arrow(global_cx, global_cy, total_dx, total_dy,\n",
    "                         color='black', alpha=0.9,\n",
    "                         head_width=0.3, head_length=0.3,\n",
    "                         length_includes_head=True, linewidth=2.0)\n",
    "\n",
    "            ax.set_title(f\"Timepoint: {source_tp} (Single Aggregated Arrow)\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        fig_agg.suptitle(f\"{subpop_name} - {cohort_name} (Single Aggregated Arrow)\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_agg.savefig(fig_agg)\n",
    "        plt.close(fig_agg)\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution Loop\n",
    "# ---------------------------\n",
    "\n",
    "for subpop_name in subpopulations:\n",
    "    for cohort_name in cohorts:\n",
    "        print(f\"Processing {subpop_name} - {cohort_name}\")\n",
    "        input_subfolder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "        if not os.path.exists(input_subfolder):\n",
    "            print(f\"Input subfolder does not exist: {input_subfolder}. Skipping.\")\n",
    "            continue\n",
    "        optimal_transport_visualization(subpop_name, cohort_name)\n",
    "\n",
    "print(\"Visualization generation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "986c8b4a-9403-488c-9849-5d33c594ed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# including new visualization (target distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bda6076c-5278-47d5-80b6-af6628287bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Activated_CD4 - control\n",
      "Processing Activated_CD4 - short_term\n",
      "Processing Activated_CD4 - long_term\n",
      "Processing Effector_CD8 - control\n",
      "Processing Effector_CD8 - short_term\n",
      "Processing Effector_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_Precursor_Effector_CD8 - control\n",
      "Processing Memory_Precursor_Effector_CD8 - short_term\n",
      "Processing Memory_Precursor_Effector_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Exhausted_T - control\n",
      "Processing Exhausted_T - short_term\n",
      "Processing Exhausted_T - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Gamma_Delta_T - control\n",
      "Processing Gamma_Delta_T - short_term\n",
      "Processing Gamma_Delta_T - long_term\n",
      "Processing Active_CD4 - control\n",
      "Processing Active_CD4 - short_term\n",
      "Processing Active_CD4 - long_term\n",
      "Processing Naive_CD4 - control\n",
      "Processing Naive_CD4 - short_term\n",
      "Processing Naive_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD4 - control\n",
      "Processing Memory_CD4 - short_term\n",
      "Processing Memory_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Central_Memory_CD8 - control\n",
      "Processing Central_Memory_CD8 - short_term\n",
      "Processing Central_Memory_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Stem_Like_CD8 - control\n",
      "Processing Stem_Like_CD8 - short_term\n",
      "Processing Stem_Like_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Effector_Memory_CD8 - control\n",
      "Processing Effector_Memory_CD8 - short_term\n",
      "Processing Effector_Memory_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Proliferating_Effector - control\n",
      "Processing Proliferating_Effector - short_term\n",
      "Processing Proliferating_Effector - long_term\n",
      "Processing CD8 - control\n",
      "Processing CD8 - short_term\n",
      "Processing CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization generation completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ot\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration Parameters\n",
    "# ---------------------------\n",
    "\n",
    "base_input_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/final_nomenclature\"\n",
    "base_output_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/final_nomenclature\"\n",
    "\n",
    "subpopulations = [\n",
    "    \"Activated_CD4\",\n",
    "    \"Effector_CD8\",\n",
    "    \"Memory_Precursor_Effector_CD8\",\n",
    "    \"Exhausted_T\",\n",
    "    \"Gamma_Delta_T\",\n",
    "    \"Active_CD4\",\n",
    "    \"Naive_CD4\",\n",
    "    \"Memory_CD4\",\n",
    "    \"Central_Memory_CD8\",\n",
    "    \"Stem_Like_CD8\",\n",
    "    \"Effector_Memory_CD8\",\n",
    "    \"Proliferating_Effector\",\n",
    "    \"CD8\"\n",
    "]\n",
    "\n",
    "cohorts = [\"control\", \"short_term\", \"long_term\"]\n",
    "\n",
    "all_timepoints = [\"Pre\", \"C1\", \"C2\", \"C4\", \"C6\", \"C9\", \"C18\", \"C36\"]\n",
    "\n",
    "cohort_colors = {\n",
    "    'control': 'yellow',\n",
    "    'short_term': 'blue',\n",
    "    'long_term': 'red'\n",
    "}\n",
    "\n",
    "clusters_of_interest = [1, 2, 3, 8, 10, 12, 14]\n",
    "\n",
    "color_mapping_file = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/Publication_Material/T_Cell_cluster_colors.csv\"\n",
    "color_mapping_df = pd.read_csv(color_mapping_file)\n",
    "cluster_colors_map = dict(zip(color_mapping_df['Cluster'], color_mapping_df['Color']))\n",
    "cluster_celltype_map = dict(zip(color_mapping_df['Cluster'], color_mapping_df['Celltype']))\n",
    "\n",
    "def optimal_transport_visualization(subpop_name, cohort_name):\n",
    "    input_folder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "    output_folder = os.path.join(base_output_dir, subpop_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return None\n",
    "    \n",
    "    timepoint_folders = sorted([f for f in os.listdir(input_folder) if f.startswith(\"Timepoint_\")])\n",
    "    available_timepoints = [tp.split(\"_\")[1] for tp in timepoint_folders]\n",
    "    cohort_timepoints = [tp for tp in all_timepoints if tp in available_timepoints]\n",
    "    \n",
    "    if not cohort_timepoints:\n",
    "        print(f\"No timepoints available for {subpop_name} in {cohort_name} cohort.\")\n",
    "        return None\n",
    "\n",
    "    all_x_coords = []\n",
    "    all_y_coords = []\n",
    "    \n",
    "    cell_counts_source = []\n",
    "    cell_counts_gray = []\n",
    "    full_data = {}\n",
    "    \n",
    "    for tp in cohort_timepoints:\n",
    "        source_folder = os.path.join(input_folder, f\"Timepoint_{tp}\")\n",
    "        source_cells_file = os.path.join(source_folder, 'source_cells_new.csv')\n",
    "        gray_cells_file = os.path.join(source_folder, 'gray_cells_new.csv')\n",
    "        \n",
    "        if not (os.path.exists(source_cells_file) and os.path.exists(gray_cells_file)):\n",
    "            print(f\"Missing files for {tp}. Skipping this timepoint.\")\n",
    "            continue\n",
    "        \n",
    "        source_cells = pd.read_csv(source_cells_file)\n",
    "        gray_cells = pd.read_csv(gray_cells_file)\n",
    "        \n",
    "        if source_cells.empty:\n",
    "            print(f\"No source cells for {tp}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        full_data[tp] = {\n",
    "            'source': source_cells,\n",
    "            'gray': gray_cells\n",
    "        }\n",
    "        \n",
    "        all_x_coords.extend(gray_cells['UMAP_2'])\n",
    "        all_x_coords.extend(source_cells['UMAP_2'])\n",
    "        all_y_coords.extend(gray_cells['UMAP_1'])\n",
    "        all_y_coords.extend(source_cells['UMAP_1'])\n",
    "        \n",
    "        cell_counts_source.append(len(source_cells))\n",
    "        cell_counts_gray.append(len(gray_cells))\n",
    "    \n",
    "    if not full_data:\n",
    "        print(\"No valid timepoints with data.\")\n",
    "        return None\n",
    "    \n",
    "    x_min, x_max = min(all_x_coords), max(all_x_coords)\n",
    "    y_min, y_max = min(all_y_coords), max(all_y_coords)\n",
    "    \n",
    "    min_source_cells = min(cell_counts_source) if cell_counts_source else 0\n",
    "    min_gray_cells = min(cell_counts_gray) if cell_counts_gray else 0\n",
    "    if min_source_cells == 0 or min_gray_cells == 0:\n",
    "        print(\"Insufficient cells. Skipping visualization.\")\n",
    "        return None\n",
    "    \n",
    "    timepoint_results = {}\n",
    "    distributions_coi = {tp: {cohort_name: {c: {} for c in clusters_of_interest}} for tp in cohort_timepoints}\n",
    "    \n",
    "    for i, source_tp in enumerate(cohort_timepoints):\n",
    "        source_data = full_data[source_tp]\n",
    "        source_cells = source_data['source']\n",
    "        \n",
    "        sampled_source_cells = source_cells.sample(n=min_source_cells, random_state=42)\n",
    "        sampled_gray_cells = full_data[source_tp]['gray'].sample(n=min_gray_cells, random_state=42)\n",
    "        \n",
    "        timepoint_results[source_tp] = {\n",
    "            'sampled_source': sampled_source_cells,\n",
    "            'sampled_gray': sampled_gray_cells\n",
    "        }\n",
    "        \n",
    "        if i < len(cohort_timepoints) - 1:\n",
    "            target_tp = cohort_timepoints[i+1]\n",
    "            target_data = full_data[target_tp]\n",
    "            target_cells = target_data['source']\n",
    "            \n",
    "            if target_cells.empty:\n",
    "                timepoint_results[source_tp]['single_cell_displacements'] = np.array([])\n",
    "                timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "            else:\n",
    "                pc_cols = [c for c in source_cells.columns if c.startswith('PC_')]\n",
    "                full_source_coords_pc = source_cells[pc_cols].values\n",
    "                full_target_coords_pc = target_cells[pc_cols].values\n",
    "                \n",
    "                full_source_coords_umap = source_cells[['UMAP_2', 'UMAP_1']].values\n",
    "                full_target_coords_umap = target_cells[['UMAP_2', 'UMAP_1']].values\n",
    "                \n",
    "                a = np.ones((full_source_coords_pc.shape[0],)) / full_source_coords_pc.shape[0]\n",
    "                b = np.ones((full_target_coords_pc.shape[0],)) / full_target_coords_pc.shape[0]\n",
    "                \n",
    "                cost_matrix = ot.dist(full_source_coords_pc, full_target_coords_pc, metric='euclidean')\n",
    "                \n",
    "                try:\n",
    "                    transport_plan = ot.emd(a, b, cost_matrix, numItermax=100000)\n",
    "                except Exception as e:\n",
    "                    print(f\"OT computation failed for {source_tp} to {target_tp}: {e}\")\n",
    "                    timepoint_results[source_tp]['single_cell_displacements'] = np.array([])\n",
    "                    timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "                    timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                    timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "                    # no distributions_coi update in case of failure\n",
    "                    continue\n",
    "                \n",
    "                full_target_indices = np.argmax(transport_plan, axis=1)\n",
    "                displacement_vectors_full = full_target_coords_umap[full_target_indices] - full_source_coords_umap\n",
    "                \n",
    "                norms_full = np.linalg.norm(displacement_vectors_full, axis=1)\n",
    "                arrow_lengths_full = norms_full\n",
    "                \n",
    "                sampled_source_indices = source_cells.index.get_indexer_for(sampled_source_cells.index)\n",
    "                sampled_displacements = displacement_vectors_full[sampled_source_indices, :]\n",
    "                sampled_norms = norms_full[sampled_source_indices]\n",
    "                \n",
    "                arrow_lengths_sampled = sampled_norms\n",
    "                \n",
    "                timepoint_results[source_tp]['single_cell_displacements'] = sampled_displacements\n",
    "                timepoint_results[source_tp]['single_cell_arrow_lengths'] = arrow_lengths_sampled\n",
    "                \n",
    "                if 'seurat_clusters' in source_cells.columns and 'seurat_clusters' in target_cells.columns:\n",
    "                    source_cells['seurat_clusters'] = source_cells['seurat_clusters'].replace({16:14,17:14})\n",
    "                    target_cells['seurat_clusters'] = target_cells['seurat_clusters'].replace({16:14,17:14})\n",
    "                    source_cells['seurat_clusters'] = source_cells['seurat_clusters'].replace({9:6,18:6})\n",
    "                    target_cells['seurat_clusters'] = target_cells['seurat_clusters'].replace({9:6,18:6})\n",
    "                    \n",
    "                    # Cluster-level arrows\n",
    "                    source_clusters_full = source_cells['seurat_clusters'].values\n",
    "                    df_cluster_full = pd.DataFrame({\n",
    "                        'cluster': source_clusters_full,\n",
    "                        'sx': full_source_coords_umap[:,0],\n",
    "                        'sy': full_source_coords_umap[:,1],\n",
    "                        'dx': displacement_vectors_full[:,0],\n",
    "                        'dy': displacement_vectors_full[:,1],\n",
    "                        'norm': norms_full\n",
    "                    })\n",
    "                    \n",
    "                    gray_cells = full_data[source_tp]['gray']\n",
    "                    all_cells_combined = pd.concat([source_cells, gray_cells])\n",
    "                    all_cells_combined['seurat_clusters'] = all_cells_combined['seurat_clusters'].replace({16:14,17:14})\n",
    "                    all_cells_combined['seurat_clusters'] = all_cells_combined['seurat_clusters'].replace({9:6,18:6})\n",
    "                    \n",
    "                    all_clusters = all_cells_combined['seurat_clusters'].values\n",
    "                    all_coords_umap = all_cells_combined[['UMAP_2', 'UMAP_1']].values\n",
    "                    \n",
    "                    df_all = pd.DataFrame({\n",
    "                        'cluster': all_clusters,\n",
    "                        'sx': all_coords_umap[:,0],\n",
    "                        'sy': all_coords_umap[:,1]\n",
    "                    })\n",
    "                    \n",
    "                    all_group = df_all.groupby('cluster')\n",
    "                    centroids = all_group[['sx','sy']].median()\n",
    "                    \n",
    "                    group = df_cluster_full.groupby('cluster')\n",
    "                    mean_disp = group[['dx','dy']].mean()\n",
    "                \n",
    "                    cluster_norms = np.sqrt(mean_disp['dx']**2 + mean_disp['dy']**2)\n",
    "                    cluster_arrow_lengths = cluster_norms\n",
    "                    \n",
    "                    cluster_arrows = []\n",
    "                    common_clusters = centroids.index.intersection(mean_disp.index)\n",
    "                    for clust in common_clusters:\n",
    "                        if clust in clusters_of_interest:\n",
    "                            cx, cy = centroids.loc[clust, ['sx','sy']]\n",
    "                            cdx, cdy = mean_disp.loc[clust, ['dx','dy']]\n",
    "                            cnorm = np.sqrt(cdx**2 + cdy**2)\n",
    "                            if cnorm > 0:\n",
    "                                cdx /= cnorm\n",
    "                                cdy /= cnorm\n",
    "                            \n",
    "                            length = cluster_arrow_lengths.loc[clust]\n",
    "                            cdx *= length\n",
    "                            cdy *= length\n",
    "                            cluster_arrows.append((clust, cx, cy, cdx, cdy))\n",
    "                    \n",
    "                    timepoint_results[source_tp]['cluster_arrows'] = cluster_arrows\n",
    "    \n",
    "                    if len(cluster_arrows) > 0:\n",
    "                        source_median_x = source_cells['UMAP_2'].median()\n",
    "                        source_median_y = source_cells['UMAP_1'].median()\n",
    "                        total_dx = sum([arrow[3] for arrow in cluster_arrows])\n",
    "                        total_dy = sum([arrow[4] for arrow in cluster_arrows])\n",
    "                        timepoint_results[source_tp]['aggregated_arrow'] = (source_median_x, source_median_y, total_dx, total_dy)\n",
    "                    else:\n",
    "                        timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "    \n",
    "                    # Compute distributions for each cluster_of_interest\n",
    "                    for coi in clusters_of_interest:\n",
    "                        coi_mask = (source_cells['seurat_clusters'] == coi)\n",
    "                        if np.any(coi_mask):\n",
    "                            selected_target_indices = full_target_indices[coi_mask]\n",
    "                            selected_target_clusters = target_cells['seurat_clusters'].iloc[selected_target_indices].values\n",
    "                            unique_tclusters, counts = np.unique(selected_target_clusters, return_counts=True)\n",
    "                            total_count = counts.sum()\n",
    "                            if total_count > 0:\n",
    "                                fraction_dict = {int(tc): (ct / total_count) for tc, ct in zip(unique_tclusters, counts)}\n",
    "                            else:\n",
    "                                fraction_dict = {}\n",
    "                            distributions_coi[source_tp][cohort_name][coi] = fraction_dict\n",
    "                        else:\n",
    "                            distributions_coi[source_tp][cohort_name][coi] = {}\n",
    "                else:\n",
    "                    timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                    timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "                    for coi in clusters_of_interest:\n",
    "                        distributions_coi[source_tp][cohort_name][coi] = {}\n",
    "        else:\n",
    "            # Last timepoint has no next timepoint\n",
    "            timepoint_results[source_tp]['single_cell_displacements'] = np.array([])\n",
    "            timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "            timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "            timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "            for coi in clusters_of_interest:\n",
    "                distributions_coi[source_tp][cohort_name][coi] = {}\n",
    "\n",
    "    # ---------------------------\n",
    "    # Generate Original PDFs\n",
    "    # ---------------------------\n",
    "    # Single-cell arrows PDF\n",
    "    output_file_original = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_differential_arrow_lengths_equal_cells_using_PCs.pdf\")\n",
    "    with PdfPages(output_file_original) as pdf_original:\n",
    "        plot_tps = list(timepoint_results.keys())\n",
    "        num_timepoints = len(plot_tps)\n",
    "        fig_orig, axes_orig = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        if num_timepoints == 1:\n",
    "            axes_orig = [axes_orig]\n",
    "        \n",
    "        for i, source_tp in enumerate(plot_tps):\n",
    "            ax = axes_orig[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            sampled_source_cells = res['sampled_source']\n",
    "            sampled_gray_cells = res['sampled_gray']\n",
    "            \n",
    "            gray_x = sampled_gray_cells['UMAP_2'].values\n",
    "            gray_y = sampled_gray_cells['UMAP_1'].values\n",
    "            source_x = sampled_source_cells['UMAP_2'].values\n",
    "            source_y = sampled_source_cells['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "            \n",
    "            single_cell_displacements = res['single_cell_displacements']\n",
    "            single_cell_arrow_lengths = res['single_cell_arrow_lengths']\n",
    "            \n",
    "            if len(single_cell_displacements) > 0:\n",
    "                source_coords_sampled = sampled_source_cells[['UMAP_2','UMAP_1']].values\n",
    "                sampled_norms = np.linalg.norm(single_cell_displacements, axis=1)\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    unit_vectors = single_cell_displacements / sampled_norms[:, np.newaxis]\n",
    "                    unit_vectors[~np.isfinite(unit_vectors)] = 0\n",
    "                scaled_vectors = unit_vectors * single_cell_arrow_lengths[:, np.newaxis]\n",
    "                \n",
    "                for j in range(len(source_coords_sampled)):\n",
    "                    sx, sy = source_coords_sampled[j]\n",
    "                    dx, dy = scaled_vectors[j]\n",
    "                    ax.arrow(sx, sy, dx, dy,\n",
    "                             color='black', alpha=0.7, head_width=0.05, head_length=0.05,\n",
    "                             length_includes_head=True, linewidth=0.5)\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_orig.suptitle(f\"{subpop_name} - {cohort_name}\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_original.savefig(fig_orig)\n",
    "        plt.close(fig_orig)\n",
    "    \n",
    "    # Cluster-level arrows PDF\n",
    "    output_file_cluster = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_aggregated_cluster_arrows_equal_cells_using_PCs.pdf\")\n",
    "    with PdfPages(output_file_cluster) as pdf_cluster:\n",
    "        plot_tps = list(timepoint_results.keys())\n",
    "        num_timepoints = len(plot_tps)\n",
    "        fig_clust, axes_clust = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        if num_timepoints == 1:\n",
    "            axes_clust = [axes_clust]\n",
    "        \n",
    "        for i, source_tp in enumerate(plot_tps):\n",
    "            ax = axes_clust[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            \n",
    "            gray_x = res['sampled_gray']['UMAP_2'].values\n",
    "            gray_y = res['sampled_gray']['UMAP_1'].values\n",
    "            source_x = res['sampled_source']['UMAP_2'].values\n",
    "            source_y = res['sampled_source']['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "            \n",
    "            cluster_arrows = res.get('cluster_arrows', [])\n",
    "            \n",
    "            source_cells = full_data[source_tp]['source']\n",
    "            if 'seurat_clusters' in source_cells.columns:\n",
    "                source_cluster_counts = source_cells['seurat_clusters'].value_counts()\n",
    "                total_source_cells_current = len(source_cells)\n",
    "            else:\n",
    "                source_cluster_counts = pd.Series()\n",
    "                total_source_cells_current = 1\n",
    "            \n",
    "            for (clust, cx, cy, cdx, cdy) in cluster_arrows:\n",
    "                arrow_color = cluster_colors_map.get(clust, 'black')\n",
    "                proportion = source_cluster_counts.get(clust, 0) / total_source_cells_current\n",
    "                line_width = 1.0 + 8.0 * proportion\n",
    "\n",
    "                # Draw boundary arrow\n",
    "                ax.arrow(cx, cy, cdx, cdy,\n",
    "                         color='black',\n",
    "                         alpha=1,\n",
    "                         head_width=0.2 + 0.5 * proportion,\n",
    "                         head_length=0.1 + 0.1 * proportion,\n",
    "                         length_includes_head=True,\n",
    "                         linewidth=line_width + 2)\n",
    "                \n",
    "                # Draw main arrow on top\n",
    "                ax.arrow(cx, cy, cdx, cdy,\n",
    "                         color=arrow_color, alpha=1,\n",
    "                         head_width=0.2+0.5*proportion, head_length=0.1+0.1*proportion,\n",
    "                         length_includes_head=True, linewidth=line_width)\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_clust.suptitle(f\"{subpop_name} - {cohort_name} (Cluster-Level Arrows)\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_cluster.savefig(fig_clust)\n",
    "        plt.close(fig_clust)\n",
    "\n",
    "    # Single aggregated arrow PDF\n",
    "    output_file_single_agg = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_aggregated_single_arrow_equal_cells_using_PCs.pdf\")\n",
    "    with PdfPages(output_file_single_agg) as pdf_agg:\n",
    "        plot_tps = list(timepoint_results.keys())\n",
    "        num_timepoints = len(plot_tps)\n",
    "        fig_agg, axes_agg = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        if num_timepoints == 1:\n",
    "            axes_agg = [axes_agg]\n",
    "\n",
    "        for i, source_tp in enumerate(plot_tps):\n",
    "            ax = axes_agg[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "\n",
    "            gray_x = res['sampled_gray']['UMAP_2'].values\n",
    "            gray_y = res['sampled_gray']['UMAP_1'].values\n",
    "            source_x = res['sampled_source']['UMAP_2'].values\n",
    "            source_y = res['sampled_source']['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "\n",
    "            aggregated_arrow = res.get('aggregated_arrow', None)\n",
    "            if aggregated_arrow is not None:\n",
    "                global_cx, global_cy, total_dx, total_dy = aggregated_arrow\n",
    "                ax.arrow(global_cx, global_cy, total_dx, total_dy,\n",
    "                         color='black', alpha=0.9,\n",
    "                         head_width=0.3, head_length=0.3,\n",
    "                         length_includes_head=True, linewidth=2.0)\n",
    "\n",
    "            ax.set_title(f\"Timepoint: {source_tp} (Single Aggregated Arrow)\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        fig_agg.suptitle(f\"{subpop_name} - {cohort_name} (Single Aggregated Arrow)\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_agg.savefig(fig_agg)\n",
    "        plt.close(fig_agg)\n",
    "    \n",
    "    # Return the distributions for stacked bar chart plotting done outside\n",
    "    return distributions_coi\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution Loop\n",
    "# ---------------------------\n",
    "\n",
    "for subpop_name in subpopulations:\n",
    "    distributions_coi_all = {tp: {c: {coi: {} for coi in clusters_of_interest} for c in cohorts} for tp in all_timepoints}\n",
    "    \n",
    "    for cohort_name in cohorts:\n",
    "        print(f\"Processing {subpop_name} - {cohort_name}\")\n",
    "        input_subfolder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "        if not os.path.exists(input_subfolder):\n",
    "            print(f\"Input subfolder does not exist: {input_subfolder}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        dist_coi = optimal_transport_visualization(subpop_name, cohort_name)\n",
    "        if dist_coi is not None:\n",
    "            for tp in dist_coi:\n",
    "                for c in dist_coi[tp]:\n",
    "                    for coi in dist_coi[tp][c]:\n",
    "                        distributions_coi_all[tp][c][coi] = dist_coi[tp][c][coi]\n",
    "\n",
    "    # Determine all target clusters that appear\n",
    "    all_target_clusters = set()\n",
    "    for tp in distributions_coi_all:\n",
    "        for c in cohorts:\n",
    "            for coi in clusters_of_interest:\n",
    "                all_target_clusters.update(distributions_coi_all[tp][c][coi].keys())\n",
    "    all_target_clusters = sorted(all_target_clusters)\n",
    "\n",
    "    def get_celltype_name(clust_id):\n",
    "        return cluster_celltype_map.get(clust_id, f\"Cluster {clust_id}\")\n",
    "\n",
    "    def get_cluster_color(clust_id):\n",
    "        return cluster_colors_map.get(clust_id, 'gray')\n",
    "\n",
    "    # Generate the stacked bar chart PDF\n",
    "    output_file_stacked = os.path.join(base_output_dir, f\"{subpop_name}_target_cluster_distribution_coi.pdf\")\n",
    "    with PdfPages(output_file_stacked) as pdf_dist:\n",
    "        fig, axes = plt.subplots(len(clusters_of_interest), len(all_timepoints), \n",
    "                                 figsize=(4*len(all_timepoints), 3*len(clusters_of_interest)), \n",
    "                                 sharex=False, sharey=True)\n",
    "        \n",
    "        if len(clusters_of_interest) == 1 and len(all_timepoints) == 1:\n",
    "            axes = np.array([[axes]])\n",
    "        elif len(clusters_of_interest) == 1:\n",
    "            axes = axes[np.newaxis, :]\n",
    "        elif len(all_timepoints) == 1:\n",
    "            axes = axes[:, np.newaxis]\n",
    "\n",
    "        # Plot bars\n",
    "        for row_i, coi in enumerate(clusters_of_interest):\n",
    "            for col_i, tp in enumerate(all_timepoints):\n",
    "                ax = axes[row_i, col_i]\n",
    "                bar_positions = np.arange(len(cohorts))\n",
    "                cohort_distributions = [distributions_coi_all[tp][c][coi] for c in cohorts]\n",
    "                bottoms = np.zeros(len(cohorts))\n",
    "\n",
    "                stack_data = []\n",
    "                for tc in all_target_clusters:\n",
    "                    heights = [d.get(tc,0) for d in cohort_distributions]\n",
    "                    stack_data.append((tc, heights))\n",
    "        \n",
    "                # Sort by sum of fractions\n",
    "                stack_data.sort(key=lambda x: sum(x[1]), reverse=True)\n",
    "                \n",
    "                for (tc, h) in stack_data:\n",
    "                    color = get_cluster_color(tc)\n",
    "                    ax.bar(bar_positions, h, bottom=bottoms, color=color, edgecolor='black')\n",
    "                    # Add labels\n",
    "                    for idx, val in enumerate(h):\n",
    "                        if val > 0.05:\n",
    "                            mid_y = bottoms[idx] + val/2\n",
    "                            ax.text(bar_positions[idx], mid_y, get_celltype_name(tc),\n",
    "                                    ha='center', va='center', fontsize=6, color='white')\n",
    "                    bottoms += h\n",
    "\n",
    "                if row_i == 0:\n",
    "                    ax.set_title(f\"{tp}\", fontsize=10)\n",
    "\n",
    "                ax.set_xticks(bar_positions)\n",
    "                ax.set_xticklabels(cohorts, rotation=45, ha='right', fontsize=8)\n",
    "                ax.set_ylim(0,1)\n",
    "\n",
    "        fig.suptitle(f\"Distribution of Target Clusters per COI - {subpop_name}\", fontsize=16)\n",
    "\n",
    "        # Apply tight layout first\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Add space for legend and row labels\n",
    "        fig.subplots_adjust(left=0.15, top=0.88)  \n",
    "        \n",
    "        # Label rows with celltype of the COI\n",
    "        n_rows = len(clusters_of_interest)\n",
    "        for row_i, coi in enumerate(clusters_of_interest):\n",
    "            y_pos = 0.88 - (row_i + 0.5)*(0.88-0.1)/n_rows\n",
    "            fig.text(0.05, y_pos, get_celltype_name(coi), va='center', ha='right', fontsize=10, color='black')\n",
    "\n",
    "        # Create legend patches with celltype names for target clusters\n",
    "        legend_patches = []\n",
    "        for tc in all_target_clusters:\n",
    "            legend_patches.append(plt.Rectangle((0,0),1,1,\n",
    "                                                facecolor=get_cluster_color(tc),\n",
    "                                                edgecolor='black',\n",
    "                                                label=get_celltype_name(tc)))\n",
    "        \n",
    "        # Place legend at top center\n",
    "        fig.legend(handles=legend_patches, loc='upper center', bbox_to_anchor=(0.5, 0.98), \n",
    "                   ncol=len(all_target_clusters), fontsize=8, title=\"Target Celltypes\")\n",
    "\n",
    "        pdf_dist.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "print(\"Visualization generation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50a5c63b-a8ce-43e5-aa8a-05ce806100bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# December 23\n",
    "# March 21 - changed cluster specific arrow color to black color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3946f0bc-5ff3-4c0e-bf40-2333bb014a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Activated_CD4 - control\n",
      "Processing Activated_CD4 - short_term\n",
      "Processing Activated_CD4 - long_term\n",
      "Processing Effector_CD8 - control\n",
      "Processing Effector_CD8 - short_term\n",
      "Processing Effector_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/dtran642_927/SonLe/USC_Source/source/miniconda3/envs/vef_env/lib/python3.9/site-packages/ot/lp/__init__.py:388: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_Precursor_Effector_CD8 - control\n",
      "Processing Memory_Precursor_Effector_CD8 - short_term\n",
      "Processing Memory_Precursor_Effector_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/dtran642_927/SonLe/USC_Source/source/miniconda3/envs/vef_env/lib/python3.9/site-packages/ot/lp/__init__.py:388: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Exhausted_T - control\n",
      "Processing Exhausted_T - short_term\n",
      "Processing Exhausted_T - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/dtran642_927/SonLe/USC_Source/source/miniconda3/envs/vef_env/lib/python3.9/site-packages/ot/lp/__init__.py:388: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/project/dtran642_927/SonLe/USC_Source/source/miniconda3/envs/vef_env/lib/python3.9/site-packages/ot/lp/__init__.py:388: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Gamma_Delta_T - control\n",
      "Processing Gamma_Delta_T - short_term\n",
      "Processing Gamma_Delta_T - long_term\n",
      "Processing Active_CD4 - control\n",
      "Processing Active_CD4 - short_term\n",
      "Processing Active_CD4 - long_term\n",
      "Processing Naive_CD4 - control\n",
      "Processing Naive_CD4 - short_term\n",
      "Processing Naive_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/dtran642_927/SonLe/USC_Source/source/miniconda3/envs/vef_env/lib/python3.9/site-packages/ot/lp/__init__.py:388: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/project/dtran642_927/SonLe/USC_Source/source/miniconda3/envs/vef_env/lib/python3.9/site-packages/ot/lp/__init__.py:388: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD4 - control\n",
      "Processing Memory_CD4 - short_term\n",
      "Processing Memory_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/dtran642_927/SonLe/USC_Source/source/miniconda3/envs/vef_env/lib/python3.9/site-packages/ot/lp/__init__.py:388: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/project/dtran642_927/SonLe/USC_Source/source/miniconda3/envs/vef_env/lib/python3.9/site-packages/ot/lp/__init__.py:388: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Central_Memory_CD8 - control\n",
      "Processing Central_Memory_CD8 - short_term\n",
      "Processing Central_Memory_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/dtran642_927/SonLe/USC_Source/source/miniconda3/envs/vef_env/lib/python3.9/site-packages/ot/lp/__init__.py:388: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/project/dtran642_927/SonLe/USC_Source/source/miniconda3/envs/vef_env/lib/python3.9/site-packages/ot/lp/__init__.py:388: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Stem_Like_CD8 - control\n",
      "Processing Stem_Like_CD8 - short_term\n",
      "Processing Stem_Like_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/dtran642_927/SonLe/USC_Source/source/miniconda3/envs/vef_env/lib/python3.9/site-packages/ot/lp/__init__.py:388: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/project/dtran642_927/SonLe/USC_Source/source/miniconda3/envs/vef_env/lib/python3.9/site-packages/ot/lp/__init__.py:388: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Effector_Memory_CD8 - control\n",
      "Processing Effector_Memory_CD8 - short_term\n",
      "Processing Effector_Memory_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/dtran642_927/SonLe/USC_Source/source/miniconda3/envs/vef_env/lib/python3.9/site-packages/ot/lp/__init__.py:388: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Proliferating_Effector - control\n",
      "Processing Proliferating_Effector - short_term\n",
      "Processing Proliferating_Effector - long_term\n",
      "Processing CD8 - control\n",
      "Processing CD8 - short_term\n",
      "Processing CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/dtran642_927/SonLe/USC_Source/source/miniconda3/envs/vef_env/lib/python3.9/site-packages/ot/lp/__init__.py:388: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/project/dtran642_927/SonLe/USC_Source/source/miniconda3/envs/vef_env/lib/python3.9/site-packages/ot/lp/__init__.py:388: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization generation completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ot\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration Parameters\n",
    "# ---------------------------\n",
    "\n",
    "base_input_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/final_nomenclature\"\n",
    "base_output_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/final_nomenclature\"\n",
    "\n",
    "subpopulations = [\n",
    "    \"Activated_CD4\",\n",
    "    \"Effector_CD8\",\n",
    "    \"Memory_Precursor_Effector_CD8\",\n",
    "    \"Exhausted_T\",\n",
    "    \"Gamma_Delta_T\",\n",
    "    \"Active_CD4\",\n",
    "    \"Naive_CD4\",\n",
    "    \"Memory_CD4\",\n",
    "    \"Central_Memory_CD8\",\n",
    "    \"Stem_Like_CD8\",\n",
    "    \"Effector_Memory_CD8\",\n",
    "    \"Proliferating_Effector\",\n",
    "    \"CD8\"\n",
    "]\n",
    "\n",
    "cohorts = [\"control\", \"short_term\", \"long_term\"]\n",
    "\n",
    "all_timepoints = [\"Pre\", \"C1\", \"C2\", \"C4\", \"C6\", \"C9\", \"C18\", \"C36\"]\n",
    "\n",
    "cohort_colors = {\n",
    "    'control': 'yellow',\n",
    "    'short_term': 'blue',\n",
    "    'long_term': 'red'\n",
    "}\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Clusters we are interested in as the \"source\" (COI)\n",
    "# --------------------------------------------------\n",
    "clusters_of_interest = [1, 2, 3, 8, 10, 12, 14]\n",
    "\n",
    "# Provide the path to your CSV that maps each cluster to a color and celltype\n",
    "color_mapping_file = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/Publication_Material/T_Cell_cluster_colors.csv\"\n",
    "color_mapping_df = pd.read_csv(color_mapping_file)\n",
    "cluster_colors_map = dict(zip(color_mapping_df['Cluster'], color_mapping_df['Color']))\n",
    "cluster_celltype_map = dict(zip(color_mapping_df['Cluster'], color_mapping_df['Celltype']))\n",
    "\n",
    "def optimal_transport_visualization(subpop_name, cohort_name):\n",
    "    \"\"\"\n",
    "    For a single subpopulation and cohort, run the full OT pipeline:\n",
    "    1) Load data at each timepoint.\n",
    "    2) Perform OT to map from source to next timepoint.\n",
    "    3) Collect single-cell and cluster-level arrows, plus aggregated arrows.\n",
    "    4) Generate PDFs of the movement plots (single-cell, cluster-level, aggregated).\n",
    "    5) Return distributions_coi for stacked-bar plotting.\n",
    "    \"\"\"\n",
    "    input_folder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "    output_folder = os.path.join(base_output_dir, subpop_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return None\n",
    "    \n",
    "    timepoint_folders = sorted([f for f in os.listdir(input_folder) if f.startswith(\"Timepoint_\")])\n",
    "    available_timepoints = [tp.split(\"_\")[1] for tp in timepoint_folders]\n",
    "    cohort_timepoints = [tp for tp in all_timepoints if tp in available_timepoints]\n",
    "    \n",
    "    if not cohort_timepoints:\n",
    "        print(f\"No timepoints available for {subpop_name} in {cohort_name} cohort.\")\n",
    "        return None\n",
    "\n",
    "    all_x_coords = []\n",
    "    all_y_coords = []\n",
    "    \n",
    "    cell_counts_source = []\n",
    "    cell_counts_gray = []\n",
    "    full_data = {}\n",
    "    \n",
    "    # Gather data across all valid timepoints\n",
    "    for tp in cohort_timepoints:\n",
    "        source_folder = os.path.join(input_folder, f\"Timepoint_{tp}\")\n",
    "        source_cells_file = os.path.join(source_folder, 'source_cells_new.csv')\n",
    "        gray_cells_file = os.path.join(source_folder, 'gray_cells_new.csv')\n",
    "        \n",
    "        if not (os.path.exists(source_cells_file) and os.path.exists(gray_cells_file)):\n",
    "            print(f\"Missing files for {tp}. Skipping this timepoint.\")\n",
    "            continue\n",
    "        \n",
    "        source_cells = pd.read_csv(source_cells_file)\n",
    "        gray_cells = pd.read_csv(gray_cells_file)\n",
    "        \n",
    "        if source_cells.empty:\n",
    "            print(f\"No source cells for {tp}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        full_data[tp] = {\n",
    "            'source': source_cells,\n",
    "            'gray': gray_cells\n",
    "        }\n",
    "        \n",
    "        all_x_coords.extend(gray_cells['UMAP_2'])\n",
    "        all_x_coords.extend(source_cells['UMAP_2'])\n",
    "        all_y_coords.extend(gray_cells['UMAP_1'])\n",
    "        all_y_coords.extend(source_cells['UMAP_1'])\n",
    "        \n",
    "        cell_counts_source.append(len(source_cells))\n",
    "        cell_counts_gray.append(len(gray_cells))\n",
    "    \n",
    "    if not full_data:\n",
    "        print(\"No valid timepoints with data.\")\n",
    "        return None\n",
    "    \n",
    "    x_min, x_max = min(all_x_coords), max(all_x_coords)\n",
    "    y_min, y_max = min(all_y_coords), max(all_y_coords)\n",
    "    \n",
    "    min_source_cells = min(cell_counts_source) if cell_counts_source else 0\n",
    "    min_gray_cells = min(cell_counts_gray) if cell_counts_gray else 0\n",
    "    if min_source_cells == 0 or min_gray_cells == 0:\n",
    "        print(\"Insufficient cells. Skipping visualization.\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare data structures for results\n",
    "    timepoint_results = {}\n",
    "    distributions_coi = {tp: {cohort_name: {c: {} for c in clusters_of_interest}} for tp in cohort_timepoints}\n",
    "    \n",
    "    for i, source_tp in enumerate(cohort_timepoints):\n",
    "        source_data = full_data[source_tp]\n",
    "        source_cells = source_data['source']\n",
    "        \n",
    "        # Downsample so each timepoint has the same # of source & gray cells for plotting\n",
    "        sampled_source_cells = source_cells.sample(n=min_source_cells, random_state=42)\n",
    "        sampled_gray_cells = full_data[source_tp]['gray'].sample(n=min_gray_cells, random_state=42)\n",
    "        \n",
    "        timepoint_results[source_tp] = {\n",
    "            'sampled_source': sampled_source_cells,\n",
    "            'sampled_gray': sampled_gray_cells\n",
    "        }\n",
    "        \n",
    "        # If not the last timepoint, compute OT from source_tp -> target_tp\n",
    "        if i < len(cohort_timepoints) - 1:\n",
    "            target_tp = cohort_timepoints[i+1]\n",
    "            target_data = full_data[target_tp]\n",
    "            target_cells = target_data['source']\n",
    "            \n",
    "            if target_cells.empty:\n",
    "                # If there's no data in the next timepoint, skip\n",
    "                timepoint_results[source_tp]['single_cell_displacements'] = np.array([])\n",
    "                timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "            else:\n",
    "                # Identify columns for PCs\n",
    "                pc_cols = [c for c in source_cells.columns if c.startswith('PC_')]\n",
    "                full_source_coords_pc = source_cells[pc_cols].values\n",
    "                full_target_coords_pc = target_cells[pc_cols].values\n",
    "                \n",
    "                # UMAP coords\n",
    "                full_source_coords_umap = source_cells[['UMAP_2', 'UMAP_1']].values\n",
    "                full_target_coords_umap = target_cells[['UMAP_2', 'UMAP_1']].values\n",
    "                \n",
    "                # Uniform distribution over source / target\n",
    "                a = np.ones((full_source_coords_pc.shape[0],)) / full_source_coords_pc.shape[0]\n",
    "                b = np.ones((full_target_coords_pc.shape[0],)) / full_target_coords_pc.shape[0]\n",
    "                \n",
    "                cost_matrix = ot.dist(full_source_coords_pc, full_target_coords_pc, metric='euclidean')\n",
    "                \n",
    "                try:\n",
    "                    transport_plan = ot.emd(a, b, cost_matrix, numItermax=100000)\n",
    "                except Exception as e:\n",
    "                    print(f\"OT computation failed for {source_tp} to {target_tp}: {e}\")\n",
    "                    timepoint_results[source_tp]['single_cell_displacements'] = np.array([])\n",
    "                    timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "                    timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                    timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "                    continue\n",
    "                \n",
    "                full_target_indices = np.argmax(transport_plan, axis=1)\n",
    "                displacement_vectors_full = full_target_coords_umap[full_target_indices] - full_source_coords_umap\n",
    "                norms_full = np.linalg.norm(displacement_vectors_full, axis=1)\n",
    "                \n",
    "                # Displacements for the downsampled subset\n",
    "                sampled_source_indices = source_cells.index.get_indexer_for(sampled_source_cells.index)\n",
    "                sampled_displacements = displacement_vectors_full[sampled_source_indices, :]\n",
    "                sampled_norms = norms_full[sampled_source_indices]\n",
    "                \n",
    "                timepoint_results[source_tp]['single_cell_displacements'] = sampled_displacements\n",
    "                timepoint_results[source_tp]['single_cell_arrow_lengths'] = sampled_norms\n",
    "                \n",
    "                if 'seurat_clusters' in source_cells.columns and 'seurat_clusters' in target_cells.columns:\n",
    "                    # Lump some clusters if needed\n",
    "                    source_cells['seurat_clusters'] = source_cells['seurat_clusters'].replace({16:14,17:14})\n",
    "                    target_cells['seurat_clusters'] = target_cells['seurat_clusters'].replace({16:14,17:14})\n",
    "                    source_cells['seurat_clusters'] = source_cells['seurat_clusters'].replace({9:6,18:6})\n",
    "                    target_cells['seurat_clusters'] = target_cells['seurat_clusters'].replace({9:6,18:6})\n",
    "                    \n",
    "                    # For cluster-level arrows, compute average displacement per cluster\n",
    "                    source_clusters_full = source_cells['seurat_clusters'].values\n",
    "                    df_cluster_full = pd.DataFrame({\n",
    "                        'cluster': source_clusters_full,\n",
    "                        'sx': full_source_coords_umap[:,0],\n",
    "                        'sy': full_source_coords_umap[:,1],\n",
    "                        'dx': displacement_vectors_full[:,0],\n",
    "                        'dy': displacement_vectors_full[:,1],\n",
    "                        'norm': norms_full\n",
    "                    })\n",
    "                    \n",
    "                    gray_cells_tp = full_data[source_tp]['gray']\n",
    "                    all_cells_combined = pd.concat([source_cells, gray_cells_tp])\n",
    "                    all_cells_combined['seurat_clusters'] = all_cells_combined['seurat_clusters'].replace({16:14,17:14})\n",
    "                    all_cells_combined['seurat_clusters'] = all_cells_combined['seurat_clusters'].replace({9:6,18:6})\n",
    "                    \n",
    "                    all_clusters = all_cells_combined['seurat_clusters'].values\n",
    "                    all_coords_umap = all_cells_combined[['UMAP_2', 'UMAP_1']].values\n",
    "                    \n",
    "                    df_all = pd.DataFrame({\n",
    "                        'cluster': all_clusters,\n",
    "                        'sx': all_coords_umap[:,0],\n",
    "                        'sy': all_coords_umap[:,1]\n",
    "                    })\n",
    "                    \n",
    "                    centroids = df_all.groupby('cluster')[['sx','sy']].median()\n",
    "                    mean_disp = df_cluster_full.groupby('cluster')[['dx','dy']].mean()\n",
    "                    \n",
    "                    cluster_norms = np.sqrt(mean_disp['dx']**2 + mean_disp['dy']**2)\n",
    "                    \n",
    "                    # Build arrow info for each cluster of interest\n",
    "                    cluster_arrows = []\n",
    "                    common_clusters = centroids.index.intersection(mean_disp.index)\n",
    "                    for clust in common_clusters:\n",
    "                        if clust in clusters_of_interest:\n",
    "                            cx, cy = centroids.loc[clust, ['sx','sy']]\n",
    "                            cdx, cdy = mean_disp.loc[clust, ['dx','dy']]\n",
    "                            cnorm = np.sqrt(cdx**2 + cdy**2)\n",
    "                            if cnorm > 0:\n",
    "                                cdx /= cnorm\n",
    "                                cdy /= cnorm\n",
    "                            length = cluster_norms.loc[clust]\n",
    "                            cdx *= length\n",
    "                            cdy *= length\n",
    "                            cluster_arrows.append((clust, cx, cy, cdx, cdy))\n",
    "                    \n",
    "                    timepoint_results[source_tp]['cluster_arrows'] = cluster_arrows\n",
    "    \n",
    "                    if len(cluster_arrows) > 0:\n",
    "                        # \"Aggregated\" arrow by summation\n",
    "                        source_median_x = source_cells['UMAP_2'].median()\n",
    "                        source_median_y = source_cells['UMAP_1'].median()\n",
    "                        total_dx = sum([arrow[3] for arrow in cluster_arrows])\n",
    "                        total_dy = sum([arrow[4] for arrow in cluster_arrows])\n",
    "                        timepoint_results[source_tp]['aggregated_arrow'] = (\n",
    "                            source_median_x, \n",
    "                            source_median_y, \n",
    "                            total_dx, \n",
    "                            total_dy\n",
    "                        )\n",
    "                    else:\n",
    "                        timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "    \n",
    "                    # Compute target cluster distributions for each COI (source cluster)\n",
    "                    for coi in clusters_of_interest:\n",
    "                        coi_mask = (source_cells['seurat_clusters'] == coi)\n",
    "                        if np.any(coi_mask):\n",
    "                            # Which target clusters do these COI cells map to?\n",
    "                            selected_target_indices = full_target_indices[coi_mask]\n",
    "                            selected_target_clusters = target_cells['seurat_clusters'].iloc[selected_target_indices].values\n",
    "                            unique_tclusters, counts = np.unique(selected_target_clusters, return_counts=True)\n",
    "                            total_count = counts.sum()\n",
    "                            if total_count > 0:\n",
    "                                fraction_dict = {int(tc): (ct / total_count) for tc, ct in zip(unique_tclusters, counts)}\n",
    "                            else:\n",
    "                                fraction_dict = {}\n",
    "                            distributions_coi[source_tp][cohort_name][coi] = fraction_dict\n",
    "                        else:\n",
    "                            distributions_coi[source_tp][cohort_name][coi] = {}\n",
    "                else:\n",
    "                    # If cluster info is missing\n",
    "                    timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                    timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "                    for coi in clusters_of_interest:\n",
    "                        distributions_coi[source_tp][cohort_name][coi] = {}\n",
    "        else:\n",
    "            # Last timepoint has no \"next\" timepoint\n",
    "            timepoint_results[source_tp]['single_cell_displacements'] = np.array([])\n",
    "            timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "            timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "            timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "            for coi in clusters_of_interest:\n",
    "                distributions_coi[source_tp][cohort_name][coi] = {}\n",
    "\n",
    "    # ---------------------------\n",
    "    # Generate PDFs of movement\n",
    "    # ---------------------------\n",
    "\n",
    "    # 1) Single-cell arrows\n",
    "    output_file_original = os.path.join(\n",
    "        output_folder, \n",
    "        f\"{subpop_name}_{cohort_name}_movement_plots_differential_arrow_lengths_equal_cells_using_PCs.pdf\"\n",
    "    )\n",
    "    with PdfPages(output_file_original) as pdf_original:\n",
    "        plot_tps = list(timepoint_results.keys())\n",
    "        num_timepoints = len(plot_tps)\n",
    "        fig_orig, axes_orig = plt.subplots(\n",
    "            1, \n",
    "            num_timepoints, \n",
    "            figsize=(4 * num_timepoints, 4), \n",
    "            sharex=True, \n",
    "            sharey=True\n",
    "        )\n",
    "        if num_timepoints == 1:\n",
    "            axes_orig = [axes_orig]\n",
    "        \n",
    "        for i, source_tp in enumerate(plot_tps):\n",
    "            ax = axes_orig[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            sampled_source_cells = res['sampled_source']\n",
    "            sampled_gray_cells = res['sampled_gray']\n",
    "            \n",
    "            gray_x = sampled_gray_cells['UMAP_2'].values\n",
    "            gray_y = sampled_gray_cells['UMAP_1'].values\n",
    "            source_x = sampled_source_cells['UMAP_2'].values\n",
    "            source_y = sampled_source_cells['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "            \n",
    "            single_cell_displacements = res['single_cell_displacements']\n",
    "            single_cell_arrow_lengths = res['single_cell_arrow_lengths']\n",
    "            \n",
    "            if len(single_cell_displacements) > 0:\n",
    "                source_coords_sampled = sampled_source_cells[['UMAP_2','UMAP_1']].values\n",
    "                sampled_norms = np.linalg.norm(single_cell_displacements, axis=1)\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    unit_vectors = single_cell_displacements / sampled_norms[:, np.newaxis]\n",
    "                    unit_vectors[~np.isfinite(unit_vectors)] = 0\n",
    "                scaled_vectors = unit_vectors * single_cell_arrow_lengths[:, np.newaxis]\n",
    "                \n",
    "                for j in range(len(source_coords_sampled)):\n",
    "                    sx, sy = source_coords_sampled[j]\n",
    "                    dx, dy = scaled_vectors[j]\n",
    "                    ax.arrow(\n",
    "                        sx, sy, \n",
    "                        dx, dy,\n",
    "                        color='black', \n",
    "                        alpha=0.7, \n",
    "                        head_width=0.05, \n",
    "                        head_length=0.05,\n",
    "                        length_includes_head=True, \n",
    "                        linewidth=0.5\n",
    "                    )\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_orig.suptitle(f\"{subpop_name} - {cohort_name}\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_original.savefig(fig_orig)\n",
    "        plt.close(fig_orig)\n",
    "    \n",
    "    # 2) Cluster-level arrows\n",
    "    output_file_cluster = os.path.join(\n",
    "        output_folder, \n",
    "        f\"{subpop_name}_{cohort_name}_movement_plots_aggregated_cluster_arrows_equal_cells_using_PCs.pdf\"\n",
    "    )\n",
    "    with PdfPages(output_file_cluster) as pdf_cluster:\n",
    "        plot_tps = list(timepoint_results.keys())\n",
    "        num_timepoints = len(plot_tps)\n",
    "        fig_clust, axes_clust = plt.subplots(\n",
    "            1, \n",
    "            num_timepoints, \n",
    "            figsize=(4 * num_timepoints, 4), \n",
    "            sharex=True, \n",
    "            sharey=True\n",
    "        )\n",
    "        if num_timepoints == 1:\n",
    "            axes_clust = [axes_clust]\n",
    "        \n",
    "        for i, source_tp in enumerate(plot_tps):\n",
    "            ax = axes_clust[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            \n",
    "            gray_x = res['sampled_gray']['UMAP_2'].values\n",
    "            gray_y = res['sampled_gray']['UMAP_1'].values\n",
    "            source_x = res['sampled_source']['UMAP_2'].values\n",
    "            source_y = res['sampled_source']['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "            \n",
    "            cluster_arrows = res.get('cluster_arrows', [])\n",
    "            \n",
    "            source_cells = full_data[source_tp]['source']\n",
    "            if 'seurat_clusters' in source_cells.columns:\n",
    "                source_cluster_counts = source_cells['seurat_clusters'].value_counts()\n",
    "                total_source_cells_current = len(source_cells)\n",
    "            else:\n",
    "                source_cluster_counts = pd.Series()\n",
    "                total_source_cells_current = 1\n",
    "            \n",
    "            for (clust, cx, cy, cdx, cdy) in cluster_arrows:\n",
    "                arrow_color = cluster_colors_map.get(clust, 'black')\n",
    "                proportion = source_cluster_counts.get(clust, 0) / total_source_cells_current\n",
    "                line_width = 1.0 + 8.0 * proportion\n",
    "\n",
    "                # Outline in black for clarity\n",
    "                ax.arrow(\n",
    "                    cx, cy,\n",
    "                    cdx, cdy,\n",
    "                    color='black',\n",
    "                    alpha=1,\n",
    "                    head_width=0.2 + 0.5 * proportion,\n",
    "                    head_length=0.1 + 0.1 * proportion,\n",
    "                    length_includes_head=True,\n",
    "                    linewidth=line_width + 2\n",
    "                )\n",
    "                \n",
    "                # Main arrow in cluster color\n",
    "                ax.arrow(\n",
    "                    cx, cy,\n",
    "                    cdx, cdy,\n",
    "                    # color=arrow_color,\n",
    "                    color='black',\n",
    "                    alpha=1,\n",
    "                    head_width=0.2 + 0.5 * proportion,\n",
    "                    head_length=0.1 + 0.1 * proportion,\n",
    "                    length_includes_head=True,\n",
    "                    linewidth=line_width\n",
    "                )\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_clust.suptitle(f\"{subpop_name} - {cohort_name} (Cluster-Level Arrows)\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_cluster.savefig(fig_clust)\n",
    "        plt.close(fig_clust)\n",
    "\n",
    "    # 3) Single aggregated arrow\n",
    "    output_file_single_agg = os.path.join(\n",
    "        output_folder, \n",
    "        f\"{subpop_name}_{cohort_name}_movement_plots_aggregated_single_arrow_equal_cells_using_PCs.pdf\"\n",
    "    )\n",
    "    with PdfPages(output_file_single_agg) as pdf_agg:\n",
    "        plot_tps = list(timepoint_results.keys())\n",
    "        num_timepoints = len(plot_tps)\n",
    "        fig_agg, axes_agg = plt.subplots(\n",
    "            1,\n",
    "            num_timepoints,\n",
    "            figsize=(4 * num_timepoints, 4),\n",
    "            sharex=True,\n",
    "            sharey=True\n",
    "        )\n",
    "        if num_timepoints == 1:\n",
    "            axes_agg = [axes_agg]\n",
    "\n",
    "        for i, source_tp in enumerate(plot_tps):\n",
    "            ax = axes_agg[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "\n",
    "            gray_x = res['sampled_gray']['UMAP_2'].values\n",
    "            gray_y = res['sampled_gray']['UMAP_1'].values\n",
    "            source_x = res['sampled_source']['UMAP_2'].values\n",
    "            source_y = res['sampled_source']['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "\n",
    "            aggregated_arrow = res.get('aggregated_arrow', None)\n",
    "            if aggregated_arrow is not None:\n",
    "                global_cx, global_cy, total_dx, total_dy = aggregated_arrow\n",
    "                ax.arrow(\n",
    "                    global_cx, global_cy,\n",
    "                    total_dx, total_dy,\n",
    "                    color='black', \n",
    "                    alpha=0.9,\n",
    "                    head_width=0.3, \n",
    "                    head_length=0.3,\n",
    "                    length_includes_head=True, \n",
    "                    linewidth=2.0\n",
    "                )\n",
    "\n",
    "            ax.set_title(f\"Timepoint: {source_tp} (Single Aggregated Arrow)\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        fig_agg.suptitle(f\"{subpop_name} - {cohort_name} (Single Aggregated Arrow)\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_agg.savefig(fig_agg)\n",
    "        plt.close(fig_agg)\n",
    "    \n",
    "    # Return the distributions for stacked-bar plotting\n",
    "    return distributions_coi\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution Loop\n",
    "# ---------------------------\n",
    "for subpop_name in subpopulations:\n",
    "    # Prepare a nested dict so we can collect distributions\n",
    "    distributions_coi_all = {\n",
    "        tp: {\n",
    "            c: {coi: {} for coi in clusters_of_interest} \n",
    "            for c in cohorts\n",
    "        }\n",
    "        for tp in all_timepoints\n",
    "    }\n",
    "    \n",
    "    # Run for each cohort\n",
    "    for cohort_name in cohorts:\n",
    "        print(f\"Processing {subpop_name} - {cohort_name}\")\n",
    "        input_subfolder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "        if not os.path.exists(input_subfolder):\n",
    "            print(f\"Input subfolder does not exist: {input_subfolder}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        dist_coi = optimal_transport_visualization(subpop_name, cohort_name)\n",
    "        if dist_coi is not None:\n",
    "            for tp in dist_coi:\n",
    "                for c in dist_coi[tp]:\n",
    "                    for coi in dist_coi[tp][c]:\n",
    "                        distributions_coi_all[tp][c][coi] = dist_coi[tp][c][coi]\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 1) Group non-COI target clusters into \"Others\"\n",
    "    # --------------------------------------------------\n",
    "    for tp in distributions_coi_all:\n",
    "        for c in cohorts:\n",
    "            for coi in clusters_of_interest:\n",
    "                fraction_dict = distributions_coi_all[tp][c][coi]\n",
    "                if fraction_dict:\n",
    "                    new_dict = {}\n",
    "                    others_sum = 0.0\n",
    "                    for tclust, frac in fraction_dict.items():\n",
    "                        # If the target cluster is not in our \"source\" list, \n",
    "                        # put it in the \"Others\" bin\n",
    "                        if tclust not in clusters_of_interest:\n",
    "                            others_sum += frac\n",
    "                        else:\n",
    "                            new_dict[tclust] = frac\n",
    "                    if others_sum > 0:\n",
    "                        new_dict['Others'] = others_sum\n",
    "                    distributions_coi_all[tp][c][coi] = new_dict\n",
    "\n",
    "    # Gather all target clusters (including \"Others\")\n",
    "    all_target_clusters = []\n",
    "    for tp in distributions_coi_all:\n",
    "        for c in cohorts:\n",
    "            for coi in clusters_of_interest:\n",
    "                all_target_clusters.extend(distributions_coi_all[tp][c][coi].keys())\n",
    "    all_target_clusters = set(all_target_clusters)\n",
    "\n",
    "    # Handle \"Others\" separately for sorting\n",
    "    others_present = (\"Others\" in all_target_clusters)\n",
    "    if others_present:\n",
    "        all_target_clusters.remove(\"Others\")\n",
    "\n",
    "    # Now only numeric cluster IDs remain\n",
    "    # Sort numeric clusters in ascending order\n",
    "    all_target_clusters = sorted(all_target_clusters)\n",
    "\n",
    "    # Put \"Others\" at the end if present\n",
    "    if others_present:\n",
    "        all_target_clusters.append(\"Others\")\n",
    "\n",
    "    def get_celltype_name(clust_id):\n",
    "        if clust_id == 'Others':\n",
    "            return \"Others\"\n",
    "        return cluster_celltype_map.get(clust_id, f\"Cluster {clust_id}\")\n",
    "\n",
    "    def get_cluster_color(clust_id):\n",
    "        if clust_id == 'Others':\n",
    "            return 'gray'\n",
    "        return cluster_colors_map.get(clust_id, 'gray')\n",
    "\n",
    "    # ---------------------------\n",
    "    # Generate the stacked bar chart PDF\n",
    "    # ---------------------------\n",
    "    output_file_stacked = os.path.join(base_output_dir, f\"{subpop_name}_target_cluster_distribution_coi_without_stack_labels.pdf\")\n",
    "    with PdfPages(output_file_stacked) as pdf_dist:\n",
    "        fig, axes = plt.subplots(\n",
    "            len(clusters_of_interest), \n",
    "            len(all_timepoints), \n",
    "            figsize=(4 * len(all_timepoints), 3 * len(clusters_of_interest)), \n",
    "            sharex=False, \n",
    "            sharey=True\n",
    "        )\n",
    "        \n",
    "        # Handle shape for single row/col\n",
    "        if len(clusters_of_interest) == 1 and len(all_timepoints) == 1:\n",
    "            axes = np.array([[axes]])\n",
    "        elif len(clusters_of_interest) == 1:\n",
    "            axes = axes[np.newaxis, :]\n",
    "        elif len(all_timepoints) == 1:\n",
    "            axes = axes[:, np.newaxis]\n",
    "\n",
    "        # Prepare legend patches\n",
    "        legend_patches = []\n",
    "        for tc in all_target_clusters:\n",
    "            legend_patches.append(\n",
    "                plt.Rectangle((0,0),1,1,\n",
    "                              facecolor=get_cluster_color(tc),\n",
    "                              edgecolor='black',\n",
    "                              label=get_celltype_name(tc))\n",
    "            )\n",
    "        \n",
    "        # Plot bars\n",
    "        for row_i, coi in enumerate(clusters_of_interest):\n",
    "            for col_i, tp in enumerate(all_timepoints):\n",
    "                ax = axes[row_i, col_i]\n",
    "                bar_positions = np.arange(len(cohorts))\n",
    "                bottoms = np.zeros(len(cohorts))\n",
    "\n",
    "                # Retrieve distribution data for each cohort at (tp, coi)\n",
    "                cohort_distributions = [distributions_coi_all[tp][c][coi] for c in cohorts]\n",
    "\n",
    "                # Build up data for stacked bars: (target_cluster, [fractions for each cohort])\n",
    "                stack_data = []\n",
    "                for tc in all_target_clusters:\n",
    "                    h = [dist.get(tc, 0.0) for dist in cohort_distributions]\n",
    "                    stack_data.append((tc, h))\n",
    "\n",
    "                # Sort so biggest fraction (summed across cohorts) is at the bottom\n",
    "                stack_data.sort(key=lambda x: sum(x[1]), reverse=True)\n",
    "\n",
    "                # Plot\n",
    "                for (tc, heights) in stack_data:\n",
    "                    color = get_cluster_color(tc)\n",
    "                    ax.bar(\n",
    "                        bar_positions, \n",
    "                        heights, \n",
    "                        bottom=bottoms, \n",
    "                        color=color, \n",
    "                        edgecolor='black'\n",
    "                    )\n",
    "                    # Label each segment if fraction is big enough\n",
    "                    # for idx, val in enumerate(heights):\n",
    "                    #     if val > 0.05:\n",
    "                    #         mid_y = bottoms[idx] + val / 2\n",
    "                    #         ax.text(\n",
    "                    #             bar_positions[idx], \n",
    "                    #             mid_y, \n",
    "                    #             get_celltype_name(tc),\n",
    "                    #             ha='center', \n",
    "                    #             va='center',\n",
    "                    #             fontsize=6, \n",
    "                    #             color='white'\n",
    "                    #         )\n",
    "                    bottoms += heights\n",
    "\n",
    "                if row_i == 0:\n",
    "                    ax.set_title(f\"{tp}\", fontsize=10)\n",
    "\n",
    "                ax.set_xticks(bar_positions)\n",
    "                ax.set_xticklabels(cohorts, rotation=45, ha='right', fontsize=8)\n",
    "                ax.set_ylim(0, 1)\n",
    "\n",
    "        fig.suptitle(f\"Distribution of Target Clusters per COI - {subpop_name}\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Adjust to make room for row labels & legend\n",
    "        fig.subplots_adjust(left=0.15, top=0.86, right=0.82)  # extra space on the right for legend\n",
    "\n",
    "        # Label rows with the celltype of the COI\n",
    "        n_rows = len(clusters_of_interest)\n",
    "        for row_i, coi in enumerate(clusters_of_interest):\n",
    "            y_pos = 0.86 - (row_i + 0.5)*(0.86-0.1)/n_rows\n",
    "            fig.text(0.05, y_pos, get_celltype_name(coi), va='center', ha='right', fontsize=10, color='black')\n",
    "\n",
    "        # Create a single-column legend on the right\n",
    "        fig.legend(\n",
    "            handles=legend_patches,\n",
    "            loc='upper left',\n",
    "            bbox_to_anchor=(0.84, 0.95),\n",
    "            ncol=1,\n",
    "            fontsize=8,\n",
    "            title=\"Target Celltypes\"\n",
    "        )\n",
    "\n",
    "        pdf_dist.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "print(\"Visualization generation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d04bee4b-3b5d-4d3f-bf7c-7fb4150301e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Example mapping from your R code to Python\n",
    "celltype_to_cluster = {\n",
    "    \"Activated_CD4\": [0],\n",
    "    \"Effector_CD8\": [1],\n",
    "    \"Memory_Precursor_Effector_CD8\": [2],\n",
    "    \"Exhausted_T\": [3],\n",
    "    \"Gamma_Delta_T\": [4],\n",
    "    \"Active_CD4\": [5],\n",
    "    \"Naive_CD4\": [6, 9, 18],\n",
    "    \"Memory_CD4\": [7],\n",
    "    \"Stem_Like_CD8\": [8],\n",
    "    \"Effector_Memory_CD8\": [10],\n",
    "    \"Central_Memory_CD8\": [12],\n",
    "    \"Proliferating_Effector\": [14, 16, 17]\n",
    "}\n",
    "\n",
    "def compute_transition_significance(\n",
    "    base_directory,\n",
    "    subpop_name,\n",
    "    mapping,\n",
    "    timepoint,\n",
    "    group1_names,\n",
    "    group2_names,\n",
    "    source_subpop,\n",
    "    target_subpop,\n",
    "    n_boot=10000\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute whether two sets of group(s) differ significantly in the fraction of cells\n",
    "    transitioning from a source subpopulation to a target subpopulation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_directory : str\n",
    "        Path to the base directory. Underneath this directory, we have:\n",
    "           base_directory/subpop_name/group_name/Timepoint_X/source_cells_new.csv, target_cells_new.csv\n",
    "\n",
    "    subpop_name : str\n",
    "        Name of the subpopulation folder (e.g., 'CD8').\n",
    "\n",
    "    mapping : dict\n",
    "        A dictionary mapping celltype name to a list of cluster IDs (e.g. celltype_to_cluster).\n",
    "\n",
    "    timepoint : str\n",
    "        The timepoint for the source (e.g. 'C1'). We interpret the \"next\" timepoint\n",
    "        by incrementing the integer. If timepoint='Pre', we define the next as 'C1'.\n",
    "\n",
    "    group1_names : list of str\n",
    "        e.g. [\"control\"] or [\"control\", \"short_term\"].\n",
    "\n",
    "    group2_names : list of str\n",
    "        e.g. [\"long_term\"] or [\"short_term\", \"long_term\"].\n",
    "\n",
    "    source_subpop : str\n",
    "        Key in 'mapping' that indicates the source subpopulation (e.g. 'Proliferating_Effector').\n",
    "\n",
    "    target_subpop : str\n",
    "        Key in 'mapping' that indicates the target subpopulation (e.g. 'Exhausted_T').\n",
    "\n",
    "    n_boot : int\n",
    "        Number of permutation (or bootstrap) iterations for the null distribution.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    p_value : float\n",
    "        Estimated p-value (two-sided) from the permutation test.\n",
    "\n",
    "    mean_diff_observed : float\n",
    "        The actual difference in mean fractions (Group1 - Group2).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - We assume each (group_name, timepoint) folder has a source_cells_new.csv and target_cells_new.csv.\n",
    "    - If your real code uses \"source_cells_new.csv\" for timepoint T and \"source_cells_new.csv\" for T+1, \n",
    "      adjust accordingly.\n",
    "    - The function uses a simple \"mapped_cluster\" approach for demonstration.\n",
    "      In practice, you'd need to adapt how you find the target cluster for each source cell.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------------------\n",
    "    # 1) Determine next timepoint\n",
    "    # ---------------------------\n",
    "    # If timepoint='Pre', we define the next as 'C1'\n",
    "    # Otherwise, if timepoint starts with 'C', parse the integer and increment\n",
    "    if timepoint == 'Pre':\n",
    "        next_timepoint = 'C1'\n",
    "    else:\n",
    "        # e.g., timepoint='C1' -> next_timepoint='C2'\n",
    "        # watch out if timepoint='C36', you'd do 'C37'\n",
    "        # Adjust logic if your data doesn't go that high\n",
    "        t_int = int(timepoint.strip('C'))\n",
    "        next_timepoint = f'C{t_int + 1}'\n",
    "\n",
    "    # Helper to compute fraction for one group\n",
    "    def compute_fraction_for_group(group_name):\n",
    "        \"\"\"\n",
    "        For the given group_name (e.g., 'control'), read:\n",
    "          base_directory/subpop_name/group_name/Timepoint_{timepoint}/source_cells_new.csv\n",
    "          base_directory/subpop_name/group_name/Timepoint_{next_timepoint}/target_cells_new.csv\n",
    "        Then compute fraction of source_subpop cells that map to target_subpop clusters.\n",
    "        Return fraction or NaN if not computable.\n",
    "        \"\"\"\n",
    "        # Build paths\n",
    "        source_dir = os.path.join(base_directory, subpop_name, group_name, f\"Timepoint_{timepoint}\")\n",
    "        print(source_dir)\n",
    "        target_dir = os.path.join(base_directory, subpop_name, group_name, f\"Timepoint_{timepoint}\")\n",
    "        print(target_dir)\n",
    "        source_csv = os.path.join(source_dir, \"source_cells_new.csv\")\n",
    "        target_csv = os.path.join(target_dir, \"target_cells_new.csv\")\n",
    "\n",
    "        if not (os.path.exists(source_csv) and os.path.exists(target_csv)):\n",
    "            return np.nan\n",
    "\n",
    "        df_source = pd.read_csv(source_csv)\n",
    "        df_target = pd.read_csv(target_csv)\n",
    "\n",
    "        print(df_source.head())\n",
    "        print(df_target.head())\n",
    "\n",
    "        if df_source.empty or df_target.empty:\n",
    "            return np.nan\n",
    "\n",
    "        # Clusters for source and target subpop\n",
    "        source_clusters = mapping[source_subpop]\n",
    "        target_clusters = mapping[target_subpop]\n",
    "\n",
    "        print(source_clusters)\n",
    "        print(target_clusters)\n",
    "\n",
    "        # Identify which source cells belong to the source_subpop\n",
    "        mask_source = df_source['seurat_clusters'].isin(source_clusters)\n",
    "        n_source = mask_source.sum()\n",
    "        if n_source == 0:\n",
    "            # No source-subpop cells exist\n",
    "            return 0.0\n",
    "\n",
    "        # Now we need to figure out which of these source cells map to the target subpop.\n",
    "        # This logic depends entirely on how you store the \"next timepoint\" cluster for each source cell.\n",
    "        #\n",
    "        # For demonstration, assume there's a column \"mapped_cluster\" in df_source that says\n",
    "        # which cluster the cell maps to in the next timepoint. \n",
    "        # If you do not have that, you'll need your own approach, e.g. matching barcodes in df_target, etc.\n",
    "\n",
    "        if 'mapped_cluster' not in df_source.columns:\n",
    "            return np.nan\n",
    "\n",
    "        # Among the source-subpop cells, how many map to the target clusters?\n",
    "        mask_target = df_source.loc[mask_source, 'mapped_cluster'].isin(target_clusters)\n",
    "        n_target = mask_target.sum()\n",
    "        fraction = n_target / n_source\n",
    "        return fraction\n",
    "\n",
    "    # ---------------------------\n",
    "    # 2) Gather fractions for each group in group1 and group2\n",
    "    # ---------------------------\n",
    "    fractions_group1 = []\n",
    "    for grp in group1_names:\n",
    "        frac = compute_fraction_for_group(grp)\n",
    "        if not np.isnan(frac):\n",
    "            fractions_group1.append(frac)\n",
    "\n",
    "    fractions_group2 = []\n",
    "    for grp in group2_names:\n",
    "        frac = compute_fraction_for_group(grp)\n",
    "        if not np.isnan(frac):\n",
    "            fractions_group2.append(frac)\n",
    "\n",
    "    if len(fractions_group1) == 0 or len(fractions_group2) == 0:\n",
    "        print(\"Insufficient data for either group. Returning NaN.\")\n",
    "        return np.nan, 0.0\n",
    "\n",
    "    # Observed difference in means\n",
    "    mean_diff_observed = np.mean(fractions_group1) - np.mean(fractions_group2)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 3) Permutation Test\n",
    "    # ---------------------------\n",
    "    all_fractions = np.concatenate([fractions_group1, fractions_group2])\n",
    "    n1 = len(fractions_group1)\n",
    "    n2 = len(fractions_group2)\n",
    "\n",
    "    count_more_extreme = 0\n",
    "    for _ in range(n_boot):\n",
    "        perm = np.random.permutation(all_fractions)\n",
    "        group1_star = perm[:n1]\n",
    "        group2_star = perm[n1:]\n",
    "        diff_star = np.mean(group1_star) - np.mean(group2_star)\n",
    "        # two-sided\n",
    "        if abs(diff_star) >= abs(mean_diff_observed):\n",
    "            count_more_extreme += 1\n",
    "\n",
    "    p_value = count_more_extreme / n_boot\n",
    "\n",
    "    return p_value, mean_diff_observed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "016abdb3-e941-46c5-9ecf-ab3e75261015",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/final_nomenclature/Proliferating_Effector/short_term/Timepoint_C2\n",
      "/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/final_nomenclature/Proliferating_Effector/short_term/Timepoint_C2\n",
      "               cell_id TimePoint SurvivorGroup  seurat_clusters origin  \\\n",
      "0  AAAGCAACATATGGTC-35        C2    short_term                3   10C2   \n",
      "1  AACTCCCTCAAGATCC-35        C2    short_term                1   10C2   \n",
      "2  ACAGCTAAGGCCGAAT-35        C2    short_term                8   10C2   \n",
      "3  ACAGCTAAGGGTCTCC-35        C2    short_term               17   10C2   \n",
      "4  ACATACGCAATGTTGC-35        C2    short_term                8   10C2   \n",
      "\n",
      "     UMAP_1    UMAP_2       PC_1      PC_2      PC_3  ...     PC_41     PC_42  \\\n",
      "0  3.522864 -2.184298   0.279686 -2.635812  5.064660  ...  0.636210  0.286694   \n",
      "1  2.763960 -0.055100   0.078407  0.458175  1.927083  ... -0.037759 -0.450294   \n",
      "2  0.583180  0.733396  -0.792074  0.234836  0.253369  ... -0.146049  0.128695   \n",
      "3  6.003644 -1.506457  23.593364 -7.492250  7.719354  ...  2.820140 -1.659611   \n",
      "4  1.079622 -0.214469  -1.939252 -0.138899  1.285283  ... -1.787468 -0.095265   \n",
      "\n",
      "      PC_43     PC_44     PC_45     PC_46     PC_47     PC_48     PC_49  \\\n",
      "0 -0.372892  0.389256  2.160050 -0.922400 -0.601009 -1.745800  0.209776   \n",
      "1 -1.082831  2.263158  0.244340  0.453370  0.173227 -1.537483  1.672559   \n",
      "2 -1.608786  1.577806 -0.669843 -0.488859  0.725239 -1.908704 -1.293022   \n",
      "3  0.328507 -0.118197  1.638641 -0.337064  0.893301  0.841005 -0.500633   \n",
      "4 -0.064203  2.240893 -1.955655  1.021801  0.530789 -1.046245 -0.384048   \n",
      "\n",
      "      PC_50  \n",
      "0  0.600704  \n",
      "1  0.240199  \n",
      "2 -0.744948  \n",
      "3 -1.966789  \n",
      "4  0.239699  \n",
      "\n",
      "[5 rows x 57 columns]\n",
      "               cell_id TimePoint SurvivorGroup  seurat_clusters origin  \\\n",
      "0  AAAGTAGGTTTGGCGC-31        C4    short_term               14    7C4   \n",
      "1  AACCATGTCGCAAGCC-31        C4    short_term               16    7C4   \n",
      "2  AACTCCCGTATGAATG-31        C4    short_term               14    7C4   \n",
      "3  AAGGAGCTCCAAGCCG-31        C4    short_term               14    7C4   \n",
      "4  ACACCAAAGTGTTAGA-31        C4    short_term                1    7C4   \n",
      "\n",
      "     UMAP_1    UMAP_2       PC_1      PC_2      PC_3  ...     PC_41     PC_42  \\\n",
      "0  4.981221 -2.573762  10.775226 -7.940453  6.831420  ... -0.104514 -1.128705   \n",
      "1  7.587323 -2.768169  39.262888 -8.695929  6.728037  ...  0.124444 -1.384092   \n",
      "2  6.781694 -2.910426  18.622876 -4.346399  8.339048  ...  2.499024 -0.235864   \n",
      "3  5.415590 -1.502050  17.191655 -3.605197  8.793166  ...  0.414082  1.315937   \n",
      "4  1.784192  0.278292  -0.372689  0.101870  2.858551  ...  1.880171  0.292641   \n",
      "\n",
      "      PC_43     PC_44     PC_45     PC_46     PC_47     PC_48     PC_49  \\\n",
      "0  2.042385 -2.249584  0.541474 -0.044471 -0.627080  0.710219  3.009231   \n",
      "1  1.527684 -1.645105  0.978954  0.362681  2.050119  2.551262 -1.661024   \n",
      "2  1.517046 -0.419644 -0.478246  1.776597  0.584147  1.504966 -1.104152   \n",
      "3  0.050220 -0.750009  0.069637 -0.994795 -0.150168  1.378182 -0.002793   \n",
      "4 -1.435063 -0.524027 -0.064131 -1.007480  0.202771  0.502932  1.768557   \n",
      "\n",
      "      PC_50  \n",
      "0  0.126951  \n",
      "1 -0.733190  \n",
      "2  0.603108  \n",
      "3  0.866671  \n",
      "4  0.564913  \n",
      "\n",
      "[5 rows x 57 columns]\n",
      "[14, 16, 17]\n",
      "[3]\n",
      "/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/final_nomenclature/Proliferating_Effector/long_term/Timepoint_C2\n",
      "/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/final_nomenclature/Proliferating_Effector/long_term/Timepoint_C2\n",
      "               cell_id TimePoint SurvivorGroup  seurat_clusters origin  \\\n",
      "0  AAACCTGGTGATGCCC-64        C2     long_term                7   21C2   \n",
      "1  AAACGGGAGAAGGCCT-64        C2     long_term                1   21C2   \n",
      "2  AAACGGGCACCAGTTA-64        C2     long_term               10   21C2   \n",
      "3  AAACGGGTCATACGGT-64        C2     long_term                1   21C2   \n",
      "4  AAACGGGTCCTAGAAC-64        C2     long_term                1   21C2   \n",
      "\n",
      "     UMAP_1    UMAP_2      PC_1      PC_2      PC_3  ...     PC_41     PC_42  \\\n",
      "0 -0.922394  1.897627 -3.868864  3.948016 -1.927920  ...  2.155101  0.985137   \n",
      "1  1.499581  1.725030  0.006526  3.105058  1.945450  ...  0.800713  0.189381   \n",
      "2  1.151472  6.338067  0.110877  3.177895  1.299452  ... -1.173853  1.648916   \n",
      "3  2.286668  0.375361 -0.011185  0.612122  2.876631  ...  0.888321 -0.463597   \n",
      "4  1.020682  4.015626 -0.818065  2.678286  0.916149  ... -1.376384  0.056345   \n",
      "\n",
      "      PC_43     PC_44     PC_45     PC_46     PC_47     PC_48     PC_49  \\\n",
      "0 -1.756322  0.016604 -0.815422  0.494463  0.435376  1.748516 -0.695240   \n",
      "1 -0.845135  1.431845 -0.384846 -0.104054 -0.631026 -0.682636  0.871510   \n",
      "2  0.965082 -2.059011 -0.746483 -2.170084  0.935626 -2.505592 -1.362259   \n",
      "3 -4.102691 -2.061298  0.369257  2.115988 -1.554370  1.170550  0.061717   \n",
      "4  0.506919 -1.966597  0.758157 -1.198838  0.359404 -0.883803 -0.865126   \n",
      "\n",
      "      PC_50  \n",
      "0 -0.495560  \n",
      "1  0.544548  \n",
      "2  0.096067  \n",
      "3 -0.309610  \n",
      "4  2.186511  \n",
      "\n",
      "[5 rows x 57 columns]\n",
      "               cell_id TimePoint SurvivorGroup  seurat_clusters origin  \\\n",
      "0  AAACCTGCATTAACCG-57        C4     long_term                2   21C4   \n",
      "1  AAACGGGCAACTTGAC-57        C4     long_term                2   21C4   \n",
      "2  AAACGGGGTGTTTGTG-57        C4     long_term                2   21C4   \n",
      "3  AAACGGGTCCGCGTTT-57        C4     long_term                7   21C4   \n",
      "4  AAACGGGTCGTCCGTT-57        C4     long_term                1   21C4   \n",
      "\n",
      "     UMAP_1    UMAP_2      PC_1      PC_2      PC_3  ...     PC_41     PC_42  \\\n",
      "0  0.079229  4.391014 -1.870428  6.605330 -0.144815  ... -1.753477 -0.024494   \n",
      "1 -0.276443  3.145332 -1.133596  5.469335 -0.129501  ...  0.667169  0.004869   \n",
      "2  0.517505  3.451734  0.509504  5.534131  1.979243  ... -1.321301  0.842813   \n",
      "3 -5.065909 -5.532187 -2.281063 -0.540594  0.328656  ...  1.015279 -0.318747   \n",
      "4  1.074084  2.007698 -1.357149  4.082907  1.681826  ... -0.533175 -0.693629   \n",
      "\n",
      "      PC_43     PC_44     PC_45     PC_46     PC_47     PC_48     PC_49  \\\n",
      "0  0.539782 -0.601769 -0.318202 -2.157904 -1.185733 -0.774082 -2.715464   \n",
      "1  1.977857  0.895366  0.171562  0.409986 -3.465528 -0.535056  0.760130   \n",
      "2  0.903332  0.641813  0.656116 -1.210475  0.132223  1.911922 -1.652006   \n",
      "3 -0.702376 -0.186245  0.479495  2.681848  0.549831  0.800730 -0.363880   \n",
      "4 -0.222163 -0.218924 -1.767666  1.739426 -0.394329  0.602685  0.442253   \n",
      "\n",
      "      PC_50  \n",
      "0  0.793948  \n",
      "1  1.291874  \n",
      "2 -1.077596  \n",
      "3  0.120654  \n",
      "4 -0.383754  \n",
      "\n",
      "[5 rows x 57 columns]\n",
      "[14, 16, 17]\n",
      "[3]\n",
      "Insufficient data for either group. Returning NaN.\n",
      "Observed difference in fraction (Group1 - Group2): 0.0\n",
      "Permutation-based p-value: nan\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "base_directory = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/final_nomenclature\"\n",
    "subpop_name = \"Proliferating_Effector\"\n",
    "timepoint = \"C2\"                   # i.e. from C1 -> C2\n",
    "group1_names = [\"short_term\"]         # One group is control\n",
    "group2_names = [\"long_term\"]  # Another group combines short_term + long_term\n",
    "source_subpop = \"Proliferating_Effector\"\n",
    "target_subpop = \"Exhausted_T\"\n",
    "mapping = celltype_to_cluster\n",
    "\n",
    "p_val, diff_obs = compute_transition_significance(\n",
    "    base_directory=base_directory,\n",
    "    subpop_name=subpop_name,\n",
    "    mapping=mapping,\n",
    "    timepoint=timepoint,\n",
    "    group1_names=group1_names,\n",
    "    group2_names=group2_names,\n",
    "    source_subpop=source_subpop,\n",
    "    target_subpop=target_subpop,\n",
    "    n_boot=10000\n",
    ")\n",
    "\n",
    "print(\"Observed difference in fraction (Group1 - Group2):\", diff_obs)\n",
    "print(\"Permutation-based p-value:\", p_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89f8e9c-e928-4f95-b933-4312a14ad253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vef_env",
   "language": "python",
   "name": "vef_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
