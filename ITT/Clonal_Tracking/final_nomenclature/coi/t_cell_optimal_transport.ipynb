{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efdf6fae-d039-4de7-a615-a40fba7b5b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Activated_CD4 - control\n",
      "Processing Activated_CD4 - short_term\n",
      "Processing Activated_CD4 - long_term\n",
      "Processing Effector_CD8 - control\n",
      "Processing Effector_CD8 - short_term\n",
      "Processing Effector_CD8 - long_term\n",
      "Processing Effector_Memory_CD8 - control\n",
      "Processing Effector_Memory_CD8 - short_term\n",
      "Processing Effector_Memory_CD8 - long_term\n",
      "Processing Exhausted_T - control\n",
      "Processing Exhausted_T - short_term\n",
      "Processing Exhausted_T - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Gamma_Delta_T - control\n",
      "Processing Gamma_Delta_T - short_term\n",
      "Processing Gamma_Delta_T - long_term\n",
      "Processing Active_CD4 - control\n",
      "Processing Active_CD4 - short_term\n",
      "Processing Active_CD4 - long_term\n",
      "Processing Naive_CD4 - control\n",
      "Processing Naive_CD4 - short_term\n",
      "Processing Naive_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD4 - control\n",
      "Processing Memory_CD4 - short_term\n",
      "Processing Memory_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD8 - control\n",
      "Processing Memory_CD8 - short_term\n",
      "Processing Memory_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Anergic_CD8 - control\n",
      "Processing Anergic_CD8 - short_term\n",
      "Processing Anergic_CD8 - long_term\n",
      "Processing Naive_CD8 - control\n",
      "Processing Naive_CD8 - short_term\n",
      "Processing Naive_CD8 - long_term\n",
      "Processing Hyperactivated_CD8 - control\n",
      "Processing Hyperactivated_CD8 - short_term\n",
      "Processing Hyperactivated_CD8 - long_term\n",
      "Processing Proliferating_Effector - control\n",
      "Processing Proliferating_Effector - short_term\n",
      "Processing Proliferating_Effector - long_term\n",
      "Processing CD8 - control\n",
      "Processing CD8 - short_term\n",
      "Processing CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization generation completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ot\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration Parameters\n",
    "# ---------------------------\n",
    "\n",
    "# Base directory where the CSV files are stored\n",
    "base_input_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/new/temp\"\n",
    "\n",
    "# Base directory to save the plots\n",
    "base_output_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/new/temp\"\n",
    "\n",
    "# Subpopulations (should match the names used in R)\n",
    "subpopulations = [\n",
    "    \"Activated_CD4\",\n",
    "    \"Effector_CD8\",\n",
    "    \"Effector_Memory_CD8\",\n",
    "    \"Exhausted_T\",\n",
    "    \"Gamma_Delta_T\",\n",
    "    \"Active_CD4\",\n",
    "    \"Naive_CD4\",\n",
    "    \"Memory_CD4\",\n",
    "    \"Memory_CD8\",\n",
    "    \"Anergic_CD8\",\n",
    "    \"Naive_CD8\",\n",
    "    \"Hyperactivated_CD8\",\n",
    "    \"Proliferating_Effector\",\n",
    "    \"CD8\"\n",
    "]\n",
    "\n",
    "# Cohorts\n",
    "cohorts = [\"control\", \"short_term\", \"long_term\"]\n",
    "\n",
    "# Ordered timepoints\n",
    "all_timepoints = [\"Pre\", \"C1\", \"C2\", \"C4\", \"C6\", \"C9\", \"C18\", \"C36\"]\n",
    "\n",
    "# Color mapping for cohorts\n",
    "cohort_colors = {\n",
    "    'control': 'yellow',\n",
    "    'short_term': 'blue',\n",
    "    'long_term': 'red'\n",
    "}\n",
    "\n",
    "# Desired arrow length for unit vectors\n",
    "arrow_length = 0.5  # Adjust as needed\n",
    "\n",
    "# ---------------------------\n",
    "# Visualization Function\n",
    "# ---------------------------\n",
    "\n",
    "def optimal_transport_visualization(subpop_name, cohort_name):\n",
    "    input_folder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "    output_folder = os.path.join(base_output_dir, subpop_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Get list of available timepoint folders\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    timepoint_folders = sorted([f for f in os.listdir(input_folder) if f.startswith(\"Timepoint_\")])\n",
    "    available_timepoints = [tp.split(\"_\")[1] for tp in timepoint_folders]\n",
    "    \n",
    "    # Ensure timepoints are in the correct order\n",
    "    cohort_timepoints = [tp for tp in all_timepoints if tp in available_timepoints]\n",
    "    \n",
    "    # Check if there are any timepoints to process\n",
    "    if not cohort_timepoints:\n",
    "        print(f\"No timepoints available for {subpop_name} in {cohort_name} cohort.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize lists to store axes limits\n",
    "    all_x_coords = []\n",
    "    all_y_coords = []\n",
    "    \n",
    "    # First pass to collect coordinates for consistent axis limits\n",
    "    for tp in cohort_timepoints:\n",
    "        source_folder = os.path.join(input_folder, f\"Timepoint_{tp}\")\n",
    "        source_cells_file = os.path.join(source_folder, 'source_cells.csv')\n",
    "        gray_cells_file = os.path.join(source_folder, 'gray_cells.csv')\n",
    "        \n",
    "        # Check if source files exist\n",
    "        if not (os.path.exists(source_cells_file) and os.path.exists(gray_cells_file)):\n",
    "            print(f\"Missing files for timepoint {tp} in {cohort_name} cohort. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        source_cells = pd.read_csv(source_cells_file)\n",
    "        gray_cells = pd.read_csv(gray_cells_file)\n",
    "        \n",
    "        if source_cells.empty:\n",
    "            print(f\"No source cells for timepoint {tp} in {cohort_name} cohort.\")\n",
    "            continue\n",
    "        \n",
    "        # Collect coordinates (flipped)\n",
    "        all_x_coords.extend(gray_cells['UMAP_2'])\n",
    "        all_x_coords.extend(source_cells['UMAP_2'])\n",
    "        all_y_coords.extend(gray_cells['UMAP_1'])\n",
    "        all_y_coords.extend(source_cells['UMAP_1'])\n",
    "    \n",
    "    # Check if any coordinates were collected\n",
    "    if not all_x_coords or not all_y_coords:\n",
    "        print(f\"No coordinates found for {subpop_name} in {cohort_name} cohort.\")\n",
    "        return\n",
    "    \n",
    "    # Determine consistent axis limits\n",
    "    x_min, x_max = min(all_x_coords), max(all_x_coords)\n",
    "    y_min, y_max = min(all_y_coords), max(all_y_coords)\n",
    "    \n",
    "    # Start a PDF to save all plots in one file\n",
    "    output_file = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots.pdf\")\n",
    "    with PdfPages(output_file) as pdf:\n",
    "        # Create a figure with subplots arranged in one row\n",
    "        num_timepoints = len(cohort_timepoints)\n",
    "        fig, axes = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        \n",
    "        # Ensure axes is iterable\n",
    "        if num_timepoints == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for idx, ax in enumerate(axes):\n",
    "            source_tp = cohort_timepoints[idx]\n",
    "            source_folder = os.path.join(input_folder, f\"Timepoint_{source_tp}\")\n",
    "            \n",
    "            source_cells_file = os.path.join(source_folder, 'source_cells.csv')\n",
    "            gray_cells_file = os.path.join(source_folder, 'gray_cells.csv')\n",
    "            \n",
    "            # Check if source files exist\n",
    "            if not (os.path.exists(source_cells_file) and os.path.exists(gray_cells_file)):\n",
    "                print(f\"Missing files for timepoint {source_tp}. Skipping subplot.\")\n",
    "                ax.axis('off')\n",
    "                continue  # Skip if any source file is missing\n",
    "            \n",
    "            source_cells = pd.read_csv(source_cells_file)\n",
    "            gray_cells = pd.read_csv(gray_cells_file)\n",
    "            \n",
    "            if source_cells.empty:\n",
    "                print(f\"No source cells for timepoint {source_tp}. Skipping subplot.\")\n",
    "                ax.axis('off')\n",
    "                continue\n",
    "            \n",
    "            # Flip UMAP coordinates by swapping UMAP_1 and UMAP_2\n",
    "            gray_x = gray_cells['UMAP_2']\n",
    "            gray_y = gray_cells['UMAP_1']\n",
    "            source_x = source_cells['UMAP_2']\n",
    "            source_y = source_cells['UMAP_1']\n",
    "            \n",
    "            # Plot gray background cells\n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5, label='Background')\n",
    "            # Plot source cells\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1, label='Highlighted')\n",
    "            \n",
    "            # For all but the last timepoint, compute optimal transport and plot arrows\n",
    "            if idx < len(cohort_timepoints) - 1:\n",
    "                target_tp = cohort_timepoints[idx + 1]\n",
    "                target_folder = os.path.join(input_folder, f\"Timepoint_{target_tp}\")\n",
    "                \n",
    "                target_cells_file = os.path.join(target_folder, 'source_cells.csv')  # Target highlighted cells\n",
    "                \n",
    "                # Check if target_cells_file exists\n",
    "                if not os.path.exists(target_cells_file):\n",
    "                    print(f\"Missing target cells for timepoint {target_tp}. No arrows will be plotted for {source_tp}.\")\n",
    "                    ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "                    continue  # Skip if target file is missing\n",
    "                \n",
    "                target_cells = pd.read_csv(target_cells_file)\n",
    "                \n",
    "                if target_cells.empty:\n",
    "                    print(f\"No target cells for timepoint {target_tp}. No arrows will be plotted for {source_tp}.\")\n",
    "                    ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "                    continue\n",
    "                \n",
    "                # Extract UMAP coordinates (flipped)\n",
    "                source_coords = source_cells[['UMAP_2', 'UMAP_1']].values  # [x, y]\n",
    "                target_coords = target_cells[['UMAP_2', 'UMAP_1']].values  # [x, y]\n",
    "                \n",
    "                # Compute cost matrix (Euclidean distance)\n",
    "                cost_matrix = ot.dist(source_coords, target_coords, metric='euclidean')\n",
    "                \n",
    "                # Uniform weights for source and target\n",
    "                a = np.ones((source_coords.shape[0],)) / source_coords.shape[0]\n",
    "                b = np.ones((target_coords.shape[0],)) / target_coords.shape[0]\n",
    "                \n",
    "                # Compute optimal transport plan\n",
    "                transport_plan = ot.emd(a, b, cost_matrix)\n",
    "                \n",
    "                # Assign each source cell to the target cell with the highest transport probability\n",
    "                target_indices = np.argmax(transport_plan, axis=1)\n",
    "                \n",
    "                # Compute displacement vectors from source to assigned target\n",
    "                displacement_vectors = target_coords[target_indices] - source_coords\n",
    "                \n",
    "                # Normalize vectors to unit length\n",
    "                norms = np.linalg.norm(displacement_vectors, axis=1, keepdims=True)\n",
    "                norms[norms == 0] = 1  # Avoid division by zero\n",
    "                unit_vectors = displacement_vectors / norms\n",
    "                \n",
    "                # Scale unit vectors to desired length\n",
    "                scaled_vectors = unit_vectors * arrow_length\n",
    "                \n",
    "                # Draw unit length arrows\n",
    "                for j in range(len(source_coords)):\n",
    "                    ax.arrow(source_coords[j, 0], source_coords[j, 1],\n",
    "                             scaled_vectors[j, 0], scaled_vectors[j, 1],\n",
    "                             color='black', alpha=0.7, head_width=0.05, head_length=0.05, \n",
    "                             length_includes_head=True, linewidth=0.5)\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min-1, x_max+1)\n",
    "            ax.set_ylim(y_min-1, y_max+1)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        # Add a global title\n",
    "        fig.suptitle(f\"{subpop_name} - {cohort_name}\", fontsize=16)\n",
    "        \n",
    "        # Adjust layout to accommodate the global title\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        \n",
    "        # Save the figure to PDF\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution Loop\n",
    "# ---------------------------\n",
    "\n",
    "# Loop over subpopulations and cohorts\n",
    "for subpop_name in subpopulations:\n",
    "    for cohort_name in cohorts:\n",
    "        print(f\"Processing {subpop_name} - {cohort_name}\")\n",
    "        input_subfolder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "        if not os.path.exists(input_subfolder):\n",
    "            print(f\"Input subfolder does not exist: {input_subfolder}. Skipping.\")\n",
    "            continue  # Skip if input subfolder does not exist\n",
    "        optimal_transport_visualization(subpop_name, cohort_name)\n",
    "\n",
    "print(\"Visualization generation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7f7f0ca-f51c-4591-8b6c-9d5a4ac32927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Activated_CD4 - control\n",
      "Processing Activated_CD4 - short_term\n",
      "Processing Activated_CD4 - long_term\n",
      "Processing Effector_CD8 - control\n",
      "Processing Effector_CD8 - short_term\n",
      "Processing Effector_CD8 - long_term\n",
      "Processing Effector_Memory_CD8 - control\n",
      "Processing Effector_Memory_CD8 - short_term\n",
      "Processing Effector_Memory_CD8 - long_term\n",
      "Processing Exhausted_T - control\n",
      "Processing Exhausted_T - short_term\n",
      "Processing Exhausted_T - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Gamma_Delta_T - control\n",
      "Processing Gamma_Delta_T - short_term\n",
      "Processing Gamma_Delta_T - long_term\n",
      "Processing Active_CD4 - control\n",
      "Processing Active_CD4 - short_term\n",
      "Processing Active_CD4 - long_term\n",
      "Processing Naive_CD4 - control\n",
      "Processing Naive_CD4 - short_term\n",
      "Processing Naive_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD4 - control\n",
      "Processing Memory_CD4 - short_term\n",
      "Processing Memory_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD8 - control\n",
      "Processing Memory_CD8 - short_term\n",
      "Processing Memory_CD8 - long_term\n",
      "Processing Anergic_CD8 - control\n",
      "Processing Anergic_CD8 - short_term\n",
      "Processing Anergic_CD8 - long_term\n",
      "Processing Naive_CD8 - control\n",
      "Processing Naive_CD8 - short_term\n",
      "Processing Naive_CD8 - long_term\n",
      "Processing Hyperactivated_CD8 - control\n",
      "Processing Hyperactivated_CD8 - short_term\n",
      "Processing Hyperactivated_CD8 - long_term\n",
      "Processing Proliferating_Effector - control\n",
      "Processing Proliferating_Effector - short_term\n",
      "Processing Proliferating_Effector - long_term\n",
      "Processing CD8 - control\n",
      "Processing CD8 - short_term\n",
      "Processing CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization generation completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ot\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration Parameters\n",
    "# ---------------------------\n",
    "\n",
    "# Base directory where the CSV files are stored\n",
    "base_input_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/new/temp\"  # Replace with your actual path\n",
    "\n",
    "# Base directory to save the plots\n",
    "base_output_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/new/temp\"  # Replace with your desired output path\n",
    "\n",
    "# Subpopulations (should match the names used in R)\n",
    "subpopulations = [\n",
    "    \"Activated_CD4\",\n",
    "    \"Effector_CD8\",\n",
    "    \"Effector_Memory_CD8\",\n",
    "    \"Exhausted_T\",\n",
    "    \"Gamma_Delta_T\",\n",
    "    \"Active_CD4\",\n",
    "    \"Naive_CD4\",\n",
    "    \"Memory_CD4\",\n",
    "    \"Memory_CD8\",\n",
    "    \"Anergic_CD8\",\n",
    "    \"Naive_CD8\",\n",
    "    \"Hyperactivated_CD8\",\n",
    "    \"Proliferating_Effector\",\n",
    "    \"CD8\"\n",
    "]\n",
    "\n",
    "# Cohorts\n",
    "cohorts = [\"control\", \"short_term\", \"long_term\"]\n",
    "\n",
    "# Ordered timepoints\n",
    "all_timepoints = [\"Pre\", \"C1\", \"C2\", \"C4\", \"C6\", \"C9\", \"C18\", \"C36\"]\n",
    "\n",
    "# Color mapping for cohorts\n",
    "cohort_colors = {\n",
    "    'control': 'yellow',\n",
    "    'short_term': 'blue',\n",
    "    'long_term': 'red'\n",
    "}\n",
    "\n",
    "# Desired minimum and maximum arrow lengths for visualization\n",
    "min_arrow_length = 0.3  # Adjust as needed\n",
    "max_arrow_length = 2  # Adjust as needed\n",
    "\n",
    "# ---------------------------\n",
    "# Visualization Function\n",
    "# ---------------------------\n",
    "\n",
    "def optimal_transport_visualization(subpop_name, cohort_name):\n",
    "    input_folder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "    output_folder = os.path.join(base_output_dir, subpop_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Get list of available timepoint folders\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    timepoint_folders = sorted([f for f in os.listdir(input_folder) if f.startswith(\"Timepoint_\")])\n",
    "    available_timepoints = [tp.split(\"_\")[1] for tp in timepoint_folders]\n",
    "    \n",
    "    # Ensure timepoints are in the correct order\n",
    "    cohort_timepoints = [tp for tp in all_timepoints if tp in available_timepoints]\n",
    "    \n",
    "    # Check if there are any timepoints to process\n",
    "    if not cohort_timepoints:\n",
    "        print(f\"No timepoints available for {subpop_name} in {cohort_name} cohort.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize lists to store axes limits\n",
    "    all_x_coords = []\n",
    "    all_y_coords = []\n",
    "    \n",
    "    # First pass to collect coordinates for consistent axis limits\n",
    "    for tp in cohort_timepoints:\n",
    "        source_folder = os.path.join(input_folder, f\"Timepoint_{tp}\")\n",
    "        source_cells_file = os.path.join(source_folder, 'source_cells.csv')\n",
    "        gray_cells_file = os.path.join(source_folder, 'gray_cells.csv')\n",
    "        \n",
    "        # Check if source files exist\n",
    "        if not (os.path.exists(source_cells_file) and os.path.exists(gray_cells_file)):\n",
    "            print(f\"Missing files for timepoint {tp} in {cohort_name} cohort. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        source_cells = pd.read_csv(source_cells_file)\n",
    "        gray_cells = pd.read_csv(gray_cells_file)\n",
    "        \n",
    "        if source_cells.empty:\n",
    "            print(f\"No source cells for timepoint {tp} in {cohort_name} cohort.\")\n",
    "            continue\n",
    "        \n",
    "        # Collect coordinates (flipped)\n",
    "        all_x_coords.extend(gray_cells['UMAP_2'])\n",
    "        all_x_coords.extend(source_cells['UMAP_2'])\n",
    "        all_y_coords.extend(gray_cells['UMAP_1'])\n",
    "        all_y_coords.extend(source_cells['UMAP_1'])\n",
    "    \n",
    "    # Check if any coordinates were collected\n",
    "    if not all_x_coords or not all_y_coords:\n",
    "        print(f\"No coordinates found for {subpop_name} in {cohort_name} cohort.\")\n",
    "        return\n",
    "    \n",
    "    # Determine consistent axis limits\n",
    "    x_min, x_max = min(all_x_coords), max(all_x_coords)\n",
    "    y_min, y_max = min(all_y_coords), max(all_y_coords)\n",
    "    \n",
    "    # Start a PDF to save all plots in one file\n",
    "    output_file = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_differential_arrow_lengths.pdf\")\n",
    "    with PdfPages(output_file) as pdf:\n",
    "        # Create a figure with subplots arranged in one row\n",
    "        num_timepoints = len(cohort_timepoints)\n",
    "        fig, axes = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        \n",
    "        # Ensure axes is iterable\n",
    "        if num_timepoints == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for idx, ax in enumerate(axes):\n",
    "            source_tp = cohort_timepoints[idx]\n",
    "            source_folder = os.path.join(input_folder, f\"Timepoint_{source_tp}\")\n",
    "            \n",
    "            source_cells_file = os.path.join(source_folder, 'source_cells.csv')\n",
    "            gray_cells_file = os.path.join(source_folder, 'gray_cells.csv')\n",
    "            \n",
    "            # Check if source files exist\n",
    "            if not (os.path.exists(source_cells_file) and os.path.exists(gray_cells_file)):\n",
    "                print(f\"Missing files for timepoint {source_tp}. Skipping subplot.\")\n",
    "                ax.axis('off')\n",
    "                continue  # Skip if any source file is missing\n",
    "            \n",
    "            source_cells = pd.read_csv(source_cells_file)\n",
    "            gray_cells = pd.read_csv(gray_cells_file)\n",
    "            \n",
    "            if source_cells.empty:\n",
    "                print(f\"No source cells for timepoint {source_tp}. Skipping subplot.\")\n",
    "                ax.axis('off')\n",
    "                continue\n",
    "            \n",
    "            # Flip UMAP coordinates by swapping UMAP_1 and UMAP_2\n",
    "            gray_x = gray_cells['UMAP_2']\n",
    "            gray_y = gray_cells['UMAP_1']\n",
    "            source_x = source_cells['UMAP_2']\n",
    "            source_y = source_cells['UMAP_1']\n",
    "            \n",
    "            # Plot gray background cells with reduced size\n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5, label='Background')  # Reduced size\n",
    "            \n",
    "            # Plot source cells with reduced size\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1, label='Highlighted')  # Reduced size\n",
    "            \n",
    "            # For all but the last timepoint, compute optimal transport and plot arrows\n",
    "            if idx < len(cohort_timepoints) - 1:\n",
    "                target_tp = cohort_timepoints[idx + 1]\n",
    "                target_folder = os.path.join(input_folder, f\"Timepoint_{target_tp}\")\n",
    "                \n",
    "                target_cells_file = os.path.join(target_folder, 'source_cells.csv')  # Target highlighted cells\n",
    "                \n",
    "                # Check if target_cells_file exists\n",
    "                if not os.path.exists(target_cells_file):\n",
    "                    print(f\"Missing target cells for timepoint {target_tp}. No arrows will be plotted for {source_tp}.\")\n",
    "                    ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "                    continue  # Skip if target file is missing\n",
    "                \n",
    "                target_cells = pd.read_csv(target_cells_file)\n",
    "                \n",
    "                if target_cells.empty:\n",
    "                    print(f\"No target cells for timepoint {target_tp}. No arrows will be plotted for {source_tp}.\")\n",
    "                    ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "                    continue\n",
    "                \n",
    "                # Extract UMAP coordinates (flipped)\n",
    "                source_coords = source_cells[['UMAP_2', 'UMAP_1']].values  # [x, y]\n",
    "                target_coords = target_cells[['UMAP_2', 'UMAP_1']].values  # [x, y]\n",
    "                \n",
    "                # Compute cost matrix (Euclidean distance)\n",
    "                cost_matrix = ot.dist(source_coords, target_coords, metric='euclidean')\n",
    "                \n",
    "                # Uniform weights for source and target\n",
    "                a = np.ones((source_coords.shape[0],)) / source_coords.shape[0]\n",
    "                b = np.ones((target_coords.shape[0],)) / target_coords.shape[0]\n",
    "                \n",
    "                # Compute optimal transport plan with increased numItermax\n",
    "                try:\n",
    "                    transport_plan = ot.emd(a, b, cost_matrix, numItermax=100000)\n",
    "                except Exception as e:\n",
    "                    print(f\"Optimal transport computation failed for timepoint {source_tp} to {target_tp}: {e}\")\n",
    "                    ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "                    continue\n",
    "                \n",
    "                # Assign each source cell to the target cell with the highest transport probability\n",
    "                target_indices = np.argmax(transport_plan, axis=1)\n",
    "                \n",
    "                # Compute displacement vectors from source to assigned target\n",
    "                displacement_vectors = target_coords[target_indices] - source_coords\n",
    "                \n",
    "                # Compute norms (lengths) of displacement vectors\n",
    "                norms = np.linalg.norm(displacement_vectors, axis=1)\n",
    "                \n",
    "                # Map norms to desired arrow length range [min_arrow_length, max_arrow_length]\n",
    "                min_norm = np.min(norms)\n",
    "                max_norm = np.max(norms)\n",
    "                \n",
    "                if max_norm - min_norm > 0:\n",
    "                    arrow_lengths = ((norms - min_norm) / (max_norm - min_norm)) * (max_arrow_length - min_arrow_length) + min_arrow_length\n",
    "                else:\n",
    "                    arrow_lengths = np.full_like(norms, min_arrow_length)\n",
    "                \n",
    "                # Compute unit vectors\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    unit_vectors = displacement_vectors / norms[:, np.newaxis]\n",
    "                    unit_vectors[~np.isfinite(unit_vectors)] = 0  # Handle divisions by zero\n",
    "                \n",
    "                # Compute scaled vectors\n",
    "                scaled_vectors = unit_vectors * arrow_lengths[:, np.newaxis]\n",
    "                \n",
    "                # Draw arrows with lengths representing distances\n",
    "                for j in range(len(source_coords)):\n",
    "                    ax.arrow(source_coords[j, 0], source_coords[j, 1],\n",
    "                             scaled_vectors[j, 0], scaled_vectors[j, 1],\n",
    "                             color='black', alpha=0.7, head_width=0.05, head_length=0.05, \n",
    "                             length_includes_head=True, linewidth=0.5)\n",
    "                \n",
    "            # Set titles and limits\n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        # Add a global title\n",
    "        fig.suptitle(f\"{subpop_name} - {cohort_name}\", fontsize=16)\n",
    "        \n",
    "        # Adjust layout to accommodate the global title\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        \n",
    "        # Save the figure to PDF\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution Loop\n",
    "# ---------------------------\n",
    "\n",
    "# Loop over subpopulations and cohorts\n",
    "for subpop_name in subpopulations:\n",
    "    for cohort_name in cohorts:\n",
    "        print(f\"Processing {subpop_name} - {cohort_name}\")\n",
    "        input_subfolder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "        if not os.path.exists(input_subfolder):\n",
    "            print(f\"Input subfolder does not exist: {input_subfolder}. Skipping.\")\n",
    "            continue  # Skip if input subfolder does not exist\n",
    "        optimal_transport_visualization(subpop_name, cohort_name)\n",
    "\n",
    "print(\"Visualization generation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1156e99-97fb-40b0-909d-c8bfeabcb16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Activated_CD4 - control\n",
      "Minimum number of source cells across timepoints: 143\n",
      "Minimum number of gray cells across timepoints: 2165\n",
      "Processing Activated_CD4 - short_term\n",
      "Minimum number of source cells across timepoints: 445\n",
      "Minimum number of gray cells across timepoints: 1315\n",
      "Processing Activated_CD4 - long_term\n",
      "Minimum number of source cells across timepoints: 206\n",
      "Minimum number of gray cells across timepoints: 1474\n",
      "Processing Effector_CD8 - control\n",
      "Minimum number of source cells across timepoints: 449\n",
      "Minimum number of gray cells across timepoints: 1939\n",
      "Processing Effector_CD8 - short_term\n",
      "Minimum number of source cells across timepoints: 285\n",
      "Minimum number of gray cells across timepoints: 1479\n",
      "Processing Effector_CD8 - long_term\n",
      "Minimum number of source cells across timepoints: 571\n",
      "Minimum number of gray cells across timepoints: 1428\n",
      "Processing Effector_Memory_CD8 - control\n",
      "Minimum number of source cells across timepoints: 339\n",
      "Minimum number of gray cells across timepoints: 2049\n",
      "Processing Effector_Memory_CD8 - short_term\n",
      "Minimum number of source cells across timepoints: 214\n",
      "Minimum number of gray cells across timepoints: 1541\n",
      "Processing Effector_Memory_CD8 - long_term\n",
      "Minimum number of source cells across timepoints: 432\n",
      "Minimum number of gray cells across timepoints: 1519\n",
      "Processing Exhausted_T - control\n",
      "Minimum number of source cells across timepoints: 707\n",
      "Minimum number of gray cells across timepoints: 1681\n",
      "Processing Exhausted_T - short_term\n",
      "Minimum number of source cells across timepoints: 429\n",
      "Minimum number of gray cells across timepoints: 1411\n",
      "Processing Exhausted_T - long_term\n",
      "Minimum number of source cells across timepoints: 665\n",
      "Minimum number of gray cells across timepoints: 1312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Gamma_Delta_T - control\n",
      "Minimum number of source cells across timepoints: 248\n",
      "Minimum number of gray cells across timepoints: 2140\n",
      "Processing Gamma_Delta_T - short_term\n",
      "Minimum number of source cells across timepoints: 102\n",
      "Minimum number of gray cells across timepoints: 1639\n",
      "Processing Gamma_Delta_T - long_term\n",
      "Minimum number of source cells across timepoints: 438\n",
      "Minimum number of gray cells across timepoints: 1561\n",
      "Processing Active_CD4 - control\n",
      "Minimum number of source cells across timepoints: 327\n",
      "Minimum number of gray cells across timepoints: 2061\n",
      "Processing Active_CD4 - short_term\n",
      "Minimum number of source cells across timepoints: 310\n",
      "Minimum number of gray cells across timepoints: 1530\n",
      "Processing Active_CD4 - long_term\n",
      "Minimum number of source cells across timepoints: 357\n",
      "Minimum number of gray cells across timepoints: 1591\n",
      "Processing Naive_CD4 - control\n",
      "Minimum number of source cells across timepoints: 454\n",
      "Minimum number of gray cells across timepoints: 1934\n",
      "Processing Naive_CD4 - short_term\n",
      "Minimum number of source cells across timepoints: 246\n",
      "Minimum number of gray cells across timepoints: 1488\n",
      "Processing Naive_CD4 - long_term\n",
      "Minimum number of source cells across timepoints: 482\n",
      "Minimum number of gray cells across timepoints: 1451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD4 - control\n",
      "Minimum number of source cells across timepoints: 7\n",
      "Minimum number of gray cells across timepoints: 2377\n",
      "Processing Memory_CD4 - short_term\n",
      "Minimum number of source cells across timepoints: 10\n",
      "Minimum number of gray cells across timepoints: 1760\n",
      "Processing Memory_CD4 - long_term\n",
      "Minimum number of source cells across timepoints: 171\n",
      "Minimum number of gray cells across timepoints: 1768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD8 - control\n",
      "Minimum number of source cells across timepoints: 404\n",
      "Minimum number of gray cells across timepoints: 1982\n",
      "Processing Memory_CD8 - short_term\n",
      "Minimum number of source cells across timepoints: 308\n",
      "Minimum number of gray cells across timepoints: 1370\n",
      "Processing Memory_CD8 - long_term\n",
      "Minimum number of source cells across timepoints: 536\n",
      "Minimum number of gray cells across timepoints: 1463\n",
      "Processing Anergic_CD8 - control\n",
      "Minimum number of source cells across timepoints: 300\n",
      "Minimum number of gray cells across timepoints: 2076\n",
      "Processing Anergic_CD8 - short_term\n",
      "Minimum number of source cells across timepoints: 224\n",
      "Minimum number of gray cells across timepoints: 1475\n",
      "Processing Anergic_CD8 - long_term\n",
      "Minimum number of source cells across timepoints: 504\n",
      "Minimum number of gray cells across timepoints: 1487\n",
      "Processing Naive_CD8 - control\n",
      "Minimum number of source cells across timepoints: 161\n",
      "Minimum number of gray cells across timepoints: 2227\n",
      "Processing Naive_CD8 - short_term\n",
      "Minimum number of source cells across timepoints: 110\n",
      "Minimum number of gray cells across timepoints: 1724\n",
      "Processing Naive_CD8 - long_term\n",
      "Minimum number of source cells across timepoints: 129\n",
      "Minimum number of gray cells across timepoints: 1870\n",
      "Processing Hyperactivated_CD8 - control\n",
      "Minimum number of source cells across timepoints: 184\n",
      "Minimum number of gray cells across timepoints: 2220\n",
      "Processing Hyperactivated_CD8 - short_term\n",
      "Minimum number of source cells across timepoints: 56\n",
      "Minimum number of gray cells across timepoints: 1706\n",
      "Processing Hyperactivated_CD8 - long_term\n",
      "Minimum number of source cells across timepoints: 60\n",
      "Minimum number of gray cells across timepoints: 1942\n",
      "Processing Proliferating_Effector - control\n",
      "Minimum number of source cells across timepoints: 269\n",
      "Minimum number of gray cells across timepoints: 2119\n",
      "Processing Proliferating_Effector - short_term\n",
      "Minimum number of source cells across timepoints: 124\n",
      "Minimum number of gray cells across timepoints: 1703\n",
      "Processing Proliferating_Effector - long_term\n",
      "Minimum number of source cells across timepoints: 460\n",
      "Minimum number of gray cells across timepoints: 1539\n",
      "Processing CD8 - control\n",
      "Minimum number of source cells across timepoints: 532\n",
      "Minimum number of gray cells across timepoints: 1856\n",
      "Processing CD8 - short_term\n",
      "Minimum number of source cells across timepoints: 414\n",
      "Minimum number of gray cells across timepoints: 1426\n",
      "Processing CD8 - long_term\n",
      "Minimum number of source cells across timepoints: 632\n",
      "Minimum number of gray cells across timepoints: 1329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization generation completed.\n"
     ]
    }
   ],
   "source": [
    "# equal number of cells at each timepoint\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ot\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration Parameters\n",
    "# ---------------------------\n",
    "\n",
    "# Base directory where the CSV files are stored\n",
    "base_input_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/new/temp\"  # Replace with your actual path\n",
    "\n",
    "# Base directory to save the plots\n",
    "base_output_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/new/temp\"  # Replace with your desired output path\n",
    "\n",
    "# Subpopulations (should match the names used in R)\n",
    "subpopulations = [\n",
    "    \"Activated_CD4\",\n",
    "    \"Effector_CD8\",\n",
    "    \"Effector_Memory_CD8\",\n",
    "    \"Exhausted_T\",\n",
    "    \"Gamma_Delta_T\",\n",
    "    \"Active_CD4\",\n",
    "    \"Naive_CD4\",\n",
    "    \"Memory_CD4\",\n",
    "    \"Memory_CD8\",\n",
    "    \"Anergic_CD8\",\n",
    "    \"Naive_CD8\",\n",
    "    \"Hyperactivated_CD8\",\n",
    "    \"Proliferating_Effector\",\n",
    "    \"CD8\"\n",
    "]\n",
    "\n",
    "# Cohorts\n",
    "cohorts = [\"control\", \"short_term\", \"long_term\"]\n",
    "\n",
    "# Ordered timepoints\n",
    "all_timepoints = [\"Pre\", \"C1\", \"C2\", \"C4\", \"C6\", \"C9\", \"C18\", \"C36\"]\n",
    "\n",
    "# Color mapping for cohorts\n",
    "cohort_colors = {\n",
    "    'control': 'yellow',\n",
    "    'short_term': 'blue',\n",
    "    'long_term': 'red'\n",
    "}\n",
    "\n",
    "# Desired minimum and maximum arrow lengths for visualization\n",
    "min_arrow_length = 0.3  # Adjust as needed\n",
    "max_arrow_length = 2  # Adjust as needed\n",
    "\n",
    "# ---------------------------\n",
    "# Visualization Function\n",
    "# ---------------------------\n",
    "\n",
    "def optimal_transport_visualization(subpop_name, cohort_name):\n",
    "    input_folder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "    output_folder = os.path.join(base_output_dir, subpop_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Get list of available timepoint folders\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    timepoint_folders = sorted([f for f in os.listdir(input_folder) if f.startswith(\"Timepoint_\")])\n",
    "    available_timepoints = [tp.split(\"_\")[1] for tp in timepoint_folders]\n",
    "    \n",
    "    # Ensure timepoints are in the correct order\n",
    "    cohort_timepoints = [tp for tp in all_timepoints if tp in available_timepoints]\n",
    "    \n",
    "    # Check if there are any timepoints to process\n",
    "    if not cohort_timepoints:\n",
    "        print(f\"No timepoints available for {subpop_name} in {cohort_name} cohort.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize lists to store axes limits\n",
    "    all_x_coords = []\n",
    "    all_y_coords = []\n",
    "    \n",
    "    # Initialize a dictionary to store cell counts for each timepoint\n",
    "    cell_counts = {'source': {}, 'gray': {}}\n",
    "    \n",
    "    # First pass to collect coordinates for consistent axis limits and cell counts\n",
    "    for tp in cohort_timepoints:\n",
    "        source_folder = os.path.join(input_folder, f\"Timepoint_{tp}\")\n",
    "        source_cells_file = os.path.join(source_folder, 'source_cells.csv')\n",
    "        gray_cells_file = os.path.join(source_folder, 'gray_cells.csv')\n",
    "        \n",
    "        # Check if source files exist\n",
    "        if not (os.path.exists(source_cells_file) and os.path.exists(gray_cells_file)):\n",
    "            print(f\"Missing files for timepoint {tp} in {cohort_name} cohort. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        source_cells = pd.read_csv(source_cells_file)\n",
    "        gray_cells = pd.read_csv(gray_cells_file)\n",
    "        \n",
    "        if source_cells.empty:\n",
    "            print(f\"No source cells for timepoint {tp} in {cohort_name} cohort.\")\n",
    "            continue\n",
    "        \n",
    "        # Collect coordinates (flipped)\n",
    "        all_x_coords.extend(gray_cells['UMAP_2'])\n",
    "        all_x_coords.extend(source_cells['UMAP_2'])\n",
    "        all_y_coords.extend(gray_cells['UMAP_1'])\n",
    "        all_y_coords.extend(source_cells['UMAP_1'])\n",
    "        \n",
    "        # Store cell counts\n",
    "        cell_counts['source'][tp] = len(source_cells)\n",
    "        cell_counts['gray'][tp] = len(gray_cells)\n",
    "    \n",
    "    # Determine the minimum number of source and gray cells across all timepoints\n",
    "    if cell_counts['source']:\n",
    "        min_source_cells = min(cell_counts['source'].values())\n",
    "    else:\n",
    "        min_source_cells = 0\n",
    "    \n",
    "    if cell_counts['gray']:\n",
    "        min_gray_cells = min(cell_counts['gray'].values())\n",
    "    else:\n",
    "        min_gray_cells = 0\n",
    "    \n",
    "    # Handle cases where no cells are found\n",
    "    if min_source_cells == 0 or min_gray_cells == 0:\n",
    "        print(f\"Insufficient cells for uniform sampling in {subpop_name} - {cohort_name}. Skipping visualization.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Minimum number of source cells across timepoints: {min_source_cells}\")\n",
    "    print(f\"Minimum number of gray cells across timepoints: {min_gray_cells}\")\n",
    "    \n",
    "    # Determine consistent axis limits\n",
    "    x_min, x_max = min(all_x_coords), max(all_x_coords)\n",
    "    y_min, y_max = min(all_y_coords), max(all_y_coords)\n",
    "    \n",
    "    # Start a PDF to save all plots in one file\n",
    "    output_file = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_differential_arrow_lengths_equal_cells.pdf\")\n",
    "    with PdfPages(output_file) as pdf:\n",
    "        # Create a figure with subplots arranged in one row\n",
    "        num_timepoints = len(cohort_timepoints)\n",
    "        fig, axes = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        \n",
    "        # Ensure axes is iterable\n",
    "        if num_timepoints == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for idx, ax in enumerate(axes):\n",
    "            source_tp = cohort_timepoints[idx]\n",
    "            source_folder = os.path.join(input_folder, f\"Timepoint_{source_tp}\")\n",
    "            \n",
    "            source_cells_file = os.path.join(source_folder, 'source_cells.csv')\n",
    "            gray_cells_file = os.path.join(source_folder, 'gray_cells.csv')\n",
    "            \n",
    "            # Check if source files exist\n",
    "            if not (os.path.exists(source_cells_file) and os.path.exists(gray_cells_file)):\n",
    "                print(f\"Missing files for timepoint {source_tp}. Skipping subplot.\")\n",
    "                ax.axis('off')\n",
    "                continue  # Skip if any source file is missing\n",
    "            \n",
    "            source_cells = pd.read_csv(source_cells_file)\n",
    "            gray_cells = pd.read_csv(gray_cells_file)\n",
    "            \n",
    "            if source_cells.empty:\n",
    "                print(f\"No source cells for timepoint {source_tp}. Skipping subplot.\")\n",
    "                ax.axis('off')\n",
    "                continue\n",
    "            \n",
    "            # Sample source_cells and gray_cells to have uniform counts\n",
    "            if len(source_cells) >= min_source_cells:\n",
    "                sampled_source_cells = source_cells.sample(n=min_source_cells, random_state=42)\n",
    "            else:\n",
    "                print(f\"Not enough source cells in {source_tp} for uniform sampling. Skipping subplot.\")\n",
    "                ax.axis('off')\n",
    "                continue\n",
    "            \n",
    "            if len(gray_cells) >= min_gray_cells:\n",
    "                sampled_gray_cells = gray_cells.sample(n=min_gray_cells, random_state=42)\n",
    "            else:\n",
    "                print(f\"Not enough gray cells in {source_tp} for uniform sampling. Skipping subplot.\")\n",
    "                ax.axis('off')\n",
    "                continue\n",
    "            \n",
    "            # Flip UMAP coordinates by swapping UMAP_1 and UMAP_2\n",
    "            gray_x = sampled_gray_cells['UMAP_2']\n",
    "            gray_y = sampled_gray_cells['UMAP_1']\n",
    "            source_x = sampled_source_cells['UMAP_2']\n",
    "            source_y = sampled_source_cells['UMAP_1']\n",
    "            \n",
    "            # Plot gray background cells with reduced size\n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5, label='Background')  # Reduced size\n",
    "            \n",
    "            # Plot source cells with reduced size\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1, label='Highlighted')  # Reduced size\n",
    "            \n",
    "            # For all but the last timepoint, compute optimal transport and plot arrows\n",
    "            if idx < len(cohort_timepoints) - 1:\n",
    "                target_tp = cohort_timepoints[idx + 1]\n",
    "                target_folder = os.path.join(input_folder, f\"Timepoint_{target_tp}\")\n",
    "                \n",
    "                target_cells_file = os.path.join(target_folder, 'source_cells.csv')  # Target highlighted cells\n",
    "                \n",
    "                # Check if target_cells_file exists\n",
    "                if not os.path.exists(target_cells_file):\n",
    "                    print(f\"Missing target cells for timepoint {target_tp}. No arrows will be plotted for {source_tp}.\")\n",
    "                    ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "                    continue  # Skip if target file is missing\n",
    "                \n",
    "                target_cells = pd.read_csv(target_cells_file)\n",
    "                \n",
    "                if target_cells.empty:\n",
    "                    print(f\"No target cells for timepoint {target_tp}. No arrows will be plotted for {source_tp}.\")\n",
    "                    ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "                    continue\n",
    "                \n",
    "                # Extract UMAP coordinates (flipped)\n",
    "                source_coords = source_cells[['UMAP_2', 'UMAP_1']].values  # [x, y]\n",
    "                target_coords = target_cells[['UMAP_2', 'UMAP_1']].values  # [x, y]\n",
    "                \n",
    "                # Compute cost matrix (Euclidean distance)\n",
    "                cost_matrix = ot.dist(source_coords, target_coords, metric='euclidean')\n",
    "                \n",
    "                # Uniform weights for source and target\n",
    "                a = np.ones((source_coords.shape[0],)) / source_coords.shape[0]\n",
    "                b = np.ones((target_coords.shape[0],)) / target_coords.shape[0]\n",
    "                \n",
    "                # Compute optimal transport plan with increased numItermax\n",
    "                try:\n",
    "                    transport_plan = ot.emd(a, b, cost_matrix, numItermax=100000)\n",
    "                except Exception as e:\n",
    "                    print(f\"Optimal transport computation failed for timepoint {source_tp} to {target_tp}: {e}\")\n",
    "                    ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "                    continue\n",
    "                \n",
    "                # Assign each source cell to the target cell with the highest transport probability\n",
    "                target_indices = np.argmax(transport_plan, axis=1)\n",
    "                \n",
    "                # Compute displacement vectors from source to assigned target\n",
    "                displacement_vectors = target_coords[target_indices] - source_coords\n",
    "                \n",
    "                # Compute norms (lengths) of displacement vectors\n",
    "                norms = np.linalg.norm(displacement_vectors, axis=1)\n",
    "                \n",
    "                # Map norms to desired arrow length range [min_arrow_length, max_arrow_length]\n",
    "                min_norm = np.min(norms)\n",
    "                max_norm = np.max(norms)\n",
    "                \n",
    "                if max_norm - min_norm > 0:\n",
    "                    arrow_lengths = ((norms - min_norm) / (max_norm - min_norm)) * (max_arrow_length - min_arrow_length) + min_arrow_length\n",
    "                else:\n",
    "                    arrow_lengths = np.full_like(norms, min_arrow_length)\n",
    "                \n",
    "                # Compute unit vectors\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    unit_vectors = displacement_vectors / norms[:, np.newaxis]\n",
    "                    unit_vectors[~np.isfinite(unit_vectors)] = 0  # Handle divisions by zero\n",
    "                \n",
    "                # Compute scaled vectors\n",
    "                scaled_vectors = unit_vectors * arrow_lengths[:, np.newaxis]\n",
    "                \n",
    "                # Draw arrows with lengths representing distances\n",
    "                for j in range(len(source_coords)):\n",
    "                    ax.arrow(source_coords[j, 0], source_coords[j, 1],\n",
    "                             scaled_vectors[j, 0], scaled_vectors[j, 1],\n",
    "                             color='black', alpha=0.7, head_width=0.05, head_length=0.05, \n",
    "                             length_includes_head=True, linewidth=0.5)\n",
    "                \n",
    "            # Set titles and limits\n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        # Add a global title\n",
    "        fig.suptitle(f\"{subpop_name} - {cohort_name}\", fontsize=16)\n",
    "        \n",
    "        # Adjust layout to accommodate the global title\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        \n",
    "        # Save the figure to PDF\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution Loop\n",
    "# ---------------------------\n",
    "\n",
    "# Loop over subpopulations and cohorts\n",
    "for subpop_name in subpopulations:\n",
    "    for cohort_name in cohorts:\n",
    "        print(f\"Processing {subpop_name} - {cohort_name}\")\n",
    "        input_subfolder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "        if not os.path.exists(input_subfolder):\n",
    "            print(f\"Input subfolder does not exist: {input_subfolder}. Skipping.\")\n",
    "            continue  # Skip if input subfolder does not exist\n",
    "        optimal_transport_visualization(subpop_name, cohort_name)\n",
    "\n",
    "print(\"Visualization generation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfec787-b660-45b4-8fca-bbb0c52a38b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of cells to sample for single-cell plotting (to reduce overcrowding)\n",
    "# This can be the minimum number found across timepoints, or a fixed number.\n",
    "# Here we will determine a uniform sampling size based on the smallest dataset\n",
    "# found across timepoints, same as before, but only for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f446a51-9c5a-48a1-ae6e-cf1f9846d21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Activated_CD4 - control\n",
      "Processing Activated_CD4 - short_term\n",
      "Processing Activated_CD4 - long_term\n",
      "Processing Effector_CD8 - control\n",
      "Processing Effector_CD8 - short_term\n",
      "Processing Effector_CD8 - long_term\n",
      "Processing Effector_Memory_CD8 - control\n",
      "Processing Effector_Memory_CD8 - short_term\n",
      "Processing Effector_Memory_CD8 - long_term\n",
      "Processing Exhausted_T - control\n",
      "Processing Exhausted_T - short_term\n",
      "Processing Exhausted_T - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Gamma_Delta_T - control\n",
      "Processing Gamma_Delta_T - short_term\n",
      "Processing Gamma_Delta_T - long_term\n",
      "Processing Active_CD4 - control\n",
      "Processing Active_CD4 - short_term\n",
      "Processing Active_CD4 - long_term\n",
      "Processing Naive_CD4 - control\n",
      "Processing Naive_CD4 - short_term\n",
      "Processing Naive_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD4 - control\n",
      "Processing Memory_CD4 - short_term\n",
      "Processing Memory_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD8 - control\n",
      "Processing Memory_CD8 - short_term\n",
      "Processing Memory_CD8 - long_term\n",
      "Processing Anergic_CD8 - control\n",
      "Processing Anergic_CD8 - short_term\n",
      "Processing Anergic_CD8 - long_term\n",
      "Processing Naive_CD8 - control\n",
      "Processing Naive_CD8 - short_term\n",
      "Processing Naive_CD8 - long_term\n",
      "Processing Hyperactivated_CD8 - control\n",
      "Processing Hyperactivated_CD8 - short_term\n",
      "Processing Hyperactivated_CD8 - long_term\n",
      "Processing Proliferating_Effector - control\n",
      "Processing Proliferating_Effector - short_term\n",
      "Processing Proliferating_Effector - long_term\n",
      "Processing CD8 - control\n",
      "Processing CD8 - short_term\n",
      "Processing CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization generation completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ot\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration Parameters\n",
    "# ---------------------------\n",
    "\n",
    "# Base directory where the CSV files are stored\n",
    "base_input_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/new/temp\"  # Replace with your actual path\n",
    "\n",
    "# Base directory to save the plots\n",
    "base_output_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/new/temp\"  # Replace with your desired output path\n",
    "\n",
    "# Subpopulations (should match the names used in R)\n",
    "subpopulations = [\n",
    "    \"Activated_CD4\",\n",
    "    \"Effector_CD8\",\n",
    "    \"Effector_Memory_CD8\",\n",
    "    \"Exhausted_T\",\n",
    "    \"Gamma_Delta_T\",\n",
    "    \"Active_CD4\",\n",
    "    \"Naive_CD4\",\n",
    "    \"Memory_CD4\",\n",
    "    \"Memory_CD8\",\n",
    "    \"Anergic_CD8\",\n",
    "    \"Naive_CD8\",\n",
    "    \"Hyperactivated_CD8\",\n",
    "    \"Proliferating_Effector\",\n",
    "    \"CD8\"\n",
    "]\n",
    "\n",
    "# Cohorts\n",
    "cohorts = [\"control\", \"short_term\", \"long_term\"]\n",
    "\n",
    "# Ordered timepoints\n",
    "all_timepoints = [\"Pre\", \"C1\", \"C2\", \"C4\", \"C6\", \"C9\", \"C18\", \"C36\"]\n",
    "\n",
    "# Color mapping for cohorts\n",
    "cohort_colors = {\n",
    "    'control': 'yellow',\n",
    "    'short_term': 'blue',\n",
    "    'long_term': 'red'\n",
    "}\n",
    "\n",
    "# Desired minimum and maximum arrow lengths for visualization\n",
    "min_arrow_length = 0.3  # Adjust as needed\n",
    "max_arrow_length = 2    # Adjust as needed\n",
    "\n",
    "# Number of cells to sample for single-cell plotting (to reduce overcrowding)\n",
    "# This can be the minimum number found across timepoints, or a fixed number.\n",
    "# Here we will determine a uniform sampling size based on the smallest dataset\n",
    "# found across timepoints, same as before, but only for plotting.\n",
    "max_plot_cells = None  # will determine at runtime\n",
    "\n",
    "def optimal_transport_visualization(subpop_name, cohort_name):\n",
    "    input_folder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "    output_folder = os.path.join(base_output_dir, subpop_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    timepoint_folders = sorted([f for f in os.listdir(input_folder) if f.startswith(\"Timepoint_\")])\n",
    "    available_timepoints = [tp.split(\"_\")[1] for tp in timepoint_folders]\n",
    "    cohort_timepoints = [tp for tp in all_timepoints if tp in available_timepoints]\n",
    "    \n",
    "    if not cohort_timepoints:\n",
    "        print(f\"No timepoints available for {subpop_name} in {cohort_name} cohort.\")\n",
    "        return\n",
    "\n",
    "    # We'll store coordinates and also find global axis limits\n",
    "    all_x_coords = []\n",
    "    all_y_coords = []\n",
    "    \n",
    "    # Load all source and gray cells (full sets) for axis limits\n",
    "    # We also determine the smallest number of source and gray cells across timepoints\n",
    "    # for single-cell plotting (downsampling).\n",
    "    cell_counts_source = []\n",
    "    cell_counts_gray = []\n",
    "    \n",
    "    full_data = {}  # store full data for each timepoint\n",
    "    \n",
    "    for tp in cohort_timepoints:\n",
    "        source_folder = os.path.join(input_folder, f\"Timepoint_{tp}\")\n",
    "        source_cells_file = os.path.join(source_folder, 'source_cells.csv')\n",
    "        gray_cells_file = os.path.join(source_folder, 'gray_cells.csv')\n",
    "        \n",
    "        if not (os.path.exists(source_cells_file) and os.path.exists(gray_cells_file)):\n",
    "            print(f\"Missing files for {tp}. Skipping this timepoint.\")\n",
    "            continue\n",
    "        \n",
    "        source_cells = pd.read_csv(source_cells_file)\n",
    "        gray_cells = pd.read_csv(gray_cells_file)\n",
    "        \n",
    "        if source_cells.empty:\n",
    "            print(f\"No source cells for {tp}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Store full data\n",
    "        full_data[tp] = {\n",
    "            'source': source_cells,\n",
    "            'gray': gray_cells\n",
    "        }\n",
    "        \n",
    "        # For axis limits\n",
    "        all_x_coords.extend(gray_cells['UMAP_2'])\n",
    "        all_x_coords.extend(source_cells['UMAP_2'])\n",
    "        all_y_coords.extend(gray_cells['UMAP_1'])\n",
    "        all_y_coords.extend(source_cells['UMAP_1'])\n",
    "        \n",
    "        cell_counts_source.append(len(source_cells))\n",
    "        cell_counts_gray.append(len(gray_cells))\n",
    "    \n",
    "    if not full_data:\n",
    "        print(\"No valid timepoints with data.\")\n",
    "        return\n",
    "    \n",
    "    x_min, x_max = min(all_x_coords), max(all_x_coords)\n",
    "    y_min, y_max = min(all_y_coords), max(all_y_coords)\n",
    "    \n",
    "    # Determine number of cells to plot for single-cell arrows (downsample for plotting only)\n",
    "    # We use the minimum across all timepoints to keep it consistent.\n",
    "    min_source_cells = min(cell_counts_source) if cell_counts_source else 0\n",
    "    min_gray_cells = min(cell_counts_gray) if cell_counts_gray else 0\n",
    "    if min_source_cells == 0 or min_gray_cells == 0:\n",
    "        print(\"Insufficient cells. Skipping visualization.\")\n",
    "        return\n",
    "    \n",
    "    # We'll do OT and cluster arrow computations using the full sets,\n",
    "    # but only plot a subsample of source & gray cells (and their arrows) in the single-cell arrow PDF.\n",
    "    \n",
    "    # We'll store results for both single-cell (subsampled) plotting and cluster-level arrows (full).\n",
    "    timepoint_results = {}\n",
    "    \n",
    "    # We need to compute OT from each timepoint to the next (except the last one)\n",
    "    for i, source_tp in enumerate(cohort_timepoints):\n",
    "        source_data = full_data[source_tp]\n",
    "        source_cells = source_data['source']\n",
    "        \n",
    "        # Store initial data for plotting\n",
    "        # Downsample for plotting single-cell arrows only\n",
    "        sampled_source_cells = source_cells.sample(n=min_source_cells, random_state=42)\n",
    "        sampled_gray_cells = full_data[source_tp]['gray'].sample(n=min_gray_cells, random_state=42)\n",
    "        \n",
    "        timepoint_results[source_tp] = {\n",
    "            'sampled_source': sampled_source_cells,\n",
    "            'sampled_gray': sampled_gray_cells\n",
    "        }\n",
    "        \n",
    "        if i < len(cohort_timepoints) - 1:\n",
    "            target_tp = cohort_timepoints[i+1]\n",
    "            target_data = full_data[target_tp]\n",
    "            target_cells = target_data['source']\n",
    "            \n",
    "            if target_cells.empty:\n",
    "                # No target cells, no OT\n",
    "                continue\n",
    "            \n",
    "            # Compute OT on full sets, not sampled\n",
    "            full_source_coords = source_cells[['UMAP_2', 'UMAP_1']].values\n",
    "            full_target_coords = target_cells[['UMAP_2', 'UMAP_1']].values\n",
    "            \n",
    "            # Uniform distributions\n",
    "            a = np.ones((full_source_coords.shape[0],)) / full_source_coords.shape[0]\n",
    "            b = np.ones((full_target_coords.shape[0],)) / full_target_coords.shape[0]\n",
    "            \n",
    "            # Compute cost matrix\n",
    "            cost_matrix = ot.dist(full_source_coords, full_target_coords, metric='euclidean')\n",
    "            \n",
    "            try:\n",
    "                transport_plan = ot.emd(a, b, cost_matrix, numItermax=100000)\n",
    "            except Exception as e:\n",
    "                print(f\"OT computation failed for {source_tp} to {target_tp}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Assign each full source cell to a target cell\n",
    "            full_target_indices = np.argmax(transport_plan, axis=1)\n",
    "            displacement_vectors_full = full_target_coords[full_target_indices] - full_source_coords\n",
    "            \n",
    "            # Compute norms\n",
    "            norms_full = np.linalg.norm(displacement_vectors_full, axis=1)\n",
    "            \n",
    "            # Scale arrow lengths for cluster-level arrows\n",
    "            min_norm_full = np.min(norms_full)\n",
    "            max_norm_full = np.max(norms_full)\n",
    "            \n",
    "            if max_norm_full - min_norm_full > 0:\n",
    "                arrow_lengths_full = ((norms_full - min_norm_full) / (max_norm_full - min_norm_full)) * (max_arrow_length - min_arrow_length) + min_arrow_length\n",
    "            else:\n",
    "                arrow_lengths_full = np.full_like(norms_full, min_arrow_length)\n",
    "            \n",
    "            # For single-cell plotting, we also need displacement vectors of sampled sets\n",
    "            # Assign each sampled source cell to target cell based on the full transport plan\n",
    "            # We must find their indices in the full set\n",
    "            # Matching sampled source cells to full source:\n",
    "            sampled_source_indices = source_cells.index.get_indexer_for(sampled_source_cells.index)\n",
    "            sampled_displacements = displacement_vectors_full[sampled_source_indices, :]\n",
    "            sampled_norms = norms_full[sampled_source_indices]\n",
    "            \n",
    "            # Scale for sampled set plotting\n",
    "            min_norm_sampled = np.min(sampled_norms)\n",
    "            max_norm_sampled = np.max(sampled_norms)\n",
    "            if max_norm_sampled - min_norm_sampled > 0:\n",
    "                arrow_lengths_sampled = ((sampled_norms - min_norm_sampled) / (max_norm_sampled - min_norm_sampled)) * (max_arrow_length - min_arrow_length) + min_arrow_length\n",
    "            else:\n",
    "                arrow_lengths_sampled = np.full_like(sampled_norms, min_arrow_length)\n",
    "            \n",
    "            # Store single-cell arrow info (for plotting)\n",
    "            timepoint_results[source_tp]['single_cell_displacements'] = sampled_displacements\n",
    "            timepoint_results[source_tp]['single_cell_arrow_lengths'] = arrow_lengths_sampled\n",
    "            \n",
    "            clusters_of_interest = {1, 2, 3, 8, 10, 12, 14}\n",
    "\n",
    "            # Compute cluster-level arrows using full data (not sampled)\n",
    "            if 'seurat_clusters' in source_cells.columns:\n",
    "                # Replace 16 and 17 with 14 before computing cluster-level arrows\n",
    "                source_cells['seurat_clusters'] = source_cells['seurat_clusters'].replace({16: 14, 17: 14})\n",
    "                source_clusters_full = source_cells['seurat_clusters'].values\n",
    "                df_cluster_full = pd.DataFrame({\n",
    "                    'cluster': source_clusters_full,\n",
    "                    'sx': full_source_coords[:,0],\n",
    "                    'sy': full_source_coords[:,1],\n",
    "                    'dx': displacement_vectors_full[:,0],\n",
    "                    'dy': displacement_vectors_full[:,1],\n",
    "                    'norm': norms_full\n",
    "                })\n",
    "                \n",
    "                # Compute cluster aggregates\n",
    "                group = df_cluster_full.groupby('cluster')\n",
    "                centroids = group[['sx','sy']].mean()\n",
    "                mean_disp = group[['dx','dy']].mean()\n",
    "            \n",
    "                cluster_norms = np.sqrt(mean_disp['dx']**2 + mean_disp['dy']**2)\n",
    "                cn_min = cluster_norms.min()\n",
    "                cn_max = cluster_norms.max()\n",
    "                \n",
    "                # Ensure cluster_arrow_lengths is a Series\n",
    "                if cn_max - cn_min > 0:\n",
    "                    cluster_arrow_lengths = ((cluster_norms - cn_min) / (cn_max - cn_min)) * (max_arrow_length - min_arrow_length) + min_arrow_length\n",
    "                else:\n",
    "                    cluster_arrow_lengths = pd.Series(np.full_like(cluster_norms, min_arrow_length), index=cluster_norms.index)\n",
    "                \n",
    "                cluster_arrows = []\n",
    "                for clust in centroids.index:\n",
    "                    # Only show aggregated arrow if cluster is in clusters_of_interest\n",
    "                    if clust in clusters_of_interest:\n",
    "                        cx, cy = centroids.loc[clust, ['sx','sy']]\n",
    "                        cdx, cdy = mean_disp.loc[clust, ['dx','dy']]\n",
    "                        cnorm = np.sqrt(cdx**2 + cdy**2)\n",
    "                        if cnorm > 0:\n",
    "                            cdx = cdx / cnorm\n",
    "                            cdy = cdy / cnorm\n",
    "                        else:\n",
    "                            cdx, cdy = 0, 0\n",
    "            \n",
    "                        # Safely access cluster_arrow_lengths\n",
    "                        try:\n",
    "                            length = cluster_arrow_lengths.loc[clust]\n",
    "                        except AttributeError:\n",
    "                            # If this happens, convert cluster_arrow_lengths to Series and retry\n",
    "                            print(\"cluster_arrow_lengths is not a Series. Attempting to convert...\")\n",
    "                            if isinstance(cluster_arrow_lengths, np.ndarray):\n",
    "                                cluster_arrow_lengths = pd.Series(cluster_arrow_lengths, index=cluster_norms.index)\n",
    "                            try:\n",
    "                                length = cluster_arrow_lengths.loc[clust]\n",
    "                            except Exception as e:\n",
    "                                print(f\"Failed to access cluster_arrow_lengths for cluster {clust}: {e}\")\n",
    "                                length = min_arrow_length\n",
    "            \n",
    "                        cdx *= length\n",
    "                        cdy *= length\n",
    "                        cluster_arrows.append((cx, cy, cdx, cdy))\n",
    "                \n",
    "                timepoint_results[source_tp]['cluster_arrows'] = cluster_arrows\n",
    "            else:\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                print(f\"No 'seurat_clusters' column in {source_tp} source cells. Cannot compute cluster arrows.\")\n",
    "\n",
    "            \n",
    "        else:\n",
    "            # Last timepoint has no next timepoint\n",
    "            timepoint_results[source_tp]['single_cell_displacements'] = np.array([]) # no arrows\n",
    "            timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "            timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "            if 'seurat_clusters' in source_cells.columns:\n",
    "                # Still record that cluster info was present\n",
    "                # but no arrows since there's no next timepoint\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "            else:\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "    \n",
    "    # Now plot the original single-cell arrows PDF\n",
    "    output_file_original = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_differential_arrow_lengths_equal_cells.pdf\")\n",
    "    with PdfPages(output_file_original) as pdf_original:\n",
    "        num_timepoints = len(cohort_timepoints)\n",
    "        fig_orig, axes_orig = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        if num_timepoints == 1:\n",
    "            axes_orig = [axes_orig]\n",
    "        \n",
    "        for i, source_tp in enumerate(cohort_timepoints):\n",
    "            ax = axes_orig[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            sampled_source_cells = res['sampled_source']\n",
    "            sampled_gray_cells = res['sampled_gray']\n",
    "            \n",
    "            gray_x = sampled_gray_cells['UMAP_2'].values\n",
    "            gray_y = sampled_gray_cells['UMAP_1'].values\n",
    "            source_x = sampled_source_cells['UMAP_2'].values\n",
    "            source_y = sampled_source_cells['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "            \n",
    "            single_cell_displacements = res['single_cell_displacements']\n",
    "            single_cell_arrow_lengths = res['single_cell_arrow_lengths']\n",
    "            \n",
    "            if len(single_cell_displacements) > 0:\n",
    "                source_coords_sampled = sampled_source_cells[['UMAP_2','UMAP_1']].values\n",
    "                # Compute unit vectors for sampled\n",
    "                sampled_norms = np.linalg.norm(single_cell_displacements, axis=1)\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    unit_vectors = single_cell_displacements / sampled_norms[:, np.newaxis]\n",
    "                    unit_vectors[~np.isfinite(unit_vectors)] = 0\n",
    "                scaled_vectors = unit_vectors * single_cell_arrow_lengths[:, np.newaxis]\n",
    "                \n",
    "                for j in range(len(source_coords_sampled)):\n",
    "                    sx, sy = source_coords_sampled[j]\n",
    "                    dx, dy = scaled_vectors[j]\n",
    "                    ax.arrow(sx, sy, dx, dy,\n",
    "                             color='black', alpha=0.7, head_width=0.05, head_length=0.05,\n",
    "                             length_includes_head=True, linewidth=0.5)\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_orig.suptitle(f\"{subpop_name} - {cohort_name}\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_original.savefig(fig_orig)\n",
    "        plt.close(fig_orig)\n",
    "    \n",
    "    # Now plot aggregated cluster arrows in a new PDF\n",
    "    output_file_cluster = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_aggregated_cluster_arrows_equal_cells.pdf\")\n",
    "    with PdfPages(output_file_cluster) as pdf_cluster:\n",
    "        num_timepoints = len(cohort_timepoints)\n",
    "        fig_clust, axes_clust = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        if num_timepoints == 1:\n",
    "            axes_clust = [axes_clust]\n",
    "        \n",
    "        for i, source_tp in enumerate(cohort_timepoints):\n",
    "            ax = axes_clust[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            \n",
    "            # For cluster-level plot, we still show the same sampled source/gray points as background\n",
    "            gray_x = res['sampled_gray']['UMAP_2'].values\n",
    "            gray_y = res['sampled_gray']['UMAP_1'].values\n",
    "            source_x = res['sampled_source']['UMAP_2'].values\n",
    "            source_y = res['sampled_source']['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "            \n",
    "            cluster_arrows = res.get('cluster_arrows', [])\n",
    "            for (cx, cy, cdx, cdy) in cluster_arrows:\n",
    "                ax.arrow(cx, cy, cdx, cdy,\n",
    "                         color='black', alpha=0.7, head_width=0.1, head_length=0.1,\n",
    "                         length_includes_head=True, linewidth=1.0)\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_clust.suptitle(f\"{subpop_name} - {cohort_name} (Cluster-Level Arrows)\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_cluster.savefig(fig_clust)\n",
    "        plt.close(fig_clust)\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution Loop\n",
    "# ---------------------------\n",
    "\n",
    "for subpop_name in subpopulations:\n",
    "    for cohort_name in cohorts:\n",
    "        print(f\"Processing {subpop_name} - {cohort_name}\")\n",
    "        input_subfolder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "        if not os.path.exists(input_subfolder):\n",
    "            print(f\"Input subfolder does not exist: {input_subfolder}. Skipping.\")\n",
    "            continue\n",
    "        optimal_transport_visualization(subpop_name, cohort_name)\n",
    "\n",
    "print(\"Visualization generation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70624a45-656a-46a0-b990-d722784bd77a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff5a940-802e-4df6-99f4-277887ad3c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4970e10-c88b-4b75-8092-c20912734f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f7bbf8-f0df-4bb5-9a1b-dbab897975fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdb161c-1f2b-47b1-a427-470b29db153b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0381700-7c21-400b-a174-d50b870c548c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2af36c3d-d75c-4a0c-99ee-0385aa1041d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the following version we use PCs to calculate the distance instead of UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "625d40c5-8796-4d82-a9b0-a56751456288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Activated_CD4 - control\n",
      "Processing Activated_CD4 - short_term\n",
      "Processing Activated_CD4 - long_term\n",
      "Processing Effector_CD8 - control\n",
      "Processing Effector_CD8 - short_term\n",
      "Processing Effector_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Effector_Memory_CD8 - control\n",
      "Processing Effector_Memory_CD8 - short_term\n",
      "Processing Effector_Memory_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Exhausted_T - control\n",
      "Processing Exhausted_T - short_term\n",
      "Processing Exhausted_T - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Gamma_Delta_T - control\n",
      "Processing Gamma_Delta_T - short_term\n",
      "Processing Gamma_Delta_T - long_term\n",
      "Processing Active_CD4 - control\n",
      "Processing Active_CD4 - short_term\n",
      "Processing Active_CD4 - long_term\n",
      "Processing Naive_CD4 - control\n",
      "Processing Naive_CD4 - short_term\n",
      "Processing Naive_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD4 - control\n",
      "Processing Memory_CD4 - short_term\n",
      "Processing Memory_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD8 - control\n",
      "Processing Memory_CD8 - short_term\n",
      "Processing Memory_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Anergic_CD8 - control\n",
      "Processing Anergic_CD8 - short_term\n",
      "Processing Anergic_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Naive_CD8 - control\n",
      "Processing Naive_CD8 - short_term\n",
      "Processing Naive_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Hyperactivated_CD8 - control\n",
      "Missing files for Pre. Skipping this timepoint.\n",
      "Missing files for C1. Skipping this timepoint.\n",
      "Missing files for C2. Skipping this timepoint.\n",
      "No valid timepoints with data.\n",
      "Processing Hyperactivated_CD8 - short_term\n",
      "Missing files for Pre. Skipping this timepoint.\n",
      "Missing files for C1. Skipping this timepoint.\n",
      "Missing files for C2. Skipping this timepoint.\n",
      "Missing files for C4. Skipping this timepoint.\n",
      "No valid timepoints with data.\n",
      "Processing Hyperactivated_CD8 - long_term\n",
      "Missing files for Pre. Skipping this timepoint.\n",
      "Missing files for C1. Skipping this timepoint.\n",
      "Missing files for C2. Skipping this timepoint.\n",
      "Missing files for C4. Skipping this timepoint.\n",
      "Missing files for C6. Skipping this timepoint.\n",
      "Missing files for C9. Skipping this timepoint.\n",
      "Missing files for C18. Skipping this timepoint.\n",
      "Missing files for C36. Skipping this timepoint.\n",
      "No valid timepoints with data.\n",
      "Processing Proliferating_Effector - control\n",
      "Processing Proliferating_Effector - short_term\n",
      "Processing Proliferating_Effector - long_term\n",
      "Processing CD8 - control\n",
      "Processing CD8 - short_term\n",
      "Processing CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization generation completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ot\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration Parameters\n",
    "# ---------------------------\n",
    "\n",
    "# Base directory where the CSV files are stored\n",
    "base_input_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/new/temp\"  # Replace with your actual path\n",
    "\n",
    "# Base directory to save the plots\n",
    "base_output_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/new/temp\"  # Replace with your desired output path\n",
    "\n",
    "# Subpopulations (should match the names used in R)\n",
    "subpopulations = [\n",
    "    \"Activated_CD4\",\n",
    "    \"Effector_CD8\",\n",
    "    \"Effector_Memory_CD8\",\n",
    "    \"Exhausted_T\",\n",
    "    \"Gamma_Delta_T\",\n",
    "    \"Active_CD4\",\n",
    "    \"Naive_CD4\",\n",
    "    \"Memory_CD4\",\n",
    "    \"Memory_CD8\",\n",
    "    \"Anergic_CD8\",\n",
    "    \"Naive_CD8\",\n",
    "    \"Hyperactivated_CD8\",\n",
    "    \"Proliferating_Effector\",\n",
    "    \"CD8\"\n",
    "]\n",
    "\n",
    "# Cohorts\n",
    "cohorts = [\"control\", \"short_term\", \"long_term\"]\n",
    "\n",
    "# Ordered timepoints\n",
    "all_timepoints = [\"Pre\", \"C1\", \"C2\", \"C4\", \"C6\", \"C9\", \"C18\", \"C36\"]\n",
    "\n",
    "# Color mapping for cohorts\n",
    "cohort_colors = {\n",
    "    'control': 'yellow',\n",
    "    'short_term': 'blue',\n",
    "    'long_term': 'red'\n",
    "}\n",
    "\n",
    "# Desired minimum and maximum arrow lengths for visualization\n",
    "min_arrow_length = 0.3  # Adjust as needed\n",
    "max_arrow_length = 2    # Adjust as needed\n",
    "\n",
    "# Number of cells to sample for single-cell plotting (to reduce overcrowding)\n",
    "# This can be the minimum number found across timepoints, or a fixed number.\n",
    "# Here we will determine a uniform sampling size based on the smallest dataset\n",
    "# found across timepoints, same as before, but only for plotting.\n",
    "max_plot_cells = None  # will determine at runtime\n",
    "\n",
    "def optimal_transport_visualization(subpop_name, cohort_name):\n",
    "    input_folder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "    output_folder = os.path.join(base_output_dir, subpop_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    timepoint_folders = sorted([f for f in os.listdir(input_folder) if f.startswith(\"Timepoint_\")])\n",
    "    available_timepoints = [tp.split(\"_\")[1] for tp in timepoint_folders]\n",
    "    cohort_timepoints = [tp for tp in all_timepoints if tp in available_timepoints]\n",
    "    \n",
    "    if not cohort_timepoints:\n",
    "        print(f\"No timepoints available for {subpop_name} in {cohort_name} cohort.\")\n",
    "        return\n",
    "\n",
    "    # We'll store coordinates and also find global axis limits\n",
    "    all_x_coords = []\n",
    "    all_y_coords = []\n",
    "    \n",
    "    # Load all source and gray cells (full sets) for axis limits\n",
    "    # We also determine the smallest number of source and gray cells across timepoints\n",
    "    # for single-cell plotting (downsampling).\n",
    "    cell_counts_source = []\n",
    "    cell_counts_gray = []\n",
    "    \n",
    "    full_data = {}  # store full data for each timepoint\n",
    "    \n",
    "    for tp in cohort_timepoints:\n",
    "        source_folder = os.path.join(input_folder, f\"Timepoint_{tp}\")\n",
    "        source_cells_file = os.path.join(source_folder, 'source_cells_new.csv')\n",
    "        gray_cells_file = os.path.join(source_folder, 'gray_cells_new.csv')\n",
    "        \n",
    "        if not (os.path.exists(source_cells_file) and os.path.exists(gray_cells_file)):\n",
    "            print(f\"Missing files for {tp}. Skipping this timepoint.\")\n",
    "            continue\n",
    "        \n",
    "        source_cells = pd.read_csv(source_cells_file)\n",
    "        gray_cells = pd.read_csv(gray_cells_file)\n",
    "        \n",
    "        if source_cells.empty:\n",
    "            print(f\"No source cells for {tp}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Store full data\n",
    "        full_data[tp] = {\n",
    "            'source': source_cells,\n",
    "            'gray': gray_cells\n",
    "        }\n",
    "        \n",
    "        # For axis limits\n",
    "        all_x_coords.extend(gray_cells['UMAP_2'])\n",
    "        all_x_coords.extend(source_cells['UMAP_2'])\n",
    "        all_y_coords.extend(gray_cells['UMAP_1'])\n",
    "        all_y_coords.extend(source_cells['UMAP_1'])\n",
    "        \n",
    "        cell_counts_source.append(len(source_cells))\n",
    "        cell_counts_gray.append(len(gray_cells))\n",
    "    \n",
    "    if not full_data:\n",
    "        print(\"No valid timepoints with data.\")\n",
    "        return\n",
    "    \n",
    "    x_min, x_max = min(all_x_coords), max(all_x_coords)\n",
    "    y_min, y_max = min(all_y_coords), max(all_y_coords)\n",
    "    \n",
    "    # Determine number of cells to plot for single-cell arrows (downsample for plotting only)\n",
    "    # We use the minimum across all timepoints to keep it consistent.\n",
    "    min_source_cells = min(cell_counts_source) if cell_counts_source else 0\n",
    "    min_gray_cells = min(cell_counts_gray) if cell_counts_gray else 0\n",
    "    if min_source_cells == 0 or min_gray_cells == 0:\n",
    "        print(\"Insufficient cells. Skipping visualization.\")\n",
    "        return\n",
    "    \n",
    "    # We'll do OT and cluster arrow computations using the full sets,\n",
    "    # but only plot a subsample of source & gray cells (and their arrows) in the single-cell arrow PDF.\n",
    "    \n",
    "    # We'll store results for both single-cell (subsampled) plotting and cluster-level arrows (full).\n",
    "    timepoint_results = {}\n",
    "    \n",
    "    # We need to compute OT from each timepoint to the next (except the last one)\n",
    "    for i, source_tp in enumerate(cohort_timepoints):\n",
    "        source_data = full_data[source_tp]\n",
    "        source_cells = source_data['source']\n",
    "        \n",
    "        # Store initial data for plotting\n",
    "        # Downsample for plotting single-cell arrows only\n",
    "        sampled_source_cells = source_cells.sample(n=min_source_cells, random_state=42)\n",
    "        sampled_gray_cells = full_data[source_tp]['gray'].sample(n=min_gray_cells, random_state=42)\n",
    "        \n",
    "        timepoint_results[source_tp] = {\n",
    "            'sampled_source': sampled_source_cells,\n",
    "            'sampled_gray': sampled_gray_cells\n",
    "        }\n",
    "        \n",
    "        if i < len(cohort_timepoints) - 1:\n",
    "            target_tp = cohort_timepoints[i+1]\n",
    "            target_data = full_data[target_tp]\n",
    "            target_cells = target_data['source']\n",
    "            \n",
    "            if target_cells.empty:\n",
    "                # No target cells, no OT\n",
    "                continue\n",
    "            \n",
    "            pc_cols = [c for c in source_cells.columns if c.startswith('PC_')]\n",
    "            full_source_coords = source_cells[pc_cols].values\n",
    "            full_target_coords = target_cells[pc_cols].values\n",
    "            \n",
    "            full_source_coords_pc = source_cells[pc_cols].values\n",
    "            full_target_coords_pc = target_cells[pc_cols].values\n",
    "            \n",
    "            # Also store UMAP coordinates for plotting and displacement vectors\n",
    "            full_source_coords_umap = source_cells[['UMAP_2', 'UMAP_1']].values\n",
    "            full_target_coords_umap = target_cells[['UMAP_2', 'UMAP_1']].values\n",
    "            \n",
    "            # Uniform distributions\n",
    "            a = np.ones((full_source_coords.shape[0],)) / full_source_coords.shape[0]\n",
    "            b = np.ones((full_target_coords.shape[0],)) / full_target_coords.shape[0]\n",
    "            \n",
    "            # Compute cost matrix\n",
    "            cost_matrix = ot.dist(full_source_coords_pc, full_target_coords_pc, metric='euclidean')\n",
    "            \n",
    "            try:\n",
    "                transport_plan = ot.emd(a, b, cost_matrix, numItermax=100000)\n",
    "            except Exception as e:\n",
    "                print(f\"OT computation failed for {source_tp} to {target_tp}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Assign each full source cell to a target cell\n",
    "            full_target_indices = np.argmax(transport_plan, axis=1)\n",
    "            displacement_vectors_full = full_target_coords_umap[full_target_indices] - full_source_coords_umap\n",
    "            \n",
    "            # Compute norms\n",
    "            norms_full = np.linalg.norm(displacement_vectors_full, axis=1)\n",
    "            \n",
    "            # Scale arrow lengths for cluster-level arrows\n",
    "            min_norm_full = np.min(norms_full)\n",
    "            max_norm_full = np.max(norms_full)\n",
    "            \n",
    "            if max_norm_full - min_norm_full > 0:\n",
    "                arrow_lengths_full = ((norms_full - min_norm_full) / (max_norm_full - min_norm_full)) * (max_arrow_length - min_arrow_length) + min_arrow_length\n",
    "            else:\n",
    "                arrow_lengths_full = np.full_like(norms_full, min_arrow_length)\n",
    "            \n",
    "            # For single-cell plotting, we also need displacement vectors of sampled sets\n",
    "            # Assign each sampled source cell to target cell based on the full transport plan\n",
    "            # We must find their indices in the full set\n",
    "            # Matching sampled source cells to full source:\n",
    "            sampled_source_indices = source_cells.index.get_indexer_for(sampled_source_cells.index)\n",
    "            sampled_displacements = displacement_vectors_full[sampled_source_indices, :]\n",
    "            sampled_norms = norms_full[sampled_source_indices]\n",
    "            \n",
    "            # Scale for sampled set plotting\n",
    "            min_norm_sampled = np.min(sampled_norms)\n",
    "            max_norm_sampled = np.max(sampled_norms)\n",
    "            if max_norm_sampled - min_norm_sampled > 0:\n",
    "                arrow_lengths_sampled = ((sampled_norms - min_norm_sampled) / (max_norm_sampled - min_norm_sampled)) * (max_arrow_length - min_arrow_length) + min_arrow_length\n",
    "            else:\n",
    "                arrow_lengths_sampled = np.full_like(sampled_norms, min_arrow_length)\n",
    "            \n",
    "            # Store single-cell arrow info (for plotting)\n",
    "            timepoint_results[source_tp]['single_cell_displacements'] = sampled_displacements\n",
    "            timepoint_results[source_tp]['single_cell_arrow_lengths'] = arrow_lengths_sampled\n",
    "            \n",
    "            clusters_of_interest = {1, 2, 3, 8, 10, 12, 14}\n",
    "\n",
    "            # Compute cluster-level arrows using full data (not sampled)\n",
    "            if 'seurat_clusters' in source_cells.columns:\n",
    "                # Replace 16 and 17 with 14 before computing cluster-level arrows\n",
    "                source_cells['seurat_clusters'] = source_cells['seurat_clusters'].replace({16: 14, 17: 14})\n",
    "                source_clusters_full = source_cells['seurat_clusters'].values\n",
    "                df_cluster_full = pd.DataFrame({\n",
    "                    'cluster': source_clusters_full,\n",
    "                    'sx': full_source_coords_umap[:,0],\n",
    "                    'sy': full_source_coords_umap[:,1],\n",
    "                    'dx': displacement_vectors_full[:,0],\n",
    "                    'dy': displacement_vectors_full[:,1],\n",
    "                    'norm': norms_full\n",
    "                })\n",
    "                \n",
    "                # Combine source and gray cells\n",
    "                all_cells_combined = pd.concat([source_cells, gray_cells])\n",
    "                \n",
    "                # Replace 16 and 17 with 14 in the combined set as well\n",
    "                all_cells_combined['seurat_clusters'] = all_cells_combined['seurat_clusters'].replace({16:14,17:14})\n",
    "                \n",
    "                # Extract UMAP coordinates for all cells\n",
    "                all_clusters = all_cells_combined['seurat_clusters'].values\n",
    "                all_coords_umap = all_cells_combined[['UMAP_2', 'UMAP_1']].values\n",
    "                \n",
    "                # Create a DataFrame for centroid calculation\n",
    "                df_all = pd.DataFrame({\n",
    "                    'cluster': all_clusters,\n",
    "                    'sx': all_coords_umap[:,0],\n",
    "                    'sy': all_coords_umap[:,1]\n",
    "                })\n",
    "                \n",
    "                # Compute centroids from all cells (source + gray)\n",
    "                all_group = df_all.groupby('cluster')\n",
    "                # centroids = all_group[['sx','sy']].mean()\n",
    "                centroids = all_group[['sx','sy']].median()\n",
    "\n",
    "                \n",
    "                # Mean displacement vectors still come from source cells only\n",
    "                group = df_cluster_full.groupby('cluster')\n",
    "                mean_disp = group[['dx','dy']].mean()\n",
    "            \n",
    "                cluster_norms = np.sqrt(mean_disp['dx']**2 + mean_disp['dy']**2)\n",
    "                cn_min = cluster_norms.min()\n",
    "                cn_max = cluster_norms.max()\n",
    "                \n",
    "                # Ensure cluster_arrow_lengths is a Series\n",
    "                if cn_max - cn_min > 0:\n",
    "                    cluster_arrow_lengths = ((cluster_norms - cn_min) / (cn_max - cn_min)) * (max_arrow_length - min_arrow_length) + min_arrow_length\n",
    "                else:\n",
    "                    cluster_arrow_lengths = pd.Series(np.full_like(cluster_norms, min_arrow_length), index=cluster_norms.index)\n",
    "                \n",
    "                cluster_arrows = []\n",
    "\n",
    "                # Only consider clusters present in both centroids and mean_disp\n",
    "                common_clusters = centroids.index.intersection(mean_disp.index)\n",
    "                for clust in common_clusters:\n",
    "                    # Only show aggregated arrow if cluster is in clusters_of_interest\n",
    "                    if clust in clusters_of_interest:\n",
    "                        cx, cy = centroids.loc[clust, ['sx','sy']]\n",
    "                        cdx, cdy = mean_disp.loc[clust, ['dx','dy']]\n",
    "                        cnorm = np.sqrt(cdx**2 + cdy**2)\n",
    "                        if cnorm > 0:\n",
    "                            cdx = cdx / cnorm\n",
    "                            cdy = cdy / cnorm\n",
    "                        else:\n",
    "                            cdx, cdy = 0, 0\n",
    "            \n",
    "                        # Safely access cluster_arrow_lengths\n",
    "                        try:\n",
    "                            length = cluster_arrow_lengths.loc[clust]\n",
    "                        except AttributeError:\n",
    "                            # If this happens, convert cluster_arrow_lengths to Series and retry\n",
    "                            print(\"cluster_arrow_lengths is not a Series. Attempting to convert...\")\n",
    "                            if isinstance(cluster_arrow_lengths, np.ndarray):\n",
    "                                cluster_arrow_lengths = pd.Series(cluster_arrow_lengths, index=cluster_norms.index)\n",
    "                            try:\n",
    "                                length = cluster_arrow_lengths.loc[clust]\n",
    "                            except Exception as e:\n",
    "                                print(f\"Failed to access cluster_arrow_lengths for cluster {clust}: {e}\")\n",
    "                                length = min_arrow_length\n",
    "            \n",
    "                        cdx *= length\n",
    "                        cdy *= length\n",
    "                        cluster_arrows.append((cx, cy, cdx, cdy))\n",
    "                \n",
    "                timepoint_results[source_tp]['cluster_arrows'] = cluster_arrows\n",
    "            else:\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                print(f\"No 'seurat_clusters' column in {source_tp} source cells. Cannot compute cluster arrows.\")\n",
    "\n",
    "            \n",
    "        else:\n",
    "            # Last timepoint has no next timepoint\n",
    "            timepoint_results[source_tp]['single_cell_displacements'] = np.array([]) # no arrows\n",
    "            timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "            timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "            if 'seurat_clusters' in source_cells.columns:\n",
    "                # Still record that cluster info was present\n",
    "                # but no arrows since there's no next timepoint\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "            else:\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "    \n",
    "    # Now plot the original single-cell arrows PDF\n",
    "    output_file_original = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_differential_arrow_lengths_equal_cells_using_PCs.pdf\")\n",
    "    with PdfPages(output_file_original) as pdf_original:\n",
    "        num_timepoints = len(cohort_timepoints)\n",
    "        fig_orig, axes_orig = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        if num_timepoints == 1:\n",
    "            axes_orig = [axes_orig]\n",
    "        \n",
    "        for i, source_tp in enumerate(cohort_timepoints):\n",
    "            ax = axes_orig[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            sampled_source_cells = res['sampled_source']\n",
    "            sampled_gray_cells = res['sampled_gray']\n",
    "            \n",
    "            gray_x = sampled_gray_cells['UMAP_2'].values\n",
    "            gray_y = sampled_gray_cells['UMAP_1'].values\n",
    "            source_x = sampled_source_cells['UMAP_2'].values\n",
    "            source_y = sampled_source_cells['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "            \n",
    "            single_cell_displacements = res['single_cell_displacements']\n",
    "            single_cell_arrow_lengths = res['single_cell_arrow_lengths']\n",
    "            \n",
    "            if len(single_cell_displacements) > 0:\n",
    "                source_coords_sampled = sampled_source_cells[['UMAP_2','UMAP_1']].values\n",
    "                # Compute unit vectors for sampled\n",
    "                sampled_norms = np.linalg.norm(single_cell_displacements, axis=1)\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    unit_vectors = single_cell_displacements / sampled_norms[:, np.newaxis]\n",
    "                    unit_vectors[~np.isfinite(unit_vectors)] = 0\n",
    "                scaled_vectors = unit_vectors * single_cell_arrow_lengths[:, np.newaxis]\n",
    "                \n",
    "                for j in range(len(source_coords_sampled)):\n",
    "                    sx, sy = source_coords_sampled[j]\n",
    "                    dx, dy = scaled_vectors[j]\n",
    "                    ax.arrow(sx, sy, dx, dy,\n",
    "                             color='black', alpha=0.7, head_width=0.05, head_length=0.05,\n",
    "                             length_includes_head=True, linewidth=0.5)\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_orig.suptitle(f\"{subpop_name} - {cohort_name}\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_original.savefig(fig_orig)\n",
    "        plt.close(fig_orig)\n",
    "    \n",
    "    # Now plot aggregated cluster arrows in a new PDF\n",
    "    output_file_cluster = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_aggregated_cluster_arrows_equal_cells_using_PCs.pdf\")\n",
    "    with PdfPages(output_file_cluster) as pdf_cluster:\n",
    "        num_timepoints = len(cohort_timepoints)\n",
    "        fig_clust, axes_clust = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        if num_timepoints == 1:\n",
    "            axes_clust = [axes_clust]\n",
    "        \n",
    "        for i, source_tp in enumerate(cohort_timepoints):\n",
    "            ax = axes_clust[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            \n",
    "            # For cluster-level plot, we still show the same sampled source/gray points as background\n",
    "            gray_x = res['sampled_gray']['UMAP_2'].values\n",
    "            gray_y = res['sampled_gray']['UMAP_1'].values\n",
    "            source_x = res['sampled_source']['UMAP_2'].values\n",
    "            source_y = res['sampled_source']['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "\n",
    "            cluster_arrows = res.get('cluster_arrows', [])\n",
    "            for (cx, cy, cdx, cdy) in cluster_arrows:\n",
    "                ax.arrow(cx, cy, cdx, cdy,\n",
    "                         color='black', alpha=0.7, head_width=0.1, head_length=0.1,\n",
    "                         length_includes_head=True, linewidth=1.0)\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_clust.suptitle(f\"{subpop_name} - {cohort_name} (Cluster-Level Arrows)\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_cluster.savefig(fig_clust)\n",
    "        plt.close(fig_clust)\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution Loop\n",
    "# ---------------------------\n",
    "\n",
    "for subpop_name in subpopulations:\n",
    "    for cohort_name in cohorts:\n",
    "        print(f\"Processing {subpop_name} - {cohort_name}\")\n",
    "        input_subfolder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "        if not os.path.exists(input_subfolder):\n",
    "            print(f\"Input subfolder does not exist: {input_subfolder}. Skipping.\")\n",
    "            continue\n",
    "        optimal_transport_visualization(subpop_name, cohort_name)\n",
    "\n",
    "print(\"Visualization generation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "642c8a17-1ed6-4fe2-b085-7618ee915696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Activated_CD4 - control\n",
      "Processing Activated_CD4 - short_term\n",
      "Processing Activated_CD4 - long_term\n",
      "Processing Effector_CD8 - control\n",
      "Processing Effector_CD8 - short_term\n",
      "Processing Effector_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Effector_Memory_CD8 - control\n",
      "Processing Effector_Memory_CD8 - short_term\n",
      "Processing Effector_Memory_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Exhausted_T - control\n",
      "Processing Exhausted_T - short_term\n",
      "Processing Exhausted_T - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Gamma_Delta_T - control\n",
      "Processing Gamma_Delta_T - short_term\n",
      "Processing Gamma_Delta_T - long_term\n",
      "Processing Active_CD4 - control\n",
      "Processing Active_CD4 - short_term\n",
      "Processing Active_CD4 - long_term\n",
      "Processing Naive_CD4 - control\n",
      "Processing Naive_CD4 - short_term\n",
      "Processing Naive_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD4 - control\n",
      "Processing Memory_CD4 - short_term\n",
      "Processing Memory_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD8 - control\n",
      "Processing Memory_CD8 - short_term\n",
      "Processing Memory_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Anergic_CD8 - control\n",
      "Processing Anergic_CD8 - short_term\n",
      "Processing Anergic_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Naive_CD8 - control\n",
      "Processing Naive_CD8 - short_term\n",
      "Processing Naive_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Hyperactivated_CD8 - control\n",
      "Missing files for Pre. Skipping this timepoint.\n",
      "Missing files for C1. Skipping this timepoint.\n",
      "Missing files for C2. Skipping this timepoint.\n",
      "No valid timepoints with data.\n",
      "Processing Hyperactivated_CD8 - short_term\n",
      "Missing files for Pre. Skipping this timepoint.\n",
      "Missing files for C1. Skipping this timepoint.\n",
      "Missing files for C2. Skipping this timepoint.\n",
      "Missing files for C4. Skipping this timepoint.\n",
      "No valid timepoints with data.\n",
      "Processing Hyperactivated_CD8 - long_term\n",
      "Missing files for Pre. Skipping this timepoint.\n",
      "Missing files for C1. Skipping this timepoint.\n",
      "Missing files for C2. Skipping this timepoint.\n",
      "Missing files for C4. Skipping this timepoint.\n",
      "Missing files for C6. Skipping this timepoint.\n",
      "Missing files for C9. Skipping this timepoint.\n",
      "Missing files for C18. Skipping this timepoint.\n",
      "Missing files for C36. Skipping this timepoint.\n",
      "No valid timepoints with data.\n",
      "Processing Proliferating_Effector - control\n",
      "Processing Proliferating_Effector - short_term\n",
      "Processing Proliferating_Effector - long_term\n",
      "Processing CD8 - control\n",
      "Processing CD8 - short_term\n",
      "Processing CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization generation completed.\n"
     ]
    }
   ],
   "source": [
    "# remove previous, if this works\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ot\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration Parameters\n",
    "# ---------------------------\n",
    "\n",
    "# Base directory where the CSV files are stored\n",
    "base_input_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/new/temp\"  # Replace with your actual path\n",
    "\n",
    "# Base directory to save the plots\n",
    "base_output_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/new/temp\"  # Replace with your desired output path\n",
    "\n",
    "# Subpopulations (should match the names used in R)\n",
    "subpopulations = [\n",
    "    \"Activated_CD4\",\n",
    "    \"Effector_CD8\",\n",
    "    \"Effector_Memory_CD8\",\n",
    "    \"Exhausted_T\",\n",
    "    \"Gamma_Delta_T\",\n",
    "    \"Active_CD4\",\n",
    "    \"Naive_CD4\",\n",
    "    \"Memory_CD4\",\n",
    "    \"Memory_CD8\",\n",
    "    \"Anergic_CD8\",\n",
    "    \"Naive_CD8\",\n",
    "    \"Hyperactivated_CD8\",\n",
    "    \"Proliferating_Effector\",\n",
    "    \"CD8\"\n",
    "]\n",
    "\n",
    "# Cohorts\n",
    "cohorts = [\"control\", \"short_term\", \"long_term\"]\n",
    "\n",
    "# Ordered timepoints\n",
    "all_timepoints = [\"Pre\", \"C1\", \"C2\", \"C4\", \"C6\", \"C9\", \"C18\", \"C36\"]\n",
    "\n",
    "# Color mapping for cohorts\n",
    "cohort_colors = {\n",
    "    'control': 'yellow',\n",
    "    'short_term': 'blue',\n",
    "    'long_term': 'red'\n",
    "}\n",
    "\n",
    "# Desired minimum and maximum arrow lengths for visualization\n",
    "min_arrow_length = 0.3  # Adjust as needed\n",
    "max_arrow_length = 2    # Adjust as needed\n",
    "\n",
    "# Number of cells to sample for single-cell plotting (to reduce overcrowding)\n",
    "max_plot_cells = None  # will determine at runtime\n",
    "\n",
    "def optimal_transport_visualization(subpop_name, cohort_name):\n",
    "    input_folder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "    output_folder = os.path.join(base_output_dir, subpop_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    timepoint_folders = sorted([f for f in os.listdir(input_folder) if f.startswith(\"Timepoint_\")])\n",
    "    available_timepoints = [tp.split(\"_\")[1] for tp in timepoint_folders]\n",
    "    cohort_timepoints = [tp for tp in all_timepoints if tp in available_timepoints]\n",
    "    \n",
    "    if not cohort_timepoints:\n",
    "        print(f\"No timepoints available for {subpop_name} in {cohort_name} cohort.\")\n",
    "        return\n",
    "\n",
    "    # We'll store coordinates and also find global axis limits\n",
    "    all_x_coords = []\n",
    "    all_y_coords = []\n",
    "    \n",
    "    cell_counts_source = []\n",
    "    cell_counts_gray = []\n",
    "    full_data = {}  # store full data for each timepoint\n",
    "    \n",
    "    for tp in cohort_timepoints:\n",
    "        source_folder = os.path.join(input_folder, f\"Timepoint_{tp}\")\n",
    "        source_cells_file = os.path.join(source_folder, 'source_cells_new.csv')\n",
    "        gray_cells_file = os.path.join(source_folder, 'gray_cells_new.csv')\n",
    "        \n",
    "        if not (os.path.exists(source_cells_file) and os.path.exists(gray_cells_file)):\n",
    "            print(f\"Missing files for {tp}. Skipping this timepoint.\")\n",
    "            continue\n",
    "        \n",
    "        source_cells = pd.read_csv(source_cells_file)\n",
    "        gray_cells = pd.read_csv(gray_cells_file)\n",
    "        \n",
    "        if source_cells.empty:\n",
    "            print(f\"No source cells for {tp}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Store full data\n",
    "        full_data[tp] = {\n",
    "            'source': source_cells,\n",
    "            'gray': gray_cells\n",
    "        }\n",
    "        \n",
    "        # For axis limits\n",
    "        all_x_coords.extend(gray_cells['UMAP_2'])\n",
    "        all_x_coords.extend(source_cells['UMAP_2'])\n",
    "        all_y_coords.extend(gray_cells['UMAP_1'])\n",
    "        all_y_coords.extend(source_cells['UMAP_1'])\n",
    "        \n",
    "        cell_counts_source.append(len(source_cells))\n",
    "        cell_counts_gray.append(len(gray_cells))\n",
    "    \n",
    "    if not full_data:\n",
    "        print(\"No valid timepoints with data.\")\n",
    "        return\n",
    "    \n",
    "    x_min, x_max = min(all_x_coords), max(all_x_coords)\n",
    "    y_min, y_max = min(all_y_coords), max(all_y_coords)\n",
    "    \n",
    "    # Determine number of cells to plot for single-cell arrows (downsample for plotting only)\n",
    "    min_source_cells = min(cell_counts_source) if cell_counts_source else 0\n",
    "    min_gray_cells = min(cell_counts_gray) if cell_counts_gray else 0\n",
    "    if min_source_cells == 0 or min_gray_cells == 0:\n",
    "        print(\"Insufficient cells. Skipping visualization.\")\n",
    "        return\n",
    "    \n",
    "    timepoint_results = {}\n",
    "    \n",
    "    # Compute OT from each timepoint to the next (except the last one)\n",
    "    for i, source_tp in enumerate(cohort_timepoints):\n",
    "        source_data = full_data[source_tp]\n",
    "        source_cells = source_data['source']\n",
    "        \n",
    "        # Downsample for plotting single-cell arrows only\n",
    "        sampled_source_cells = source_cells.sample(n=min_source_cells, random_state=42)\n",
    "        sampled_gray_cells = full_data[source_tp]['gray'].sample(n=min_gray_cells, random_state=42)\n",
    "        \n",
    "        timepoint_results[source_tp] = {\n",
    "            'sampled_source': sampled_source_cells,\n",
    "            'sampled_gray': sampled_gray_cells\n",
    "        }\n",
    "        \n",
    "        if i < len(cohort_timepoints) - 1:\n",
    "            target_tp = cohort_timepoints[i+1]\n",
    "            target_data = full_data[target_tp]\n",
    "            target_cells = target_data['source']\n",
    "            \n",
    "            if target_cells.empty:\n",
    "                continue\n",
    "            \n",
    "            pc_cols = [c for c in source_cells.columns if c.startswith('PC_')]\n",
    "            full_source_coords_pc = source_cells[pc_cols].values\n",
    "            full_target_coords_pc = target_cells[pc_cols].values\n",
    "            \n",
    "            # Also store UMAP coordinates\n",
    "            full_source_coords_umap = source_cells[['UMAP_2', 'UMAP_1']].values\n",
    "            full_target_coords_umap = target_cells[['UMAP_2', 'UMAP_1']].values\n",
    "            \n",
    "            # Uniform distributions\n",
    "            a = np.ones((full_source_coords_pc.shape[0],)) / full_source_coords_pc.shape[0]\n",
    "            b = np.ones((full_target_coords_pc.shape[0],)) / full_target_coords_pc.shape[0]\n",
    "            \n",
    "            # Compute cost matrix\n",
    "            cost_matrix = ot.dist(full_source_coords_pc, full_target_coords_pc, metric='euclidean')\n",
    "            \n",
    "            try:\n",
    "                transport_plan = ot.emd(a, b, cost_matrix, numItermax=100000)\n",
    "            except Exception as e:\n",
    "                print(f\"OT computation failed for {source_tp} to {target_tp}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            full_target_indices = np.argmax(transport_plan, axis=1)\n",
    "            displacement_vectors_full = full_target_coords_umap[full_target_indices] - full_source_coords_umap\n",
    "            \n",
    "            # Compute norms\n",
    "            norms_full = np.linalg.norm(displacement_vectors_full, axis=1)\n",
    "            min_norm_full = np.min(norms_full)\n",
    "            max_norm_full = np.max(norms_full)\n",
    "            \n",
    "            if max_norm_full - min_norm_full > 0:\n",
    "                arrow_lengths_full = ((norms_full - min_norm_full) / (max_norm_full - min_norm_full)) * (max_arrow_length - min_arrow_length) + min_arrow_length\n",
    "            else:\n",
    "                arrow_lengths_full = np.full_like(norms_full, min_arrow_length)\n",
    "            \n",
    "            # For single-cell plotting\n",
    "            sampled_source_indices = source_cells.index.get_indexer_for(sampled_source_cells.index)\n",
    "            sampled_displacements = displacement_vectors_full[sampled_source_indices, :]\n",
    "            sampled_norms = norms_full[sampled_source_indices]\n",
    "            \n",
    "            min_norm_sampled = np.min(sampled_norms)\n",
    "            max_norm_sampled = np.max(sampled_norms)\n",
    "            if max_norm_sampled - min_norm_sampled > 0:\n",
    "                arrow_lengths_sampled = ((sampled_norms - min_norm_sampled) / (max_norm_sampled - min_norm_sampled)) * (max_arrow_length - min_arrow_length) + min_arrow_length\n",
    "            else:\n",
    "                arrow_lengths_sampled = np.full_like(sampled_norms, min_arrow_length)\n",
    "            \n",
    "            timepoint_results[source_tp]['single_cell_displacements'] = sampled_displacements\n",
    "            timepoint_results[source_tp]['single_cell_arrow_lengths'] = arrow_lengths_sampled\n",
    "            \n",
    "            clusters_of_interest = {1, 2, 3, 8, 10, 12, 14}\n",
    "\n",
    "            # Compute cluster-level arrows using full data\n",
    "            if 'seurat_clusters' in source_cells.columns:\n",
    "                source_cells['seurat_clusters'] = source_cells['seurat_clusters'].replace({16: 14, 17: 14})\n",
    "                source_clusters_full = source_cells['seurat_clusters'].values\n",
    "                df_cluster_full = pd.DataFrame({\n",
    "                    'cluster': source_clusters_full,\n",
    "                    'sx': full_source_coords_umap[:,0],\n",
    "                    'sy': full_source_coords_umap[:,1],\n",
    "                    'dx': displacement_vectors_full[:,0],\n",
    "                    'dy': displacement_vectors_full[:,1],\n",
    "                    'norm': norms_full\n",
    "                })\n",
    "                \n",
    "                gray_cells = full_data[source_tp]['gray']\n",
    "                all_cells_combined = pd.concat([source_cells, gray_cells])\n",
    "                all_cells_combined['seurat_clusters'] = all_cells_combined['seurat_clusters'].replace({16:14,17:14})\n",
    "                \n",
    "                all_clusters = all_cells_combined['seurat_clusters'].values\n",
    "                all_coords_umap = all_cells_combined[['UMAP_2', 'UMAP_1']].values\n",
    "                \n",
    "                df_all = pd.DataFrame({\n",
    "                    'cluster': all_clusters,\n",
    "                    'sx': all_coords_umap[:,0],\n",
    "                    'sy': all_coords_umap[:,1]\n",
    "                })\n",
    "                \n",
    "                # Compute centroids from all cells (source + gray)\n",
    "                all_group = df_all.groupby('cluster')\n",
    "                centroids = all_group[['sx','sy']].median()\n",
    "                \n",
    "                # Mean displacement from source cells only\n",
    "                group = df_cluster_full.groupby('cluster')\n",
    "                mean_disp = group[['dx','dy']].mean()\n",
    "            \n",
    "                cluster_norms = np.sqrt(mean_disp['dx']**2 + mean_disp['dy']**2)\n",
    "                cn_min = cluster_norms.min()\n",
    "                cn_max = cluster_norms.max()\n",
    "                \n",
    "                if cn_max - cn_min > 0:\n",
    "                    cluster_arrow_lengths = ((cluster_norms - cn_min) / (cn_max - cn_min)) * (max_arrow_length - min_arrow_length) + min_arrow_length\n",
    "                else:\n",
    "                    cluster_arrow_lengths = pd.Series(np.full_like(cluster_norms, min_arrow_length), index=cluster_norms.index)\n",
    "                \n",
    "                cluster_arrows = []\n",
    "                common_clusters = centroids.index.intersection(mean_disp.index)\n",
    "                for clust in common_clusters:\n",
    "                    if clust in clusters_of_interest:\n",
    "                        cx, cy = centroids.loc[clust, ['sx','sy']]\n",
    "                        cdx, cdy = mean_disp.loc[clust, ['dx','dy']]\n",
    "                        cnorm = np.sqrt(cdx**2 + cdy**2)\n",
    "                        if cnorm > 0:\n",
    "                            cdx = cdx / cnorm\n",
    "                            cdy = cdy / cnorm\n",
    "                        else:\n",
    "                            cdx, cdy = 0, 0\n",
    "                        \n",
    "                        try:\n",
    "                            length = cluster_arrow_lengths.loc[clust]\n",
    "                        except AttributeError:\n",
    "                            if isinstance(cluster_arrow_lengths, np.ndarray):\n",
    "                                cluster_arrow_lengths = pd.Series(cluster_arrow_lengths, index=cluster_norms.index)\n",
    "                            try:\n",
    "                                length = cluster_arrow_lengths.loc[clust]\n",
    "                            except Exception as e:\n",
    "                                print(f\"Failed to access cluster_arrow_lengths for cluster {clust}: {e}\")\n",
    "                                length = min_arrow_length\n",
    "            \n",
    "                        cdx *= length\n",
    "                        cdy *= length\n",
    "                        # Store cluster ID alongside arrow coordinates for line width calculation later\n",
    "                        cluster_arrows.append((clust, cx, cy, cdx, cdy))\n",
    "                \n",
    "                timepoint_results[source_tp]['cluster_arrows'] = cluster_arrows\n",
    "\n",
    "                if len(cluster_arrows) > 0:\n",
    "                    # Instead of global centroid of all cells, use the median of source cells only:\n",
    "                    source_median_x = source_cells['UMAP_2'].median()\n",
    "                    source_median_y = source_cells['UMAP_1'].median()\n",
    "\n",
    "                    # Sum displacement vectors of all cluster arrows\n",
    "                    total_dx = sum([arrow[3] for arrow in cluster_arrows])\n",
    "                    total_dy = sum([arrow[4] for arrow in cluster_arrows])\n",
    "\n",
    "                    # Store the aggregated arrow\n",
    "                    timepoint_results[source_tp]['aggregated_arrow'] = (source_median_x, source_median_y, total_dx, total_dy)\n",
    "                else:\n",
    "                    # If no cluster arrows, no aggregated arrow\n",
    "                    timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "            else:\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "                print(f\"No 'seurat_clusters' column in {source_tp} source cells. Cannot compute cluster arrows.\")\n",
    "\n",
    "            \n",
    "        else:\n",
    "            # Last timepoint has no next timepoint\n",
    "            timepoint_results[source_tp]['single_cell_displacements'] = np.array([])\n",
    "            timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "            timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "            timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "            if 'seurat_clusters' in source_cells.columns:\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "            else:\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "    \n",
    "    # Plot single-cell arrows PDF\n",
    "    output_file_original = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_differential_arrow_lengths_equal_cells_using_PCs.pdf\")\n",
    "    with PdfPages(output_file_original) as pdf_original:\n",
    "        num_timepoints = len(cohort_timepoints)\n",
    "        fig_orig, axes_orig = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        if num_timepoints == 1:\n",
    "            axes_orig = [axes_orig]\n",
    "        \n",
    "        for i, source_tp in enumerate(cohort_timepoints):\n",
    "            ax = axes_orig[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            sampled_source_cells = res['sampled_source']\n",
    "            sampled_gray_cells = res['sampled_gray']\n",
    "            \n",
    "            gray_x = sampled_gray_cells['UMAP_2'].values\n",
    "            gray_y = sampled_gray_cells['UMAP_1'].values\n",
    "            source_x = sampled_source_cells['UMAP_2'].values\n",
    "            source_y = sampled_source_cells['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "            \n",
    "            single_cell_displacements = res['single_cell_displacements']\n",
    "            single_cell_arrow_lengths = res['single_cell_arrow_lengths']\n",
    "            \n",
    "            if len(single_cell_displacements) > 0:\n",
    "                source_coords_sampled = sampled_source_cells[['UMAP_2','UMAP_1']].values\n",
    "                sampled_norms = np.linalg.norm(single_cell_displacements, axis=1)\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    unit_vectors = single_cell_displacements / sampled_norms[:, np.newaxis]\n",
    "                    unit_vectors[~np.isfinite(unit_vectors)] = 0\n",
    "                scaled_vectors = unit_vectors * single_cell_arrow_lengths[:, np.newaxis]\n",
    "                \n",
    "                for j in range(len(source_coords_sampled)):\n",
    "                    sx, sy = source_coords_sampled[j]\n",
    "                    dx, dy = scaled_vectors[j]\n",
    "                    ax.arrow(sx, sy, dx, dy,\n",
    "                             color='black', alpha=0.7, head_width=0.05, head_length=0.05,\n",
    "                             length_includes_head=True, linewidth=0.5)\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_orig.suptitle(f\"{subpop_name} - {cohort_name}\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_original.savefig(fig_orig)\n",
    "        plt.close(fig_orig)\n",
    "    \n",
    "    # Plot aggregated cluster arrows in a new PDF\n",
    "    output_file_cluster = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_aggregated_cluster_arrows_equal_cells_using_PCs.pdf\")\n",
    "    color_mapping_file = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/Publication_Material/T_Cell_cluster_colors.csv\"  # Update with actual path\n",
    "    color_mapping_df = pd.read_csv(color_mapping_file)\n",
    "    cluster_colors_map = dict(zip(color_mapping_df['Cluster'], color_mapping_df['Color']))\n",
    "    with PdfPages(output_file_cluster) as pdf_cluster:\n",
    "        num_timepoints = len(cohort_timepoints)\n",
    "        fig_clust, axes_clust = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        if num_timepoints == 1:\n",
    "            axes_clust = [axes_clust]\n",
    "        \n",
    "        for i, source_tp in enumerate(cohort_timepoints):\n",
    "            ax = axes_clust[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            \n",
    "            gray_x = res['sampled_gray']['UMAP_2'].values\n",
    "            gray_y = res['sampled_gray']['UMAP_1'].values\n",
    "            source_x = res['sampled_source']['UMAP_2'].values\n",
    "            source_y = res['sampled_source']['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "            \n",
    "            cluster_arrows = res.get('cluster_arrows', [])\n",
    "            \n",
    "            # Compute source cluster proportions for line widths\n",
    "            source_cells = full_data[source_tp]['source']\n",
    "            source_cluster_counts = source_cells['seurat_clusters'].value_counts()\n",
    "            total_source_cells_current = len(source_cells)\n",
    "            \n",
    "            # Now each entry in cluster_arrows is (clust, cx, cy, cdx, cdy)\n",
    "            for (clust, cx, cy, cdx, cdy) in cluster_arrows:\n",
    "                # Get the color for this cluster, default to black if not found\n",
    "                arrow_color = cluster_colors_map.get(clust, 'black')\n",
    "                proportion = source_cluster_counts.get(clust, 0) / total_source_cells_current\n",
    "                line_width = 1.0 + 8.0 * proportion  # adjust scaling as needed\n",
    "\n",
    "                # Draw boundary arrow\n",
    "                boundary_color = 'black'\n",
    "                ax.arrow(cx, cy, cdx, cdy,\n",
    "                         color=boundary_color,\n",
    "                         alpha=1,\n",
    "                         head_width=0.2 + 0.5 * proportion,\n",
    "                         head_length=0.1 + 0.1 * proportion,\n",
    "                         length_includes_head=True,\n",
    "                         linewidth=line_width + 2)  # Thicker for boundary\n",
    "\n",
    "                # Draw main arrow on top\n",
    "                ax.arrow(cx, cy, cdx, cdy,\n",
    "                         color=arrow_color, alpha=1,\n",
    "                         head_width=0.2+0.5*proportion, head_length=0.1+0.1*proportion,   # bigger arrowhead\n",
    "                         length_includes_head=True, linewidth=line_width)\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_clust.suptitle(f\"{subpop_name} - {cohort_name} (Cluster-Level Arrows)\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_cluster.savefig(fig_clust)\n",
    "        plt.close(fig_clust)\n",
    "\n",
    "    # Plot single aggregated arrow in a new PDF\n",
    "    output_file_single_agg = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_aggregated_single_arrow_equal_cells_using_PCs.pdf\")\n",
    "    with PdfPages(output_file_single_agg) as pdf_agg:\n",
    "        num_timepoints = len(cohort_timepoints)\n",
    "        fig_agg, axes_agg = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        if num_timepoints == 1:\n",
    "            axes_agg = [axes_agg]\n",
    "\n",
    "        for i, source_tp in enumerate(cohort_timepoints):\n",
    "            ax = axes_agg[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "\n",
    "            gray_x = res['sampled_gray']['UMAP_2'].values\n",
    "            gray_y = res['sampled_gray']['UMAP_1'].values\n",
    "            source_x = res['sampled_source']['UMAP_2'].values\n",
    "            source_y = res['sampled_source']['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "\n",
    "            aggregated_arrow = res.get('aggregated_arrow', None)\n",
    "            if aggregated_arrow is not None:\n",
    "                global_cx, global_cy, total_dx, total_dy = aggregated_arrow\n",
    "                # Just draw one large arrow\n",
    "                # Use a fixed linewidth and arrowhead for clarity\n",
    "                ax.arrow(global_cx, global_cy, total_dx, total_dy,\n",
    "                         color='black', alpha=0.9,\n",
    "                         head_width=0.3, head_length=0.3,\n",
    "                         length_includes_head=True, linewidth=2.0)\n",
    "\n",
    "            ax.set_title(f\"Timepoint: {source_tp} (Single Aggregated Arrow)\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        fig_agg.suptitle(f\"{subpop_name} - {cohort_name} (Single Aggregated Arrow)\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_agg.savefig(fig_agg)\n",
    "        plt.close(fig_agg)\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution Loop\n",
    "# ---------------------------\n",
    "\n",
    "for subpop_name in subpopulations:\n",
    "    for cohort_name in cohorts:\n",
    "        print(f\"Processing {subpop_name} - {cohort_name}\")\n",
    "        input_subfolder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "        if not os.path.exists(input_subfolder):\n",
    "            print(f\"Input subfolder does not exist: {input_subfolder}. Skipping.\")\n",
    "            continue\n",
    "        optimal_transport_visualization(subpop_name, cohort_name)\n",
    "\n",
    "print(\"Visualization generation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cc2dd2-1469-4b71-877c-21371c2209a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7529d0bd-89e2-4546-923a-bdb271e0106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increasing the arrow lengths to actual displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06ba9b5f-2b07-4c99-8656-3152301cb507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Activated_CD4 - control\n",
      "Processing Activated_CD4 - short_term\n",
      "Processing Activated_CD4 - long_term\n",
      "Processing Effector_CD8 - control\n",
      "Processing Effector_CD8 - short_term\n",
      "Processing Effector_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Effector_Memory_CD8 - control\n",
      "Processing Effector_Memory_CD8 - short_term\n",
      "Processing Effector_Memory_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Exhausted_T - control\n",
      "Processing Exhausted_T - short_term\n",
      "Processing Exhausted_T - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Gamma_Delta_T - control\n",
      "Processing Gamma_Delta_T - short_term\n",
      "Processing Gamma_Delta_T - long_term\n",
      "Processing Active_CD4 - control\n",
      "Processing Active_CD4 - short_term\n",
      "Processing Active_CD4 - long_term\n",
      "Processing Naive_CD4 - control\n",
      "Processing Naive_CD4 - short_term\n",
      "Processing Naive_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD4 - control\n",
      "Processing Memory_CD4 - short_term\n",
      "Processing Memory_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD8 - control\n",
      "Processing Memory_CD8 - short_term\n",
      "Processing Memory_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Anergic_CD8 - control\n",
      "Processing Anergic_CD8 - short_term\n",
      "Processing Anergic_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Naive_CD8 - control\n",
      "Processing Naive_CD8 - short_term\n",
      "Processing Naive_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Hyperactivated_CD8 - control\n",
      "Missing files for Pre. Skipping this timepoint.\n",
      "Missing files for C1. Skipping this timepoint.\n",
      "Missing files for C2. Skipping this timepoint.\n",
      "No valid timepoints with data.\n",
      "Processing Hyperactivated_CD8 - short_term\n",
      "Missing files for Pre. Skipping this timepoint.\n",
      "Missing files for C1. Skipping this timepoint.\n",
      "Missing files for C2. Skipping this timepoint.\n",
      "Missing files for C4. Skipping this timepoint.\n",
      "No valid timepoints with data.\n",
      "Processing Hyperactivated_CD8 - long_term\n",
      "Missing files for Pre. Skipping this timepoint.\n",
      "Missing files for C1. Skipping this timepoint.\n",
      "Missing files for C2. Skipping this timepoint.\n",
      "Missing files for C4. Skipping this timepoint.\n",
      "Missing files for C6. Skipping this timepoint.\n",
      "Missing files for C9. Skipping this timepoint.\n",
      "Missing files for C18. Skipping this timepoint.\n",
      "Missing files for C36. Skipping this timepoint.\n",
      "No valid timepoints with data.\n",
      "Processing Proliferating_Effector - control\n",
      "Processing Proliferating_Effector - short_term\n",
      "Processing Proliferating_Effector - long_term\n",
      "Processing CD8 - control\n",
      "Processing CD8 - short_term\n",
      "Processing CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization generation completed.\n"
     ]
    }
   ],
   "source": [
    "# remove previous, if this works\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ot\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration Parameters\n",
    "# ---------------------------\n",
    "\n",
    "# Base directory where the CSV files are stored\n",
    "base_input_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/new/temp\"  # Replace with your actual path\n",
    "\n",
    "# Base directory to save the plots\n",
    "base_output_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/new/temp\"  # Replace with your desired output path\n",
    "\n",
    "# Subpopulations (should match the names used in R)\n",
    "subpopulations = [\n",
    "    \"Activated_CD4\",\n",
    "    \"Effector_CD8\",\n",
    "    \"Effector_Memory_CD8\",\n",
    "    \"Exhausted_T\",\n",
    "    \"Gamma_Delta_T\",\n",
    "    \"Active_CD4\",\n",
    "    \"Naive_CD4\",\n",
    "    \"Memory_CD4\",\n",
    "    \"Memory_CD8\",\n",
    "    \"Anergic_CD8\",\n",
    "    \"Naive_CD8\",\n",
    "    \"Hyperactivated_CD8\",\n",
    "    \"Proliferating_Effector\",\n",
    "    \"CD8\"\n",
    "]\n",
    "\n",
    "# Cohorts\n",
    "cohorts = [\"control\", \"short_term\", \"long_term\"]\n",
    "\n",
    "# Ordered timepoints\n",
    "all_timepoints = [\"Pre\", \"C1\", \"C2\", \"C4\", \"C6\", \"C9\", \"C18\", \"C36\"]\n",
    "\n",
    "# Color mapping for cohorts\n",
    "cohort_colors = {\n",
    "    'control': 'yellow',\n",
    "    'short_term': 'blue',\n",
    "    'long_term': 'red'\n",
    "}\n",
    "\n",
    "# Desired minimum and maximum arrow lengths for visualization\n",
    "# min_arrow_length = 0.3  # Adjust as needed\n",
    "# max_arrow_length = 2    # Adjust as needed\n",
    "\n",
    "# Number of cells to sample for single-cell plotting (to reduce overcrowding)\n",
    "max_plot_cells = None  # will determine at runtime\n",
    "\n",
    "def optimal_transport_visualization(subpop_name, cohort_name):\n",
    "    input_folder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "    output_folder = os.path.join(base_output_dir, subpop_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    timepoint_folders = sorted([f for f in os.listdir(input_folder) if f.startswith(\"Timepoint_\")])\n",
    "    available_timepoints = [tp.split(\"_\")[1] for tp in timepoint_folders]\n",
    "    cohort_timepoints = [tp for tp in all_timepoints if tp in available_timepoints]\n",
    "    \n",
    "    if not cohort_timepoints:\n",
    "        print(f\"No timepoints available for {subpop_name} in {cohort_name} cohort.\")\n",
    "        return\n",
    "\n",
    "    # We'll store coordinates and also find global axis limits\n",
    "    all_x_coords = []\n",
    "    all_y_coords = []\n",
    "    \n",
    "    cell_counts_source = []\n",
    "    cell_counts_gray = []\n",
    "    full_data = {}  # store full data for each timepoint\n",
    "    \n",
    "    for tp in cohort_timepoints:\n",
    "        source_folder = os.path.join(input_folder, f\"Timepoint_{tp}\")\n",
    "        source_cells_file = os.path.join(source_folder, 'source_cells_new.csv')\n",
    "        gray_cells_file = os.path.join(source_folder, 'gray_cells_new.csv')\n",
    "        \n",
    "        if not (os.path.exists(source_cells_file) and os.path.exists(gray_cells_file)):\n",
    "            print(f\"Missing files for {tp}. Skipping this timepoint.\")\n",
    "            continue\n",
    "        \n",
    "        source_cells = pd.read_csv(source_cells_file)\n",
    "        gray_cells = pd.read_csv(gray_cells_file)\n",
    "        \n",
    "        if source_cells.empty:\n",
    "            print(f\"No source cells for {tp}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Store full data\n",
    "        full_data[tp] = {\n",
    "            'source': source_cells,\n",
    "            'gray': gray_cells\n",
    "        }\n",
    "        \n",
    "        # For axis limits\n",
    "        all_x_coords.extend(gray_cells['UMAP_2'])\n",
    "        all_x_coords.extend(source_cells['UMAP_2'])\n",
    "        all_y_coords.extend(gray_cells['UMAP_1'])\n",
    "        all_y_coords.extend(source_cells['UMAP_1'])\n",
    "        \n",
    "        cell_counts_source.append(len(source_cells))\n",
    "        cell_counts_gray.append(len(gray_cells))\n",
    "    \n",
    "    if not full_data:\n",
    "        print(\"No valid timepoints with data.\")\n",
    "        return\n",
    "    \n",
    "    x_min, x_max = min(all_x_coords), max(all_x_coords)\n",
    "    y_min, y_max = min(all_y_coords), max(all_y_coords)\n",
    "    \n",
    "    # Determine number of cells to plot for single-cell arrows (downsample for plotting only)\n",
    "    min_source_cells = min(cell_counts_source) if cell_counts_source else 0\n",
    "    min_gray_cells = min(cell_counts_gray) if cell_counts_gray else 0\n",
    "    if min_source_cells == 0 or min_gray_cells == 0:\n",
    "        print(\"Insufficient cells. Skipping visualization.\")\n",
    "        return\n",
    "    \n",
    "    timepoint_results = {}\n",
    "    \n",
    "    # Compute OT from each timepoint to the next (except the last one)\n",
    "    for i, source_tp in enumerate(cohort_timepoints):\n",
    "        source_data = full_data[source_tp]\n",
    "        source_cells = source_data['source']\n",
    "        \n",
    "        # Downsample for plotting single-cell arrows only\n",
    "        sampled_source_cells = source_cells.sample(n=min_source_cells, random_state=42)\n",
    "        sampled_gray_cells = full_data[source_tp]['gray'].sample(n=min_gray_cells, random_state=42)\n",
    "        \n",
    "        timepoint_results[source_tp] = {\n",
    "            'sampled_source': sampled_source_cells,\n",
    "            'sampled_gray': sampled_gray_cells\n",
    "        }\n",
    "        \n",
    "        if i < len(cohort_timepoints) - 1:\n",
    "            target_tp = cohort_timepoints[i+1]\n",
    "            target_data = full_data[target_tp]\n",
    "            target_cells = target_data['source']\n",
    "            \n",
    "            if target_cells.empty:\n",
    "                continue\n",
    "            \n",
    "            pc_cols = [c for c in source_cells.columns if c.startswith('PC_')]\n",
    "            full_source_coords_pc = source_cells[pc_cols].values\n",
    "            full_target_coords_pc = target_cells[pc_cols].values\n",
    "            \n",
    "            # Also store UMAP coordinates\n",
    "            full_source_coords_umap = source_cells[['UMAP_2', 'UMAP_1']].values\n",
    "            full_target_coords_umap = target_cells[['UMAP_2', 'UMAP_1']].values\n",
    "            \n",
    "            # Uniform distributions\n",
    "            a = np.ones((full_source_coords_pc.shape[0],)) / full_source_coords_pc.shape[0]\n",
    "            b = np.ones((full_target_coords_pc.shape[0],)) / full_target_coords_pc.shape[0]\n",
    "            \n",
    "            # Compute cost matrix\n",
    "            cost_matrix = ot.dist(full_source_coords_pc, full_target_coords_pc, metric='euclidean')\n",
    "            \n",
    "            try:\n",
    "                transport_plan = ot.emd(a, b, cost_matrix, numItermax=100000)\n",
    "            except Exception as e:\n",
    "                print(f\"OT computation failed for {source_tp} to {target_tp}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            full_target_indices = np.argmax(transport_plan, axis=1)\n",
    "            displacement_vectors_full = full_target_coords_umap[full_target_indices] - full_source_coords_umap\n",
    "            \n",
    "            # Compute norms\n",
    "            norms_full = np.linalg.norm(displacement_vectors_full, axis=1)\n",
    "            arrow_lengths_full = norms_full\n",
    "            # min_norm_full = np.min(norms_full)\n",
    "            # max_norm_full = np.max(norms_full)\n",
    "            \n",
    "            # if max_norm_full - min_norm_full > 0:\n",
    "            #     arrow_lengths_full = ((norms_full - min_norm_full) / (max_norm_full - min_norm_full)) * (max_arrow_length - min_arrow_length) + min_arrow_length\n",
    "            # else:\n",
    "            #     arrow_lengths_full = np.full_like(norms_full, min_arrow_length)\n",
    "            \n",
    "            # For single-cell plotting\n",
    "            sampled_source_indices = source_cells.index.get_indexer_for(sampled_source_cells.index)\n",
    "            sampled_displacements = displacement_vectors_full[sampled_source_indices, :]\n",
    "            sampled_norms = norms_full[sampled_source_indices]\n",
    "            arrow_lengths_sampled = sampled_norms\n",
    "            \n",
    "            # min_norm_sampled = np.min(sampled_norms)\n",
    "            # max_norm_sampled = np.max(sampled_norms)\n",
    "            # if max_norm_sampled - min_norm_sampled > 0:\n",
    "            #     arrow_lengths_sampled = ((sampled_norms - min_norm_sampled) / (max_norm_sampled - min_norm_sampled)) * (max_arrow_length - min_arrow_length) + min_arrow_length\n",
    "            # else:\n",
    "            #     arrow_lengths_sampled = np.full_like(sampled_norms, min_arrow_length)\n",
    "            \n",
    "            timepoint_results[source_tp]['single_cell_displacements'] = sampled_displacements\n",
    "            timepoint_results[source_tp]['single_cell_arrow_lengths'] = arrow_lengths_sampled\n",
    "            \n",
    "            clusters_of_interest = {1, 2, 3, 8, 10, 12, 14}\n",
    "\n",
    "            # Compute cluster-level arrows using full data\n",
    "            if 'seurat_clusters' in source_cells.columns:\n",
    "                source_cells['seurat_clusters'] = source_cells['seurat_clusters'].replace({16: 14, 17: 14})\n",
    "                source_clusters_full = source_cells['seurat_clusters'].values\n",
    "                df_cluster_full = pd.DataFrame({\n",
    "                    'cluster': source_clusters_full,\n",
    "                    'sx': full_source_coords_umap[:,0],\n",
    "                    'sy': full_source_coords_umap[:,1],\n",
    "                    'dx': displacement_vectors_full[:,0],\n",
    "                    'dy': displacement_vectors_full[:,1],\n",
    "                    'norm': norms_full\n",
    "                })\n",
    "                \n",
    "                gray_cells = full_data[source_tp]['gray']\n",
    "                all_cells_combined = pd.concat([source_cells, gray_cells])\n",
    "                all_cells_combined['seurat_clusters'] = all_cells_combined['seurat_clusters'].replace({16:14,17:14})\n",
    "                \n",
    "                all_clusters = all_cells_combined['seurat_clusters'].values\n",
    "                all_coords_umap = all_cells_combined[['UMAP_2', 'UMAP_1']].values\n",
    "                \n",
    "                df_all = pd.DataFrame({\n",
    "                    'cluster': all_clusters,\n",
    "                    'sx': all_coords_umap[:,0],\n",
    "                    'sy': all_coords_umap[:,1]\n",
    "                })\n",
    "                \n",
    "                # Compute centroids from all cells (source + gray)\n",
    "                all_group = df_all.groupby('cluster')\n",
    "                centroids = all_group[['sx','sy']].median()\n",
    "                \n",
    "                # Mean displacement from source cells only\n",
    "                group = df_cluster_full.groupby('cluster')\n",
    "                mean_disp = group[['dx','dy']].mean()\n",
    "            \n",
    "                cluster_norms = np.sqrt(mean_disp['dx']**2 + mean_disp['dy']**2)\n",
    "                cluster_arrow_lengths = cluster_norms\n",
    "                # cn_min = cluster_norms.min()\n",
    "                # cn_max = cluster_norms.max()\n",
    "                \n",
    "                # if cn_max - cn_min > 0:\n",
    "                #     cluster_arrow_lengths = ((cluster_norms - cn_min) / (cn_max - cn_min)) * (max_arrow_length - min_arrow_length) + min_arrow_length\n",
    "                # else:\n",
    "                #     cluster_arrow_lengths = pd.Series(np.full_like(cluster_norms, min_arrow_length), index=cluster_norms.index)\n",
    "                \n",
    "                cluster_arrows = []\n",
    "                common_clusters = centroids.index.intersection(mean_disp.index)\n",
    "                for clust in common_clusters:\n",
    "                    if clust in clusters_of_interest:\n",
    "                        cx, cy = centroids.loc[clust, ['sx','sy']]\n",
    "                        cdx, cdy = mean_disp.loc[clust, ['dx','dy']]\n",
    "                        cnorm = np.sqrt(cdx**2 + cdy**2)\n",
    "                        if cnorm > 0:\n",
    "                            cdx = cdx / cnorm\n",
    "                            cdy = cdy / cnorm\n",
    "                        else:\n",
    "                            cdx, cdy = 0, 0\n",
    "                        \n",
    "                        try:\n",
    "                            length = cluster_arrow_lengths.loc[clust]\n",
    "                        except AttributeError:\n",
    "                            if isinstance(cluster_arrow_lengths, np.ndarray):\n",
    "                                cluster_arrow_lengths = pd.Series(cluster_arrow_lengths, index=cluster_norms.index)\n",
    "                            try:\n",
    "                                length = cluster_arrow_lengths.loc[clust]\n",
    "                            except Exception as e:\n",
    "                                print(f\"Failed to access cluster_arrow_lengths for cluster {clust}: {e}\")\n",
    "                                length = min_arrow_length\n",
    "            \n",
    "                        cdx *= length\n",
    "                        cdy *= length\n",
    "                        # Store cluster ID alongside arrow coordinates for line width calculation later\n",
    "                        cluster_arrows.append((clust, cx, cy, cdx, cdy))\n",
    "                \n",
    "                timepoint_results[source_tp]['cluster_arrows'] = cluster_arrows\n",
    "\n",
    "                if len(cluster_arrows) > 0:\n",
    "                    # Instead of global centroid of all cells, use the median of source cells only:\n",
    "                    source_median_x = source_cells['UMAP_2'].median()\n",
    "                    source_median_y = source_cells['UMAP_1'].median()\n",
    "\n",
    "                    # Sum displacement vectors of all cluster arrows\n",
    "                    total_dx = sum([arrow[3] for arrow in cluster_arrows])\n",
    "                    total_dy = sum([arrow[4] for arrow in cluster_arrows])\n",
    "\n",
    "                    # Store the aggregated arrow\n",
    "                    timepoint_results[source_tp]['aggregated_arrow'] = (source_median_x, source_median_y, total_dx, total_dy)\n",
    "                else:\n",
    "                    # If no cluster arrows, no aggregated arrow\n",
    "                    timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "            else:\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "                print(f\"No 'seurat_clusters' column in {source_tp} source cells. Cannot compute cluster arrows.\")\n",
    "\n",
    "            \n",
    "        else:\n",
    "            # Last timepoint has no next timepoint\n",
    "            timepoint_results[source_tp]['single_cell_displacements'] = np.array([])\n",
    "            timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "            timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "            timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "            if 'seurat_clusters' in source_cells.columns:\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "            else:\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "    \n",
    "    # Plot single-cell arrows PDF\n",
    "    output_file_original = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_differential_arrow_lengths_equal_cells_using_PCs.pdf\")\n",
    "    with PdfPages(output_file_original) as pdf_original:\n",
    "        num_timepoints = len(cohort_timepoints)\n",
    "        fig_orig, axes_orig = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        if num_timepoints == 1:\n",
    "            axes_orig = [axes_orig]\n",
    "        \n",
    "        for i, source_tp in enumerate(cohort_timepoints):\n",
    "            ax = axes_orig[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            sampled_source_cells = res['sampled_source']\n",
    "            sampled_gray_cells = res['sampled_gray']\n",
    "            \n",
    "            gray_x = sampled_gray_cells['UMAP_2'].values\n",
    "            gray_y = sampled_gray_cells['UMAP_1'].values\n",
    "            source_x = sampled_source_cells['UMAP_2'].values\n",
    "            source_y = sampled_source_cells['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "            \n",
    "            single_cell_displacements = res['single_cell_displacements']\n",
    "            single_cell_arrow_lengths = res['single_cell_arrow_lengths']\n",
    "            \n",
    "            if len(single_cell_displacements) > 0:\n",
    "                source_coords_sampled = sampled_source_cells[['UMAP_2','UMAP_1']].values\n",
    "                sampled_norms = np.linalg.norm(single_cell_displacements, axis=1)\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    unit_vectors = single_cell_displacements / sampled_norms[:, np.newaxis]\n",
    "                    unit_vectors[~np.isfinite(unit_vectors)] = 0\n",
    "                scaled_vectors = unit_vectors * single_cell_arrow_lengths[:, np.newaxis]\n",
    "                \n",
    "                for j in range(len(source_coords_sampled)):\n",
    "                    sx, sy = source_coords_sampled[j]\n",
    "                    dx, dy = scaled_vectors[j]\n",
    "                    ax.arrow(sx, sy, dx, dy,\n",
    "                             color='black', alpha=0.7, head_width=0.05, head_length=0.05,\n",
    "                             length_includes_head=True, linewidth=0.5)\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_orig.suptitle(f\"{subpop_name} - {cohort_name}\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_original.savefig(fig_orig)\n",
    "        plt.close(fig_orig)\n",
    "    \n",
    "    # Plot aggregated cluster arrows in a new PDF\n",
    "    output_file_cluster = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_aggregated_cluster_arrows_equal_cells_using_PCs.pdf\")\n",
    "    color_mapping_file = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/Publication_Material/T_Cell_cluster_colors.csv\"  # Update with actual path\n",
    "    color_mapping_df = pd.read_csv(color_mapping_file)\n",
    "    cluster_colors_map = dict(zip(color_mapping_df['Cluster'], color_mapping_df['Color']))\n",
    "    with PdfPages(output_file_cluster) as pdf_cluster:\n",
    "        num_timepoints = len(cohort_timepoints)\n",
    "        fig_clust, axes_clust = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        if num_timepoints == 1:\n",
    "            axes_clust = [axes_clust]\n",
    "        \n",
    "        for i, source_tp in enumerate(cohort_timepoints):\n",
    "            ax = axes_clust[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            \n",
    "            gray_x = res['sampled_gray']['UMAP_2'].values\n",
    "            gray_y = res['sampled_gray']['UMAP_1'].values\n",
    "            source_x = res['sampled_source']['UMAP_2'].values\n",
    "            source_y = res['sampled_source']['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "            \n",
    "            cluster_arrows = res.get('cluster_arrows', [])\n",
    "            \n",
    "            # Compute source cluster proportions for line widths\n",
    "            source_cells = full_data[source_tp]['source']\n",
    "            source_cluster_counts = source_cells['seurat_clusters'].value_counts()\n",
    "            total_source_cells_current = len(source_cells)\n",
    "            \n",
    "            # Now each entry in cluster_arrows is (clust, cx, cy, cdx, cdy)\n",
    "            for (clust, cx, cy, cdx, cdy) in cluster_arrows:\n",
    "                # Get the color for this cluster, default to black if not found\n",
    "                arrow_color = cluster_colors_map.get(clust, 'black')\n",
    "                proportion = source_cluster_counts.get(clust, 0) / total_source_cells_current\n",
    "                line_width = 1.0 + 8.0 * proportion  # adjust scaling as needed\n",
    "\n",
    "                # Draw boundary arrow\n",
    "                boundary_color = 'black'\n",
    "                ax.arrow(cx, cy, cdx, cdy,\n",
    "                         color=boundary_color,\n",
    "                         alpha=1,\n",
    "                         head_width=0.2 + 0.5 * proportion,\n",
    "                         head_length=0.1 + 0.1 * proportion,\n",
    "                         length_includes_head=True,\n",
    "                         linewidth=line_width + 2)  # Thicker for boundary\n",
    "\n",
    "                # Draw main arrow on top\n",
    "                ax.arrow(cx, cy, cdx, cdy,\n",
    "                         color=arrow_color, alpha=1,\n",
    "                         head_width=0.2+0.5*proportion, head_length=0.1+0.1*proportion,   # bigger arrowhead\n",
    "                         length_includes_head=True, linewidth=line_width)\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_clust.suptitle(f\"{subpop_name} - {cohort_name} (Cluster-Level Arrows)\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_cluster.savefig(fig_clust)\n",
    "        plt.close(fig_clust)\n",
    "\n",
    "    # Plot single aggregated arrow in a new PDF\n",
    "    output_file_single_agg = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_aggregated_single_arrow_equal_cells_using_PCs.pdf\")\n",
    "    with PdfPages(output_file_single_agg) as pdf_agg:\n",
    "        num_timepoints = len(cohort_timepoints)\n",
    "        fig_agg, axes_agg = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        if num_timepoints == 1:\n",
    "            axes_agg = [axes_agg]\n",
    "\n",
    "        for i, source_tp in enumerate(cohort_timepoints):\n",
    "            ax = axes_agg[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "\n",
    "            gray_x = res['sampled_gray']['UMAP_2'].values\n",
    "            gray_y = res['sampled_gray']['UMAP_1'].values\n",
    "            source_x = res['sampled_source']['UMAP_2'].values\n",
    "            source_y = res['sampled_source']['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "\n",
    "            aggregated_arrow = res.get('aggregated_arrow', None)\n",
    "            if aggregated_arrow is not None:\n",
    "                global_cx, global_cy, total_dx, total_dy = aggregated_arrow\n",
    "                # Just draw one large arrow\n",
    "                # Use a fixed linewidth and arrowhead for clarity\n",
    "                ax.arrow(global_cx, global_cy, total_dx, total_dy,\n",
    "                         color='black', alpha=0.9,\n",
    "                         head_width=0.3, head_length=0.3,\n",
    "                         length_includes_head=True, linewidth=2.0)\n",
    "\n",
    "            ax.set_title(f\"Timepoint: {source_tp} (Single Aggregated Arrow)\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        fig_agg.suptitle(f\"{subpop_name} - {cohort_name} (Single Aggregated Arrow)\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_agg.savefig(fig_agg)\n",
    "        plt.close(fig_agg)\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution Loop\n",
    "# ---------------------------\n",
    "\n",
    "for subpop_name in subpopulations:\n",
    "    for cohort_name in cohorts:\n",
    "        print(f\"Processing {subpop_name} - {cohort_name}\")\n",
    "        input_subfolder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "        if not os.path.exists(input_subfolder):\n",
    "            print(f\"Input subfolder does not exist: {input_subfolder}. Skipping.\")\n",
    "            continue\n",
    "        optimal_transport_visualization(subpop_name, cohort_name)\n",
    "\n",
    "print(\"Visualization generation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "986c8b4a-9403-488c-9849-5d33c594ed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# including new visualization (target distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bda6076c-5278-47d5-80b6-af6628287bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Activated_CD4 - control\n",
      "Processing Activated_CD4 - short_term\n",
      "Processing Activated_CD4 - long_term\n",
      "Processing Effector_CD8 - control\n",
      "Processing Effector_CD8 - short_term\n",
      "Processing Effector_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_Precursor_Effector_CD8 - control\n",
      "Processing Memory_Precursor_Effector_CD8 - short_term\n",
      "Processing Memory_Precursor_Effector_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Exhausted_T - control\n",
      "Processing Exhausted_T - short_term\n",
      "Processing Exhausted_T - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Gamma_Delta_T - control\n",
      "Processing Gamma_Delta_T - short_term\n",
      "Processing Gamma_Delta_T - long_term\n",
      "Processing Active_CD4 - control\n",
      "Processing Active_CD4 - short_term\n",
      "Processing Active_CD4 - long_term\n",
      "Processing Naive_CD4 - control\n",
      "Processing Naive_CD4 - short_term\n",
      "Processing Naive_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD4 - control\n",
      "Processing Memory_CD4 - short_term\n",
      "Processing Memory_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Central_Memory_CD8 - control\n",
      "Processing Central_Memory_CD8 - short_term\n",
      "Processing Central_Memory_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Stem_Like_CD8 - control\n",
      "Processing Stem_Like_CD8 - short_term\n",
      "Processing Stem_Like_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Effector_Memory_CD8 - control\n",
      "Processing Effector_Memory_CD8 - short_term\n",
      "Processing Effector_Memory_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Proliferating_Effector - control\n",
      "Processing Proliferating_Effector - short_term\n",
      "Processing Proliferating_Effector - long_term\n",
      "Processing CD8 - control\n",
      "Processing CD8 - short_term\n",
      "Processing CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization generation completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ot\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration Parameters\n",
    "# ---------------------------\n",
    "\n",
    "base_input_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/final_nomenclature\"\n",
    "base_output_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/final_nomenclature\"\n",
    "\n",
    "subpopulations = [\n",
    "    \"Activated_CD4\",\n",
    "    \"Effector_CD8\",\n",
    "    \"Memory_Precursor_Effector_CD8\",\n",
    "    \"Exhausted_T\",\n",
    "    \"Gamma_Delta_T\",\n",
    "    \"Active_CD4\",\n",
    "    \"Naive_CD4\",\n",
    "    \"Memory_CD4\",\n",
    "    \"Central_Memory_CD8\",\n",
    "    \"Stem_Like_CD8\",\n",
    "    \"Effector_Memory_CD8\",\n",
    "    \"Proliferating_Effector\",\n",
    "    \"CD8\"\n",
    "]\n",
    "\n",
    "cohorts = [\"control\", \"short_term\", \"long_term\"]\n",
    "\n",
    "all_timepoints = [\"Pre\", \"C1\", \"C2\", \"C4\", \"C6\", \"C9\", \"C18\", \"C36\"]\n",
    "\n",
    "cohort_colors = {\n",
    "    'control': 'yellow',\n",
    "    'short_term': 'blue',\n",
    "    'long_term': 'red'\n",
    "}\n",
    "\n",
    "clusters_of_interest = [1, 2, 3, 8, 10, 12, 14]\n",
    "\n",
    "color_mapping_file = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/Publication_Material/T_Cell_cluster_colors.csv\"\n",
    "color_mapping_df = pd.read_csv(color_mapping_file)\n",
    "cluster_colors_map = dict(zip(color_mapping_df['Cluster'], color_mapping_df['Color']))\n",
    "cluster_celltype_map = dict(zip(color_mapping_df['Cluster'], color_mapping_df['Celltype']))\n",
    "\n",
    "def optimal_transport_visualization(subpop_name, cohort_name):\n",
    "    input_folder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "    output_folder = os.path.join(base_output_dir, subpop_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return None\n",
    "    \n",
    "    timepoint_folders = sorted([f for f in os.listdir(input_folder) if f.startswith(\"Timepoint_\")])\n",
    "    available_timepoints = [tp.split(\"_\")[1] for tp in timepoint_folders]\n",
    "    cohort_timepoints = [tp for tp in all_timepoints if tp in available_timepoints]\n",
    "    \n",
    "    if not cohort_timepoints:\n",
    "        print(f\"No timepoints available for {subpop_name} in {cohort_name} cohort.\")\n",
    "        return None\n",
    "\n",
    "    all_x_coords = []\n",
    "    all_y_coords = []\n",
    "    \n",
    "    cell_counts_source = []\n",
    "    cell_counts_gray = []\n",
    "    full_data = {}\n",
    "    \n",
    "    for tp in cohort_timepoints:\n",
    "        source_folder = os.path.join(input_folder, f\"Timepoint_{tp}\")\n",
    "        source_cells_file = os.path.join(source_folder, 'source_cells_new.csv')\n",
    "        gray_cells_file = os.path.join(source_folder, 'gray_cells_new.csv')\n",
    "        \n",
    "        if not (os.path.exists(source_cells_file) and os.path.exists(gray_cells_file)):\n",
    "            print(f\"Missing files for {tp}. Skipping this timepoint.\")\n",
    "            continue\n",
    "        \n",
    "        source_cells = pd.read_csv(source_cells_file)\n",
    "        gray_cells = pd.read_csv(gray_cells_file)\n",
    "        \n",
    "        if source_cells.empty:\n",
    "            print(f\"No source cells for {tp}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        full_data[tp] = {\n",
    "            'source': source_cells,\n",
    "            'gray': gray_cells\n",
    "        }\n",
    "        \n",
    "        all_x_coords.extend(gray_cells['UMAP_2'])\n",
    "        all_x_coords.extend(source_cells['UMAP_2'])\n",
    "        all_y_coords.extend(gray_cells['UMAP_1'])\n",
    "        all_y_coords.extend(source_cells['UMAP_1'])\n",
    "        \n",
    "        cell_counts_source.append(len(source_cells))\n",
    "        cell_counts_gray.append(len(gray_cells))\n",
    "    \n",
    "    if not full_data:\n",
    "        print(\"No valid timepoints with data.\")\n",
    "        return None\n",
    "    \n",
    "    x_min, x_max = min(all_x_coords), max(all_x_coords)\n",
    "    y_min, y_max = min(all_y_coords), max(all_y_coords)\n",
    "    \n",
    "    min_source_cells = min(cell_counts_source) if cell_counts_source else 0\n",
    "    min_gray_cells = min(cell_counts_gray) if cell_counts_gray else 0\n",
    "    if min_source_cells == 0 or min_gray_cells == 0:\n",
    "        print(\"Insufficient cells. Skipping visualization.\")\n",
    "        return None\n",
    "    \n",
    "    timepoint_results = {}\n",
    "    distributions_coi = {tp: {cohort_name: {c: {} for c in clusters_of_interest}} for tp in cohort_timepoints}\n",
    "    \n",
    "    for i, source_tp in enumerate(cohort_timepoints):\n",
    "        source_data = full_data[source_tp]\n",
    "        source_cells = source_data['source']\n",
    "        \n",
    "        sampled_source_cells = source_cells.sample(n=min_source_cells, random_state=42)\n",
    "        sampled_gray_cells = full_data[source_tp]['gray'].sample(n=min_gray_cells, random_state=42)\n",
    "        \n",
    "        timepoint_results[source_tp] = {\n",
    "            'sampled_source': sampled_source_cells,\n",
    "            'sampled_gray': sampled_gray_cells\n",
    "        }\n",
    "        \n",
    "        if i < len(cohort_timepoints) - 1:\n",
    "            target_tp = cohort_timepoints[i+1]\n",
    "            target_data = full_data[target_tp]\n",
    "            target_cells = target_data['source']\n",
    "            \n",
    "            if target_cells.empty:\n",
    "                timepoint_results[source_tp]['single_cell_displacements'] = np.array([])\n",
    "                timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "            else:\n",
    "                pc_cols = [c for c in source_cells.columns if c.startswith('PC_')]\n",
    "                full_source_coords_pc = source_cells[pc_cols].values\n",
    "                full_target_coords_pc = target_cells[pc_cols].values\n",
    "                \n",
    "                full_source_coords_umap = source_cells[['UMAP_2', 'UMAP_1']].values\n",
    "                full_target_coords_umap = target_cells[['UMAP_2', 'UMAP_1']].values\n",
    "                \n",
    "                a = np.ones((full_source_coords_pc.shape[0],)) / full_source_coords_pc.shape[0]\n",
    "                b = np.ones((full_target_coords_pc.shape[0],)) / full_target_coords_pc.shape[0]\n",
    "                \n",
    "                cost_matrix = ot.dist(full_source_coords_pc, full_target_coords_pc, metric='euclidean')\n",
    "                \n",
    "                try:\n",
    "                    transport_plan = ot.emd(a, b, cost_matrix, numItermax=100000)\n",
    "                except Exception as e:\n",
    "                    print(f\"OT computation failed for {source_tp} to {target_tp}: {e}\")\n",
    "                    timepoint_results[source_tp]['single_cell_displacements'] = np.array([])\n",
    "                    timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "                    timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                    timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "                    # no distributions_coi update in case of failure\n",
    "                    continue\n",
    "                \n",
    "                full_target_indices = np.argmax(transport_plan, axis=1)\n",
    "                displacement_vectors_full = full_target_coords_umap[full_target_indices] - full_source_coords_umap\n",
    "                \n",
    "                norms_full = np.linalg.norm(displacement_vectors_full, axis=1)\n",
    "                arrow_lengths_full = norms_full\n",
    "                \n",
    "                sampled_source_indices = source_cells.index.get_indexer_for(sampled_source_cells.index)\n",
    "                sampled_displacements = displacement_vectors_full[sampled_source_indices, :]\n",
    "                sampled_norms = norms_full[sampled_source_indices]\n",
    "                \n",
    "                arrow_lengths_sampled = sampled_norms\n",
    "                \n",
    "                timepoint_results[source_tp]['single_cell_displacements'] = sampled_displacements\n",
    "                timepoint_results[source_tp]['single_cell_arrow_lengths'] = arrow_lengths_sampled\n",
    "                \n",
    "                if 'seurat_clusters' in source_cells.columns and 'seurat_clusters' in target_cells.columns:\n",
    "                    source_cells['seurat_clusters'] = source_cells['seurat_clusters'].replace({16:14,17:14})\n",
    "                    target_cells['seurat_clusters'] = target_cells['seurat_clusters'].replace({16:14,17:14})\n",
    "                    source_cells['seurat_clusters'] = source_cells['seurat_clusters'].replace({9:6,18:6})\n",
    "                    target_cells['seurat_clusters'] = target_cells['seurat_clusters'].replace({9:6,18:6})\n",
    "                    \n",
    "                    # Cluster-level arrows\n",
    "                    source_clusters_full = source_cells['seurat_clusters'].values\n",
    "                    df_cluster_full = pd.DataFrame({\n",
    "                        'cluster': source_clusters_full,\n",
    "                        'sx': full_source_coords_umap[:,0],\n",
    "                        'sy': full_source_coords_umap[:,1],\n",
    "                        'dx': displacement_vectors_full[:,0],\n",
    "                        'dy': displacement_vectors_full[:,1],\n",
    "                        'norm': norms_full\n",
    "                    })\n",
    "                    \n",
    "                    gray_cells = full_data[source_tp]['gray']\n",
    "                    all_cells_combined = pd.concat([source_cells, gray_cells])\n",
    "                    all_cells_combined['seurat_clusters'] = all_cells_combined['seurat_clusters'].replace({16:14,17:14})\n",
    "                    all_cells_combined['seurat_clusters'] = all_cells_combined['seurat_clusters'].replace({9:6,18:6})\n",
    "                    \n",
    "                    all_clusters = all_cells_combined['seurat_clusters'].values\n",
    "                    all_coords_umap = all_cells_combined[['UMAP_2', 'UMAP_1']].values\n",
    "                    \n",
    "                    df_all = pd.DataFrame({\n",
    "                        'cluster': all_clusters,\n",
    "                        'sx': all_coords_umap[:,0],\n",
    "                        'sy': all_coords_umap[:,1]\n",
    "                    })\n",
    "                    \n",
    "                    all_group = df_all.groupby('cluster')\n",
    "                    centroids = all_group[['sx','sy']].median()\n",
    "                    \n",
    "                    group = df_cluster_full.groupby('cluster')\n",
    "                    mean_disp = group[['dx','dy']].mean()\n",
    "                \n",
    "                    cluster_norms = np.sqrt(mean_disp['dx']**2 + mean_disp['dy']**2)\n",
    "                    cluster_arrow_lengths = cluster_norms\n",
    "                    \n",
    "                    cluster_arrows = []\n",
    "                    common_clusters = centroids.index.intersection(mean_disp.index)\n",
    "                    for clust in common_clusters:\n",
    "                        if clust in clusters_of_interest:\n",
    "                            cx, cy = centroids.loc[clust, ['sx','sy']]\n",
    "                            cdx, cdy = mean_disp.loc[clust, ['dx','dy']]\n",
    "                            cnorm = np.sqrt(cdx**2 + cdy**2)\n",
    "                            if cnorm > 0:\n",
    "                                cdx /= cnorm\n",
    "                                cdy /= cnorm\n",
    "                            \n",
    "                            length = cluster_arrow_lengths.loc[clust]\n",
    "                            cdx *= length\n",
    "                            cdy *= length\n",
    "                            cluster_arrows.append((clust, cx, cy, cdx, cdy))\n",
    "                    \n",
    "                    timepoint_results[source_tp]['cluster_arrows'] = cluster_arrows\n",
    "    \n",
    "                    if len(cluster_arrows) > 0:\n",
    "                        source_median_x = source_cells['UMAP_2'].median()\n",
    "                        source_median_y = source_cells['UMAP_1'].median()\n",
    "                        total_dx = sum([arrow[3] for arrow in cluster_arrows])\n",
    "                        total_dy = sum([arrow[4] for arrow in cluster_arrows])\n",
    "                        timepoint_results[source_tp]['aggregated_arrow'] = (source_median_x, source_median_y, total_dx, total_dy)\n",
    "                    else:\n",
    "                        timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "    \n",
    "                    # Compute distributions for each cluster_of_interest\n",
    "                    for coi in clusters_of_interest:\n",
    "                        coi_mask = (source_cells['seurat_clusters'] == coi)\n",
    "                        if np.any(coi_mask):\n",
    "                            selected_target_indices = full_target_indices[coi_mask]\n",
    "                            selected_target_clusters = target_cells['seurat_clusters'].iloc[selected_target_indices].values\n",
    "                            unique_tclusters, counts = np.unique(selected_target_clusters, return_counts=True)\n",
    "                            total_count = counts.sum()\n",
    "                            if total_count > 0:\n",
    "                                fraction_dict = {int(tc): (ct / total_count) for tc, ct in zip(unique_tclusters, counts)}\n",
    "                            else:\n",
    "                                fraction_dict = {}\n",
    "                            distributions_coi[source_tp][cohort_name][coi] = fraction_dict\n",
    "                        else:\n",
    "                            distributions_coi[source_tp][cohort_name][coi] = {}\n",
    "                else:\n",
    "                    timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                    timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "                    for coi in clusters_of_interest:\n",
    "                        distributions_coi[source_tp][cohort_name][coi] = {}\n",
    "        else:\n",
    "            # Last timepoint has no next timepoint\n",
    "            timepoint_results[source_tp]['single_cell_displacements'] = np.array([])\n",
    "            timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "            timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "            timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "            for coi in clusters_of_interest:\n",
    "                distributions_coi[source_tp][cohort_name][coi] = {}\n",
    "\n",
    "    # ---------------------------\n",
    "    # Generate Original PDFs\n",
    "    # ---------------------------\n",
    "    # Single-cell arrows PDF\n",
    "    output_file_original = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_differential_arrow_lengths_equal_cells_using_PCs.pdf\")\n",
    "    with PdfPages(output_file_original) as pdf_original:\n",
    "        plot_tps = list(timepoint_results.keys())\n",
    "        num_timepoints = len(plot_tps)\n",
    "        fig_orig, axes_orig = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        if num_timepoints == 1:\n",
    "            axes_orig = [axes_orig]\n",
    "        \n",
    "        for i, source_tp in enumerate(plot_tps):\n",
    "            ax = axes_orig[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            sampled_source_cells = res['sampled_source']\n",
    "            sampled_gray_cells = res['sampled_gray']\n",
    "            \n",
    "            gray_x = sampled_gray_cells['UMAP_2'].values\n",
    "            gray_y = sampled_gray_cells['UMAP_1'].values\n",
    "            source_x = sampled_source_cells['UMAP_2'].values\n",
    "            source_y = sampled_source_cells['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "            \n",
    "            single_cell_displacements = res['single_cell_displacements']\n",
    "            single_cell_arrow_lengths = res['single_cell_arrow_lengths']\n",
    "            \n",
    "            if len(single_cell_displacements) > 0:\n",
    "                source_coords_sampled = sampled_source_cells[['UMAP_2','UMAP_1']].values\n",
    "                sampled_norms = np.linalg.norm(single_cell_displacements, axis=1)\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    unit_vectors = single_cell_displacements / sampled_norms[:, np.newaxis]\n",
    "                    unit_vectors[~np.isfinite(unit_vectors)] = 0\n",
    "                scaled_vectors = unit_vectors * single_cell_arrow_lengths[:, np.newaxis]\n",
    "                \n",
    "                for j in range(len(source_coords_sampled)):\n",
    "                    sx, sy = source_coords_sampled[j]\n",
    "                    dx, dy = scaled_vectors[j]\n",
    "                    ax.arrow(sx, sy, dx, dy,\n",
    "                             color='black', alpha=0.7, head_width=0.05, head_length=0.05,\n",
    "                             length_includes_head=True, linewidth=0.5)\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_orig.suptitle(f\"{subpop_name} - {cohort_name}\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_original.savefig(fig_orig)\n",
    "        plt.close(fig_orig)\n",
    "    \n",
    "    # Cluster-level arrows PDF\n",
    "    output_file_cluster = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_aggregated_cluster_arrows_equal_cells_using_PCs.pdf\")\n",
    "    with PdfPages(output_file_cluster) as pdf_cluster:\n",
    "        plot_tps = list(timepoint_results.keys())\n",
    "        num_timepoints = len(plot_tps)\n",
    "        fig_clust, axes_clust = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        if num_timepoints == 1:\n",
    "            axes_clust = [axes_clust]\n",
    "        \n",
    "        for i, source_tp in enumerate(plot_tps):\n",
    "            ax = axes_clust[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            \n",
    "            gray_x = res['sampled_gray']['UMAP_2'].values\n",
    "            gray_y = res['sampled_gray']['UMAP_1'].values\n",
    "            source_x = res['sampled_source']['UMAP_2'].values\n",
    "            source_y = res['sampled_source']['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "            \n",
    "            cluster_arrows = res.get('cluster_arrows', [])\n",
    "            \n",
    "            source_cells = full_data[source_tp]['source']\n",
    "            if 'seurat_clusters' in source_cells.columns:\n",
    "                source_cluster_counts = source_cells['seurat_clusters'].value_counts()\n",
    "                total_source_cells_current = len(source_cells)\n",
    "            else:\n",
    "                source_cluster_counts = pd.Series()\n",
    "                total_source_cells_current = 1\n",
    "            \n",
    "            for (clust, cx, cy, cdx, cdy) in cluster_arrows:\n",
    "                arrow_color = cluster_colors_map.get(clust, 'black')\n",
    "                proportion = source_cluster_counts.get(clust, 0) / total_source_cells_current\n",
    "                line_width = 1.0 + 8.0 * proportion\n",
    "\n",
    "                # Draw boundary arrow\n",
    "                ax.arrow(cx, cy, cdx, cdy,\n",
    "                         color='black',\n",
    "                         alpha=1,\n",
    "                         head_width=0.2 + 0.5 * proportion,\n",
    "                         head_length=0.1 + 0.1 * proportion,\n",
    "                         length_includes_head=True,\n",
    "                         linewidth=line_width + 2)\n",
    "                \n",
    "                # Draw main arrow on top\n",
    "                ax.arrow(cx, cy, cdx, cdy,\n",
    "                         color=arrow_color, alpha=1,\n",
    "                         head_width=0.2+0.5*proportion, head_length=0.1+0.1*proportion,\n",
    "                         length_includes_head=True, linewidth=line_width)\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_clust.suptitle(f\"{subpop_name} - {cohort_name} (Cluster-Level Arrows)\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_cluster.savefig(fig_clust)\n",
    "        plt.close(fig_clust)\n",
    "\n",
    "    # Single aggregated arrow PDF\n",
    "    output_file_single_agg = os.path.join(output_folder, f\"{subpop_name}_{cohort_name}_movement_plots_aggregated_single_arrow_equal_cells_using_PCs.pdf\")\n",
    "    with PdfPages(output_file_single_agg) as pdf_agg:\n",
    "        plot_tps = list(timepoint_results.keys())\n",
    "        num_timepoints = len(plot_tps)\n",
    "        fig_agg, axes_agg = plt.subplots(1, num_timepoints, figsize=(4 * num_timepoints, 4), sharex=True, sharey=True)\n",
    "        if num_timepoints == 1:\n",
    "            axes_agg = [axes_agg]\n",
    "\n",
    "        for i, source_tp in enumerate(plot_tps):\n",
    "            ax = axes_agg[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "\n",
    "            gray_x = res['sampled_gray']['UMAP_2'].values\n",
    "            gray_y = res['sampled_gray']['UMAP_1'].values\n",
    "            source_x = res['sampled_source']['UMAP_2'].values\n",
    "            source_y = res['sampled_source']['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "\n",
    "            aggregated_arrow = res.get('aggregated_arrow', None)\n",
    "            if aggregated_arrow is not None:\n",
    "                global_cx, global_cy, total_dx, total_dy = aggregated_arrow\n",
    "                ax.arrow(global_cx, global_cy, total_dx, total_dy,\n",
    "                         color='black', alpha=0.9,\n",
    "                         head_width=0.3, head_length=0.3,\n",
    "                         length_includes_head=True, linewidth=2.0)\n",
    "\n",
    "            ax.set_title(f\"Timepoint: {source_tp} (Single Aggregated Arrow)\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        fig_agg.suptitle(f\"{subpop_name} - {cohort_name} (Single Aggregated Arrow)\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_agg.savefig(fig_agg)\n",
    "        plt.close(fig_agg)\n",
    "    \n",
    "    # Return the distributions for stacked bar chart plotting done outside\n",
    "    return distributions_coi\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution Loop\n",
    "# ---------------------------\n",
    "\n",
    "for subpop_name in subpopulations:\n",
    "    distributions_coi_all = {tp: {c: {coi: {} for coi in clusters_of_interest} for c in cohorts} for tp in all_timepoints}\n",
    "    \n",
    "    for cohort_name in cohorts:\n",
    "        print(f\"Processing {subpop_name} - {cohort_name}\")\n",
    "        input_subfolder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "        if not os.path.exists(input_subfolder):\n",
    "            print(f\"Input subfolder does not exist: {input_subfolder}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        dist_coi = optimal_transport_visualization(subpop_name, cohort_name)\n",
    "        if dist_coi is not None:\n",
    "            for tp in dist_coi:\n",
    "                for c in dist_coi[tp]:\n",
    "                    for coi in dist_coi[tp][c]:\n",
    "                        distributions_coi_all[tp][c][coi] = dist_coi[tp][c][coi]\n",
    "\n",
    "    # Determine all target clusters that appear\n",
    "    all_target_clusters = set()\n",
    "    for tp in distributions_coi_all:\n",
    "        for c in cohorts:\n",
    "            for coi in clusters_of_interest:\n",
    "                all_target_clusters.update(distributions_coi_all[tp][c][coi].keys())\n",
    "    all_target_clusters = sorted(all_target_clusters)\n",
    "\n",
    "    def get_celltype_name(clust_id):\n",
    "        return cluster_celltype_map.get(clust_id, f\"Cluster {clust_id}\")\n",
    "\n",
    "    def get_cluster_color(clust_id):\n",
    "        return cluster_colors_map.get(clust_id, 'gray')\n",
    "\n",
    "    # Generate the stacked bar chart PDF\n",
    "    output_file_stacked = os.path.join(base_output_dir, f\"{subpop_name}_target_cluster_distribution_coi.pdf\")\n",
    "    with PdfPages(output_file_stacked) as pdf_dist:\n",
    "        fig, axes = plt.subplots(len(clusters_of_interest), len(all_timepoints), \n",
    "                                 figsize=(4*len(all_timepoints), 3*len(clusters_of_interest)), \n",
    "                                 sharex=False, sharey=True)\n",
    "        \n",
    "        if len(clusters_of_interest) == 1 and len(all_timepoints) == 1:\n",
    "            axes = np.array([[axes]])\n",
    "        elif len(clusters_of_interest) == 1:\n",
    "            axes = axes[np.newaxis, :]\n",
    "        elif len(all_timepoints) == 1:\n",
    "            axes = axes[:, np.newaxis]\n",
    "\n",
    "        # Plot bars\n",
    "        for row_i, coi in enumerate(clusters_of_interest):\n",
    "            for col_i, tp in enumerate(all_timepoints):\n",
    "                ax = axes[row_i, col_i]\n",
    "                bar_positions = np.arange(len(cohorts))\n",
    "                cohort_distributions = [distributions_coi_all[tp][c][coi] for c in cohorts]\n",
    "                bottoms = np.zeros(len(cohorts))\n",
    "\n",
    "                stack_data = []\n",
    "                for tc in all_target_clusters:\n",
    "                    heights = [d.get(tc,0) for d in cohort_distributions]\n",
    "                    stack_data.append((tc, heights))\n",
    "        \n",
    "                # Sort by sum of fractions\n",
    "                stack_data.sort(key=lambda x: sum(x[1]), reverse=True)\n",
    "                \n",
    "                for (tc, h) in stack_data:\n",
    "                    color = get_cluster_color(tc)\n",
    "                    ax.bar(bar_positions, h, bottom=bottoms, color=color, edgecolor='black')\n",
    "                    # Add labels\n",
    "                    for idx, val in enumerate(h):\n",
    "                        if val > 0.05:\n",
    "                            mid_y = bottoms[idx] + val/2\n",
    "                            ax.text(bar_positions[idx], mid_y, get_celltype_name(tc),\n",
    "                                    ha='center', va='center', fontsize=6, color='white')\n",
    "                    bottoms += h\n",
    "\n",
    "                if row_i == 0:\n",
    "                    ax.set_title(f\"{tp}\", fontsize=10)\n",
    "\n",
    "                ax.set_xticks(bar_positions)\n",
    "                ax.set_xticklabels(cohorts, rotation=45, ha='right', fontsize=8)\n",
    "                ax.set_ylim(0,1)\n",
    "\n",
    "        fig.suptitle(f\"Distribution of Target Clusters per COI - {subpop_name}\", fontsize=16)\n",
    "\n",
    "        # Apply tight layout first\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Add space for legend and row labels\n",
    "        fig.subplots_adjust(left=0.15, top=0.88)  \n",
    "        \n",
    "        # Label rows with celltype of the COI\n",
    "        n_rows = len(clusters_of_interest)\n",
    "        for row_i, coi in enumerate(clusters_of_interest):\n",
    "            y_pos = 0.88 - (row_i + 0.5)*(0.88-0.1)/n_rows\n",
    "            fig.text(0.05, y_pos, get_celltype_name(coi), va='center', ha='right', fontsize=10, color='black')\n",
    "\n",
    "        # Create legend patches with celltype names for target clusters\n",
    "        legend_patches = []\n",
    "        for tc in all_target_clusters:\n",
    "            legend_patches.append(plt.Rectangle((0,0),1,1,\n",
    "                                                facecolor=get_cluster_color(tc),\n",
    "                                                edgecolor='black',\n",
    "                                                label=get_celltype_name(tc)))\n",
    "        \n",
    "        # Place legend at top center\n",
    "        fig.legend(handles=legend_patches, loc='upper center', bbox_to_anchor=(0.5, 0.98), \n",
    "                   ncol=len(all_target_clusters), fontsize=8, title=\"Target Celltypes\")\n",
    "\n",
    "        pdf_dist.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "print(\"Visualization generation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50a5c63b-a8ce-43e5-aa8a-05ce806100bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# December 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3946f0bc-5ff3-4c0e-bf40-2333bb014a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Activated_CD4 - control\n",
      "Processing Activated_CD4 - short_term\n",
      "Processing Activated_CD4 - long_term\n",
      "Processing Effector_CD8 - control\n",
      "Processing Effector_CD8 - short_term\n",
      "Processing Effector_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_Precursor_Effector_CD8 - control\n",
      "Processing Memory_Precursor_Effector_CD8 - short_term\n",
      "Processing Memory_Precursor_Effector_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Exhausted_T - control\n",
      "Processing Exhausted_T - short_term\n",
      "Processing Exhausted_T - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Gamma_Delta_T - control\n",
      "Processing Gamma_Delta_T - short_term\n",
      "Processing Gamma_Delta_T - long_term\n",
      "Processing Active_CD4 - control\n",
      "Processing Active_CD4 - short_term\n",
      "Processing Active_CD4 - long_term\n",
      "Processing Naive_CD4 - control\n",
      "Processing Naive_CD4 - short_term\n",
      "Processing Naive_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Memory_CD4 - control\n",
      "Processing Memory_CD4 - short_term\n",
      "Processing Memory_CD4 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Central_Memory_CD8 - control\n",
      "Processing Central_Memory_CD8 - short_term\n",
      "Processing Central_Memory_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Stem_Like_CD8 - control\n",
      "Processing Stem_Like_CD8 - short_term\n",
      "Processing Stem_Like_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Effector_Memory_CD8 - control\n",
      "Processing Effector_Memory_CD8 - short_term\n",
      "Processing Effector_Memory_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Proliferating_Effector - control\n",
      "Processing Proliferating_Effector - short_term\n",
      "Processing Proliferating_Effector - long_term\n",
      "Processing CD8 - control\n",
      "Processing CD8 - short_term\n",
      "Processing CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization generation completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ot\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration Parameters\n",
    "# ---------------------------\n",
    "\n",
    "base_input_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/final_nomenclature\"\n",
    "base_output_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/final_nomenclature\"\n",
    "\n",
    "subpopulations = [\n",
    "    \"Activated_CD4\",\n",
    "    \"Effector_CD8\",\n",
    "    \"Memory_Precursor_Effector_CD8\",\n",
    "    \"Exhausted_T\",\n",
    "    \"Gamma_Delta_T\",\n",
    "    \"Active_CD4\",\n",
    "    \"Naive_CD4\",\n",
    "    \"Memory_CD4\",\n",
    "    \"Central_Memory_CD8\",\n",
    "    \"Stem_Like_CD8\",\n",
    "    \"Effector_Memory_CD8\",\n",
    "    \"Proliferating_Effector\",\n",
    "    \"CD8\"\n",
    "]\n",
    "\n",
    "cohorts = [\"control\", \"short_term\", \"long_term\"]\n",
    "\n",
    "all_timepoints = [\"Pre\", \"C1\", \"C2\", \"C4\", \"C6\", \"C9\", \"C18\", \"C36\"]\n",
    "\n",
    "cohort_colors = {\n",
    "    'control': 'yellow',\n",
    "    'short_term': 'blue',\n",
    "    'long_term': 'red'\n",
    "}\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Clusters we are interested in as the \"source\" (COI)\n",
    "# --------------------------------------------------\n",
    "clusters_of_interest = [1, 2, 3, 8, 10, 12, 14]\n",
    "\n",
    "# Provide the path to your CSV that maps each cluster to a color and celltype\n",
    "color_mapping_file = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/Publication_Material/T_Cell_cluster_colors.csv\"\n",
    "color_mapping_df = pd.read_csv(color_mapping_file)\n",
    "cluster_colors_map = dict(zip(color_mapping_df['Cluster'], color_mapping_df['Color']))\n",
    "cluster_celltype_map = dict(zip(color_mapping_df['Cluster'], color_mapping_df['Celltype']))\n",
    "\n",
    "def optimal_transport_visualization(subpop_name, cohort_name):\n",
    "    \"\"\"\n",
    "    For a single subpopulation and cohort, run the full OT pipeline:\n",
    "    1) Load data at each timepoint.\n",
    "    2) Perform OT to map from source to next timepoint.\n",
    "    3) Collect single-cell and cluster-level arrows, plus aggregated arrows.\n",
    "    4) Generate PDFs of the movement plots (single-cell, cluster-level, aggregated).\n",
    "    5) Return distributions_coi for stacked-bar plotting.\n",
    "    \"\"\"\n",
    "    input_folder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "    output_folder = os.path.join(base_output_dir, subpop_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return None\n",
    "    \n",
    "    timepoint_folders = sorted([f for f in os.listdir(input_folder) if f.startswith(\"Timepoint_\")])\n",
    "    available_timepoints = [tp.split(\"_\")[1] for tp in timepoint_folders]\n",
    "    cohort_timepoints = [tp for tp in all_timepoints if tp in available_timepoints]\n",
    "    \n",
    "    if not cohort_timepoints:\n",
    "        print(f\"No timepoints available for {subpop_name} in {cohort_name} cohort.\")\n",
    "        return None\n",
    "\n",
    "    all_x_coords = []\n",
    "    all_y_coords = []\n",
    "    \n",
    "    cell_counts_source = []\n",
    "    cell_counts_gray = []\n",
    "    full_data = {}\n",
    "    \n",
    "    # Gather data across all valid timepoints\n",
    "    for tp in cohort_timepoints:\n",
    "        source_folder = os.path.join(input_folder, f\"Timepoint_{tp}\")\n",
    "        source_cells_file = os.path.join(source_folder, 'source_cells_new.csv')\n",
    "        gray_cells_file = os.path.join(source_folder, 'gray_cells_new.csv')\n",
    "        \n",
    "        if not (os.path.exists(source_cells_file) and os.path.exists(gray_cells_file)):\n",
    "            print(f\"Missing files for {tp}. Skipping this timepoint.\")\n",
    "            continue\n",
    "        \n",
    "        source_cells = pd.read_csv(source_cells_file)\n",
    "        gray_cells = pd.read_csv(gray_cells_file)\n",
    "        \n",
    "        if source_cells.empty:\n",
    "            print(f\"No source cells for {tp}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        full_data[tp] = {\n",
    "            'source': source_cells,\n",
    "            'gray': gray_cells\n",
    "        }\n",
    "        \n",
    "        all_x_coords.extend(gray_cells['UMAP_2'])\n",
    "        all_x_coords.extend(source_cells['UMAP_2'])\n",
    "        all_y_coords.extend(gray_cells['UMAP_1'])\n",
    "        all_y_coords.extend(source_cells['UMAP_1'])\n",
    "        \n",
    "        cell_counts_source.append(len(source_cells))\n",
    "        cell_counts_gray.append(len(gray_cells))\n",
    "    \n",
    "    if not full_data:\n",
    "        print(\"No valid timepoints with data.\")\n",
    "        return None\n",
    "    \n",
    "    x_min, x_max = min(all_x_coords), max(all_x_coords)\n",
    "    y_min, y_max = min(all_y_coords), max(all_y_coords)\n",
    "    \n",
    "    min_source_cells = min(cell_counts_source) if cell_counts_source else 0\n",
    "    min_gray_cells = min(cell_counts_gray) if cell_counts_gray else 0\n",
    "    if min_source_cells == 0 or min_gray_cells == 0:\n",
    "        print(\"Insufficient cells. Skipping visualization.\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare data structures for results\n",
    "    timepoint_results = {}\n",
    "    distributions_coi = {tp: {cohort_name: {c: {} for c in clusters_of_interest}} for tp in cohort_timepoints}\n",
    "    \n",
    "    for i, source_tp in enumerate(cohort_timepoints):\n",
    "        source_data = full_data[source_tp]\n",
    "        source_cells = source_data['source']\n",
    "        \n",
    "        # Downsample so each timepoint has the same # of source & gray cells for plotting\n",
    "        sampled_source_cells = source_cells.sample(n=min_source_cells, random_state=42)\n",
    "        sampled_gray_cells = full_data[source_tp]['gray'].sample(n=min_gray_cells, random_state=42)\n",
    "        \n",
    "        timepoint_results[source_tp] = {\n",
    "            'sampled_source': sampled_source_cells,\n",
    "            'sampled_gray': sampled_gray_cells\n",
    "        }\n",
    "        \n",
    "        # If not the last timepoint, compute OT from source_tp -> target_tp\n",
    "        if i < len(cohort_timepoints) - 1:\n",
    "            target_tp = cohort_timepoints[i+1]\n",
    "            target_data = full_data[target_tp]\n",
    "            target_cells = target_data['source']\n",
    "            \n",
    "            if target_cells.empty:\n",
    "                # If there's no data in the next timepoint, skip\n",
    "                timepoint_results[source_tp]['single_cell_displacements'] = np.array([])\n",
    "                timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "            else:\n",
    "                # Identify columns for PCs\n",
    "                pc_cols = [c for c in source_cells.columns if c.startswith('PC_')]\n",
    "                full_source_coords_pc = source_cells[pc_cols].values\n",
    "                full_target_coords_pc = target_cells[pc_cols].values\n",
    "                \n",
    "                # UMAP coords\n",
    "                full_source_coords_umap = source_cells[['UMAP_2', 'UMAP_1']].values\n",
    "                full_target_coords_umap = target_cells[['UMAP_2', 'UMAP_1']].values\n",
    "                \n",
    "                # Uniform distribution over source / target\n",
    "                a = np.ones((full_source_coords_pc.shape[0],)) / full_source_coords_pc.shape[0]\n",
    "                b = np.ones((full_target_coords_pc.shape[0],)) / full_target_coords_pc.shape[0]\n",
    "                \n",
    "                cost_matrix = ot.dist(full_source_coords_pc, full_target_coords_pc, metric='euclidean')\n",
    "                \n",
    "                try:\n",
    "                    transport_plan = ot.emd(a, b, cost_matrix, numItermax=100000)\n",
    "                except Exception as e:\n",
    "                    print(f\"OT computation failed for {source_tp} to {target_tp}: {e}\")\n",
    "                    timepoint_results[source_tp]['single_cell_displacements'] = np.array([])\n",
    "                    timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "                    timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                    timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "                    continue\n",
    "                \n",
    "                full_target_indices = np.argmax(transport_plan, axis=1)\n",
    "                displacement_vectors_full = full_target_coords_umap[full_target_indices] - full_source_coords_umap\n",
    "                norms_full = np.linalg.norm(displacement_vectors_full, axis=1)\n",
    "                \n",
    "                # Displacements for the downsampled subset\n",
    "                sampled_source_indices = source_cells.index.get_indexer_for(sampled_source_cells.index)\n",
    "                sampled_displacements = displacement_vectors_full[sampled_source_indices, :]\n",
    "                sampled_norms = norms_full[sampled_source_indices]\n",
    "                \n",
    "                timepoint_results[source_tp]['single_cell_displacements'] = sampled_displacements\n",
    "                timepoint_results[source_tp]['single_cell_arrow_lengths'] = sampled_norms\n",
    "                \n",
    "                if 'seurat_clusters' in source_cells.columns and 'seurat_clusters' in target_cells.columns:\n",
    "                    # Lump some clusters if needed\n",
    "                    source_cells['seurat_clusters'] = source_cells['seurat_clusters'].replace({16:14,17:14})\n",
    "                    target_cells['seurat_clusters'] = target_cells['seurat_clusters'].replace({16:14,17:14})\n",
    "                    source_cells['seurat_clusters'] = source_cells['seurat_clusters'].replace({9:6,18:6})\n",
    "                    target_cells['seurat_clusters'] = target_cells['seurat_clusters'].replace({9:6,18:6})\n",
    "                    \n",
    "                    # For cluster-level arrows, compute average displacement per cluster\n",
    "                    source_clusters_full = source_cells['seurat_clusters'].values\n",
    "                    df_cluster_full = pd.DataFrame({\n",
    "                        'cluster': source_clusters_full,\n",
    "                        'sx': full_source_coords_umap[:,0],\n",
    "                        'sy': full_source_coords_umap[:,1],\n",
    "                        'dx': displacement_vectors_full[:,0],\n",
    "                        'dy': displacement_vectors_full[:,1],\n",
    "                        'norm': norms_full\n",
    "                    })\n",
    "                    \n",
    "                    gray_cells_tp = full_data[source_tp]['gray']\n",
    "                    all_cells_combined = pd.concat([source_cells, gray_cells_tp])\n",
    "                    all_cells_combined['seurat_clusters'] = all_cells_combined['seurat_clusters'].replace({16:14,17:14})\n",
    "                    all_cells_combined['seurat_clusters'] = all_cells_combined['seurat_clusters'].replace({9:6,18:6})\n",
    "                    \n",
    "                    all_clusters = all_cells_combined['seurat_clusters'].values\n",
    "                    all_coords_umap = all_cells_combined[['UMAP_2', 'UMAP_1']].values\n",
    "                    \n",
    "                    df_all = pd.DataFrame({\n",
    "                        'cluster': all_clusters,\n",
    "                        'sx': all_coords_umap[:,0],\n",
    "                        'sy': all_coords_umap[:,1]\n",
    "                    })\n",
    "                    \n",
    "                    centroids = df_all.groupby('cluster')[['sx','sy']].median()\n",
    "                    mean_disp = df_cluster_full.groupby('cluster')[['dx','dy']].mean()\n",
    "                    \n",
    "                    cluster_norms = np.sqrt(mean_disp['dx']**2 + mean_disp['dy']**2)\n",
    "                    \n",
    "                    # Build arrow info for each cluster of interest\n",
    "                    cluster_arrows = []\n",
    "                    common_clusters = centroids.index.intersection(mean_disp.index)\n",
    "                    for clust in common_clusters:\n",
    "                        if clust in clusters_of_interest:\n",
    "                            cx, cy = centroids.loc[clust, ['sx','sy']]\n",
    "                            cdx, cdy = mean_disp.loc[clust, ['dx','dy']]\n",
    "                            cnorm = np.sqrt(cdx**2 + cdy**2)\n",
    "                            if cnorm > 0:\n",
    "                                cdx /= cnorm\n",
    "                                cdy /= cnorm\n",
    "                            length = cluster_norms.loc[clust]\n",
    "                            cdx *= length\n",
    "                            cdy *= length\n",
    "                            cluster_arrows.append((clust, cx, cy, cdx, cdy))\n",
    "                    \n",
    "                    timepoint_results[source_tp]['cluster_arrows'] = cluster_arrows\n",
    "    \n",
    "                    if len(cluster_arrows) > 0:\n",
    "                        # \"Aggregated\" arrow by summation\n",
    "                        source_median_x = source_cells['UMAP_2'].median()\n",
    "                        source_median_y = source_cells['UMAP_1'].median()\n",
    "                        total_dx = sum([arrow[3] for arrow in cluster_arrows])\n",
    "                        total_dy = sum([arrow[4] for arrow in cluster_arrows])\n",
    "                        timepoint_results[source_tp]['aggregated_arrow'] = (\n",
    "                            source_median_x, \n",
    "                            source_median_y, \n",
    "                            total_dx, \n",
    "                            total_dy\n",
    "                        )\n",
    "                    else:\n",
    "                        timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "    \n",
    "                    # Compute target cluster distributions for each COI (source cluster)\n",
    "                    for coi in clusters_of_interest:\n",
    "                        coi_mask = (source_cells['seurat_clusters'] == coi)\n",
    "                        if np.any(coi_mask):\n",
    "                            # Which target clusters do these COI cells map to?\n",
    "                            selected_target_indices = full_target_indices[coi_mask]\n",
    "                            selected_target_clusters = target_cells['seurat_clusters'].iloc[selected_target_indices].values\n",
    "                            unique_tclusters, counts = np.unique(selected_target_clusters, return_counts=True)\n",
    "                            total_count = counts.sum()\n",
    "                            if total_count > 0:\n",
    "                                fraction_dict = {int(tc): (ct / total_count) for tc, ct in zip(unique_tclusters, counts)}\n",
    "                            else:\n",
    "                                fraction_dict = {}\n",
    "                            distributions_coi[source_tp][cohort_name][coi] = fraction_dict\n",
    "                        else:\n",
    "                            distributions_coi[source_tp][cohort_name][coi] = {}\n",
    "                else:\n",
    "                    # If cluster info is missing\n",
    "                    timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                    timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "                    for coi in clusters_of_interest:\n",
    "                        distributions_coi[source_tp][cohort_name][coi] = {}\n",
    "        else:\n",
    "            # Last timepoint has no \"next\" timepoint\n",
    "            timepoint_results[source_tp]['single_cell_displacements'] = np.array([])\n",
    "            timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "            timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "            timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "            for coi in clusters_of_interest:\n",
    "                distributions_coi[source_tp][cohort_name][coi] = {}\n",
    "\n",
    "    # ---------------------------\n",
    "    # Generate PDFs of movement\n",
    "    # ---------------------------\n",
    "\n",
    "    # 1) Single-cell arrows\n",
    "    output_file_original = os.path.join(\n",
    "        output_folder, \n",
    "        f\"{subpop_name}_{cohort_name}_movement_plots_differential_arrow_lengths_equal_cells_using_PCs.pdf\"\n",
    "    )\n",
    "    with PdfPages(output_file_original) as pdf_original:\n",
    "        plot_tps = list(timepoint_results.keys())\n",
    "        num_timepoints = len(plot_tps)\n",
    "        fig_orig, axes_orig = plt.subplots(\n",
    "            1, \n",
    "            num_timepoints, \n",
    "            figsize=(4 * num_timepoints, 4), \n",
    "            sharex=True, \n",
    "            sharey=True\n",
    "        )\n",
    "        if num_timepoints == 1:\n",
    "            axes_orig = [axes_orig]\n",
    "        \n",
    "        for i, source_tp in enumerate(plot_tps):\n",
    "            ax = axes_orig[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            sampled_source_cells = res['sampled_source']\n",
    "            sampled_gray_cells = res['sampled_gray']\n",
    "            \n",
    "            gray_x = sampled_gray_cells['UMAP_2'].values\n",
    "            gray_y = sampled_gray_cells['UMAP_1'].values\n",
    "            source_x = sampled_source_cells['UMAP_2'].values\n",
    "            source_y = sampled_source_cells['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "            \n",
    "            single_cell_displacements = res['single_cell_displacements']\n",
    "            single_cell_arrow_lengths = res['single_cell_arrow_lengths']\n",
    "            \n",
    "            if len(single_cell_displacements) > 0:\n",
    "                source_coords_sampled = sampled_source_cells[['UMAP_2','UMAP_1']].values\n",
    "                sampled_norms = np.linalg.norm(single_cell_displacements, axis=1)\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    unit_vectors = single_cell_displacements / sampled_norms[:, np.newaxis]\n",
    "                    unit_vectors[~np.isfinite(unit_vectors)] = 0\n",
    "                scaled_vectors = unit_vectors * single_cell_arrow_lengths[:, np.newaxis]\n",
    "                \n",
    "                for j in range(len(source_coords_sampled)):\n",
    "                    sx, sy = source_coords_sampled[j]\n",
    "                    dx, dy = scaled_vectors[j]\n",
    "                    ax.arrow(\n",
    "                        sx, sy, \n",
    "                        dx, dy,\n",
    "                        color='black', \n",
    "                        alpha=0.7, \n",
    "                        head_width=0.05, \n",
    "                        head_length=0.05,\n",
    "                        length_includes_head=True, \n",
    "                        linewidth=0.5\n",
    "                    )\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_orig.suptitle(f\"{subpop_name} - {cohort_name}\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_original.savefig(fig_orig)\n",
    "        plt.close(fig_orig)\n",
    "    \n",
    "    # 2) Cluster-level arrows\n",
    "    output_file_cluster = os.path.join(\n",
    "        output_folder, \n",
    "        f\"{subpop_name}_{cohort_name}_movement_plots_aggregated_cluster_arrows_equal_cells_using_PCs.pdf\"\n",
    "    )\n",
    "    with PdfPages(output_file_cluster) as pdf_cluster:\n",
    "        plot_tps = list(timepoint_results.keys())\n",
    "        num_timepoints = len(plot_tps)\n",
    "        fig_clust, axes_clust = plt.subplots(\n",
    "            1, \n",
    "            num_timepoints, \n",
    "            figsize=(4 * num_timepoints, 4), \n",
    "            sharex=True, \n",
    "            sharey=True\n",
    "        )\n",
    "        if num_timepoints == 1:\n",
    "            axes_clust = [axes_clust]\n",
    "        \n",
    "        for i, source_tp in enumerate(plot_tps):\n",
    "            ax = axes_clust[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            \n",
    "            gray_x = res['sampled_gray']['UMAP_2'].values\n",
    "            gray_y = res['sampled_gray']['UMAP_1'].values\n",
    "            source_x = res['sampled_source']['UMAP_2'].values\n",
    "            source_y = res['sampled_source']['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "            \n",
    "            cluster_arrows = res.get('cluster_arrows', [])\n",
    "            \n",
    "            source_cells = full_data[source_tp]['source']\n",
    "            if 'seurat_clusters' in source_cells.columns:\n",
    "                source_cluster_counts = source_cells['seurat_clusters'].value_counts()\n",
    "                total_source_cells_current = len(source_cells)\n",
    "            else:\n",
    "                source_cluster_counts = pd.Series()\n",
    "                total_source_cells_current = 1\n",
    "            \n",
    "            for (clust, cx, cy, cdx, cdy) in cluster_arrows:\n",
    "                arrow_color = cluster_colors_map.get(clust, 'black')\n",
    "                proportion = source_cluster_counts.get(clust, 0) / total_source_cells_current\n",
    "                line_width = 1.0 + 8.0 * proportion\n",
    "\n",
    "                # Outline in black for clarity\n",
    "                ax.arrow(\n",
    "                    cx, cy,\n",
    "                    cdx, cdy,\n",
    "                    color='black',\n",
    "                    alpha=1,\n",
    "                    head_width=0.2 + 0.5 * proportion,\n",
    "                    head_length=0.1 + 0.1 * proportion,\n",
    "                    length_includes_head=True,\n",
    "                    linewidth=line_width + 2\n",
    "                )\n",
    "                \n",
    "                # Main arrow in cluster color\n",
    "                ax.arrow(\n",
    "                    cx, cy,\n",
    "                    cdx, cdy,\n",
    "                    color=arrow_color,\n",
    "                    alpha=1,\n",
    "                    head_width=0.2 + 0.5 * proportion,\n",
    "                    head_length=0.1 + 0.1 * proportion,\n",
    "                    length_includes_head=True,\n",
    "                    linewidth=line_width\n",
    "                )\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_clust.suptitle(f\"{subpop_name} - {cohort_name} (Cluster-Level Arrows)\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_cluster.savefig(fig_clust)\n",
    "        plt.close(fig_clust)\n",
    "\n",
    "    # 3) Single aggregated arrow\n",
    "    output_file_single_agg = os.path.join(\n",
    "        output_folder, \n",
    "        f\"{subpop_name}_{cohort_name}_movement_plots_aggregated_single_arrow_equal_cells_using_PCs.pdf\"\n",
    "    )\n",
    "    with PdfPages(output_file_single_agg) as pdf_agg:\n",
    "        plot_tps = list(timepoint_results.keys())\n",
    "        num_timepoints = len(plot_tps)\n",
    "        fig_agg, axes_agg = plt.subplots(\n",
    "            1,\n",
    "            num_timepoints,\n",
    "            figsize=(4 * num_timepoints, 4),\n",
    "            sharex=True,\n",
    "            sharey=True\n",
    "        )\n",
    "        if num_timepoints == 1:\n",
    "            axes_agg = [axes_agg]\n",
    "\n",
    "        for i, source_tp in enumerate(plot_tps):\n",
    "            ax = axes_agg[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "\n",
    "            gray_x = res['sampled_gray']['UMAP_2'].values\n",
    "            gray_y = res['sampled_gray']['UMAP_1'].values\n",
    "            source_x = res['sampled_source']['UMAP_2'].values\n",
    "            source_y = res['sampled_source']['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "\n",
    "            aggregated_arrow = res.get('aggregated_arrow', None)\n",
    "            if aggregated_arrow is not None:\n",
    "                global_cx, global_cy, total_dx, total_dy = aggregated_arrow\n",
    "                ax.arrow(\n",
    "                    global_cx, global_cy,\n",
    "                    total_dx, total_dy,\n",
    "                    color='black', \n",
    "                    alpha=0.9,\n",
    "                    head_width=0.3, \n",
    "                    head_length=0.3,\n",
    "                    length_includes_head=True, \n",
    "                    linewidth=2.0\n",
    "                )\n",
    "\n",
    "            ax.set_title(f\"Timepoint: {source_tp} (Single Aggregated Arrow)\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        fig_agg.suptitle(f\"{subpop_name} - {cohort_name} (Single Aggregated Arrow)\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_agg.savefig(fig_agg)\n",
    "        plt.close(fig_agg)\n",
    "    \n",
    "    # Return the distributions for stacked-bar plotting\n",
    "    return distributions_coi\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution Loop\n",
    "# ---------------------------\n",
    "for subpop_name in subpopulations:\n",
    "    # Prepare a nested dict so we can collect distributions\n",
    "    distributions_coi_all = {\n",
    "        tp: {\n",
    "            c: {coi: {} for coi in clusters_of_interest} \n",
    "            for c in cohorts\n",
    "        }\n",
    "        for tp in all_timepoints\n",
    "    }\n",
    "    \n",
    "    # Run for each cohort\n",
    "    for cohort_name in cohorts:\n",
    "        print(f\"Processing {subpop_name} - {cohort_name}\")\n",
    "        input_subfolder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "        if not os.path.exists(input_subfolder):\n",
    "            print(f\"Input subfolder does not exist: {input_subfolder}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        dist_coi = optimal_transport_visualization(subpop_name, cohort_name)\n",
    "        if dist_coi is not None:\n",
    "            for tp in dist_coi:\n",
    "                for c in dist_coi[tp]:\n",
    "                    for coi in dist_coi[tp][c]:\n",
    "                        distributions_coi_all[tp][c][coi] = dist_coi[tp][c][coi]\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 1) Group non-COI target clusters into \"Others\"\n",
    "    # --------------------------------------------------\n",
    "    for tp in distributions_coi_all:\n",
    "        for c in cohorts:\n",
    "            for coi in clusters_of_interest:\n",
    "                fraction_dict = distributions_coi_all[tp][c][coi]\n",
    "                if fraction_dict:\n",
    "                    new_dict = {}\n",
    "                    others_sum = 0.0\n",
    "                    for tclust, frac in fraction_dict.items():\n",
    "                        # If the target cluster is not in our \"source\" list, \n",
    "                        # put it in the \"Others\" bin\n",
    "                        if tclust not in clusters_of_interest:\n",
    "                            others_sum += frac\n",
    "                        else:\n",
    "                            new_dict[tclust] = frac\n",
    "                    if others_sum > 0:\n",
    "                        new_dict['Others'] = others_sum\n",
    "                    distributions_coi_all[tp][c][coi] = new_dict\n",
    "\n",
    "    # Gather all target clusters (including \"Others\")\n",
    "    all_target_clusters = []\n",
    "    for tp in distributions_coi_all:\n",
    "        for c in cohorts:\n",
    "            for coi in clusters_of_interest:\n",
    "                all_target_clusters.extend(distributions_coi_all[tp][c][coi].keys())\n",
    "    all_target_clusters = set(all_target_clusters)\n",
    "\n",
    "    # Handle \"Others\" separately for sorting\n",
    "    others_present = (\"Others\" in all_target_clusters)\n",
    "    if others_present:\n",
    "        all_target_clusters.remove(\"Others\")\n",
    "\n",
    "    # Now only numeric cluster IDs remain\n",
    "    # Sort numeric clusters in ascending order\n",
    "    all_target_clusters = sorted(all_target_clusters)\n",
    "\n",
    "    # Put \"Others\" at the end if present\n",
    "    if others_present:\n",
    "        all_target_clusters.append(\"Others\")\n",
    "\n",
    "    def get_celltype_name(clust_id):\n",
    "        if clust_id == 'Others':\n",
    "            return \"Others\"\n",
    "        return cluster_celltype_map.get(clust_id, f\"Cluster {clust_id}\")\n",
    "\n",
    "    def get_cluster_color(clust_id):\n",
    "        if clust_id == 'Others':\n",
    "            return 'gray'\n",
    "        return cluster_colors_map.get(clust_id, 'gray')\n",
    "\n",
    "    # ---------------------------\n",
    "    # Generate the stacked bar chart PDF\n",
    "    # ---------------------------\n",
    "    output_file_stacked = os.path.join(base_output_dir, f\"{subpop_name}_target_cluster_distribution_coi_without_stack_labels.pdf\")\n",
    "    with PdfPages(output_file_stacked) as pdf_dist:\n",
    "        fig, axes = plt.subplots(\n",
    "            len(clusters_of_interest), \n",
    "            len(all_timepoints), \n",
    "            figsize=(4 * len(all_timepoints), 3 * len(clusters_of_interest)), \n",
    "            sharex=False, \n",
    "            sharey=True\n",
    "        )\n",
    "        \n",
    "        # Handle shape for single row/col\n",
    "        if len(clusters_of_interest) == 1 and len(all_timepoints) == 1:\n",
    "            axes = np.array([[axes]])\n",
    "        elif len(clusters_of_interest) == 1:\n",
    "            axes = axes[np.newaxis, :]\n",
    "        elif len(all_timepoints) == 1:\n",
    "            axes = axes[:, np.newaxis]\n",
    "\n",
    "        # Prepare legend patches\n",
    "        legend_patches = []\n",
    "        for tc in all_target_clusters:\n",
    "            legend_patches.append(\n",
    "                plt.Rectangle((0,0),1,1,\n",
    "                              facecolor=get_cluster_color(tc),\n",
    "                              edgecolor='black',\n",
    "                              label=get_celltype_name(tc))\n",
    "            )\n",
    "        \n",
    "        # Plot bars\n",
    "        for row_i, coi in enumerate(clusters_of_interest):\n",
    "            for col_i, tp in enumerate(all_timepoints):\n",
    "                ax = axes[row_i, col_i]\n",
    "                bar_positions = np.arange(len(cohorts))\n",
    "                bottoms = np.zeros(len(cohorts))\n",
    "\n",
    "                # Retrieve distribution data for each cohort at (tp, coi)\n",
    "                cohort_distributions = [distributions_coi_all[tp][c][coi] for c in cohorts]\n",
    "\n",
    "                # Build up data for stacked bars: (target_cluster, [fractions for each cohort])\n",
    "                stack_data = []\n",
    "                for tc in all_target_clusters:\n",
    "                    h = [dist.get(tc, 0.0) for dist in cohort_distributions]\n",
    "                    stack_data.append((tc, h))\n",
    "\n",
    "                # Sort so biggest fraction (summed across cohorts) is at the bottom\n",
    "                stack_data.sort(key=lambda x: sum(x[1]), reverse=True)\n",
    "\n",
    "                # Plot\n",
    "                for (tc, heights) in stack_data:\n",
    "                    color = get_cluster_color(tc)\n",
    "                    ax.bar(\n",
    "                        bar_positions, \n",
    "                        heights, \n",
    "                        bottom=bottoms, \n",
    "                        color=color, \n",
    "                        edgecolor='black'\n",
    "                    )\n",
    "                    # Label each segment if fraction is big enough\n",
    "                    # for idx, val in enumerate(heights):\n",
    "                    #     if val > 0.05:\n",
    "                    #         mid_y = bottoms[idx] + val / 2\n",
    "                    #         ax.text(\n",
    "                    #             bar_positions[idx], \n",
    "                    #             mid_y, \n",
    "                    #             get_celltype_name(tc),\n",
    "                    #             ha='center', \n",
    "                    #             va='center',\n",
    "                    #             fontsize=6, \n",
    "                    #             color='white'\n",
    "                    #         )\n",
    "                    bottoms += heights\n",
    "\n",
    "                if row_i == 0:\n",
    "                    ax.set_title(f\"{tp}\", fontsize=10)\n",
    "\n",
    "                ax.set_xticks(bar_positions)\n",
    "                ax.set_xticklabels(cohorts, rotation=45, ha='right', fontsize=8)\n",
    "                ax.set_ylim(0, 1)\n",
    "\n",
    "        fig.suptitle(f\"Distribution of Target Clusters per COI - {subpop_name}\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Adjust to make room for row labels & legend\n",
    "        fig.subplots_adjust(left=0.15, top=0.86, right=0.82)  # extra space on the right for legend\n",
    "\n",
    "        # Label rows with the celltype of the COI\n",
    "        n_rows = len(clusters_of_interest)\n",
    "        for row_i, coi in enumerate(clusters_of_interest):\n",
    "            y_pos = 0.86 - (row_i + 0.5)*(0.86-0.1)/n_rows\n",
    "            fig.text(0.05, y_pos, get_celltype_name(coi), va='center', ha='right', fontsize=10, color='black')\n",
    "\n",
    "        # Create a single-column legend on the right\n",
    "        fig.legend(\n",
    "            handles=legend_patches,\n",
    "            loc='upper left',\n",
    "            bbox_to_anchor=(0.84, 0.95),\n",
    "            ncol=1,\n",
    "            fontsize=8,\n",
    "            title=\"Target Celltypes\"\n",
    "        )\n",
    "\n",
    "        pdf_dist.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "print(\"Visualization generation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a86ef8-6bc9-4032-b081-d3c74cbc33c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal transport only for clusters of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a89f8e9c-e928-4f95-b933-4312a14ad253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Effector_CD8 - control\n",
      "Processing Effector_CD8 - short_term\n",
      "Processing Effector_CD8 - long_term\n",
      "Processing Memory_Precursor_Effector_CD8 - control\n",
      "Processing Memory_Precursor_Effector_CD8 - short_term\n",
      "Processing Memory_Precursor_Effector_CD8 - long_term\n",
      "Processing Exhausted_T - control\n",
      "Processing Exhausted_T - short_term\n",
      "Processing Exhausted_T - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/dtran642_927/SonLe/USC_Source/source/miniconda3/envs/vef_env/lib/python3.9/site-packages/ot/lp/__init__.py:388: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Central_Memory_CD8 - control\n",
      "Processing Central_Memory_CD8 - short_term\n",
      "Processing Central_Memory_CD8 - long_term\n",
      "Processing Stem_Like_CD8 - control\n",
      "Processing Stem_Like_CD8 - short_term\n",
      "Processing Stem_Like_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/dtran642_927/SonLe/USC_Source/source/miniconda3/envs/vef_env/lib/python3.9/site-packages/ot/lp/__init__.py:388: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Effector_Memory_CD8 - control\n",
      "Processing Effector_Memory_CD8 - short_term\n",
      "Processing Effector_Memory_CD8 - long_term\n",
      "Processing Proliferating_Effector - control\n",
      "Processing Proliferating_Effector - short_term\n",
      "Processing Proliferating_Effector - long_term\n",
      "Processing All_CD8 - control\n",
      "Processing All_CD8 - short_term\n",
      "Processing All_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/dtran642_927/SonLe/USC_Source/source/miniconda3/envs/vef_env/lib/python3.9/site-packages/ot/lp/__init__.py:388: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/project/dtran642_927/SonLe/USC_Source/source/miniconda3/envs/vef_env/lib/python3.9/site-packages/ot/lp/__init__.py:388: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization generation completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ot\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration Parameters\n",
    "# ---------------------------\n",
    "\n",
    "base_input_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/final_nomenclature/coi\"\n",
    "base_output_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/final_nomenclature/coi\"\n",
    "\n",
    "subpopulations = [\n",
    "    \"Effector_CD8\",\n",
    "    \"Memory_Precursor_Effector_CD8\",\n",
    "    \"Exhausted_T\",\n",
    "    \"Central_Memory_CD8\",\n",
    "    \"Stem_Like_CD8\",\n",
    "    \"Effector_Memory_CD8\",\n",
    "    \"Proliferating_Effector\",\n",
    "    \"All_CD8\"\n",
    "]\n",
    "\n",
    "cohorts = [\"control\", \"short_term\", \"long_term\"]\n",
    "\n",
    "all_timepoints = [\"Pre\", \"C1\", \"C2\", \"C4\", \"C6\", \"C9\", \"C18\", \"C36\"]\n",
    "\n",
    "cohort_colors = {\n",
    "    'control': 'yellow',\n",
    "    'short_term': 'blue',\n",
    "    'long_term': 'red'\n",
    "}\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Clusters we are interested in as the \"source\" (COI)\n",
    "# --------------------------------------------------\n",
    "clusters_of_interest = [1, 2, 3, 8, 10, 12, 14]\n",
    "\n",
    "# Provide the path to your CSV that maps each cluster to a color and celltype\n",
    "color_mapping_file = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/Publication_Material/T_Cell_cluster_colors.csv\"\n",
    "color_mapping_df = pd.read_csv(color_mapping_file)\n",
    "cluster_colors_map = dict(zip(color_mapping_df['Cluster'], color_mapping_df['Color']))\n",
    "cluster_celltype_map = dict(zip(color_mapping_df['Cluster'], color_mapping_df['Celltype']))\n",
    "\n",
    "def optimal_transport_visualization(subpop_name, cohort_name):\n",
    "    \"\"\"\n",
    "    For a single subpopulation and cohort, run the full OT pipeline:\n",
    "    1) Load data at each timepoint.\n",
    "    2) Perform OT to map from source to next timepoint.\n",
    "    3) Collect single-cell and cluster-level arrows, plus aggregated arrows.\n",
    "    4) Generate PDFs of the movement plots (single-cell, cluster-level, aggregated).\n",
    "    5) Return distributions_coi for stacked-bar plotting.\n",
    "    \"\"\"\n",
    "    input_folder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "    output_folder = os.path.join(base_output_dir, subpop_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return None\n",
    "    \n",
    "    timepoint_folders = sorted([f for f in os.listdir(input_folder) if f.startswith(\"Timepoint_\")])\n",
    "    available_timepoints = [tp.split(\"_\")[1] for tp in timepoint_folders]\n",
    "    cohort_timepoints = [tp for tp in all_timepoints if tp in available_timepoints]\n",
    "    \n",
    "    if not cohort_timepoints:\n",
    "        print(f\"No timepoints available for {subpop_name} in {cohort_name} cohort.\")\n",
    "        return None\n",
    "\n",
    "    all_x_coords = []\n",
    "    all_y_coords = []\n",
    "    \n",
    "    cell_counts_source = []\n",
    "    cell_counts_gray = []\n",
    "    full_data = {}\n",
    "    \n",
    "    # Gather data across all valid timepoints\n",
    "    for tp in cohort_timepoints:\n",
    "        source_folder = os.path.join(input_folder, f\"Timepoint_{tp}\")\n",
    "        source_cells_file = os.path.join(source_folder, 'source_cells_new.csv')\n",
    "        gray_cells_file = os.path.join(source_folder, 'gray_cells_new.csv')\n",
    "        \n",
    "        if not (os.path.exists(source_cells_file) and os.path.exists(gray_cells_file)):\n",
    "            print(f\"Missing files for {tp}. Skipping this timepoint.\")\n",
    "            continue\n",
    "        \n",
    "        source_cells = pd.read_csv(source_cells_file)\n",
    "        gray_cells = pd.read_csv(gray_cells_file)\n",
    "        \n",
    "        if source_cells.empty:\n",
    "            print(f\"No source cells for {tp}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        full_data[tp] = {\n",
    "            'source': source_cells,\n",
    "            'gray': gray_cells\n",
    "        }\n",
    "        \n",
    "        all_x_coords.extend(gray_cells['UMAP_2'])\n",
    "        all_x_coords.extend(source_cells['UMAP_2'])\n",
    "        all_y_coords.extend(gray_cells['UMAP_1'])\n",
    "        all_y_coords.extend(source_cells['UMAP_1'])\n",
    "        \n",
    "        cell_counts_source.append(len(source_cells))\n",
    "        cell_counts_gray.append(len(gray_cells))\n",
    "    \n",
    "    if not full_data:\n",
    "        print(\"No valid timepoints with data.\")\n",
    "        return None\n",
    "    \n",
    "    x_min, x_max = min(all_x_coords), max(all_x_coords)\n",
    "    y_min, y_max = min(all_y_coords), max(all_y_coords)\n",
    "    \n",
    "    min_source_cells = min(cell_counts_source) if cell_counts_source else 0\n",
    "    min_gray_cells = min(cell_counts_gray) if cell_counts_gray else 0\n",
    "    if min_source_cells == 0 or min_gray_cells == 0:\n",
    "        print(\"Insufficient cells. Skipping visualization.\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare data structures for results\n",
    "    timepoint_results = {}\n",
    "    distributions_coi = {tp: {cohort_name: {c: {} for c in clusters_of_interest}} for tp in cohort_timepoints}\n",
    "    \n",
    "    for i, source_tp in enumerate(cohort_timepoints):\n",
    "        source_data = full_data[source_tp]\n",
    "        source_cells = source_data['source']\n",
    "        \n",
    "        # Downsample so each timepoint has the same # of source & gray cells for plotting\n",
    "        sampled_source_cells = source_cells.sample(n=min_source_cells, random_state=42)\n",
    "        sampled_gray_cells = full_data[source_tp]['gray'].sample(n=min_gray_cells, random_state=42)\n",
    "        \n",
    "        timepoint_results[source_tp] = {\n",
    "            'sampled_source': sampled_source_cells,\n",
    "            'sampled_gray': sampled_gray_cells\n",
    "        }\n",
    "        \n",
    "        # If not the last timepoint, compute OT from source_tp -> target_tp\n",
    "        if i < len(cohort_timepoints) - 1:\n",
    "            target_tp = cohort_timepoints[i+1]\n",
    "            target_data = full_data[target_tp]\n",
    "            target_cells = target_data['source']\n",
    "            \n",
    "            if target_cells.empty:\n",
    "                # If there's no data in the next timepoint, skip\n",
    "                timepoint_results[source_tp]['single_cell_displacements'] = np.array([])\n",
    "                timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "            else:\n",
    "                # Identify columns for PCs\n",
    "                pc_cols = [c for c in source_cells.columns if c.startswith('PC_')]\n",
    "                full_source_coords_pc = source_cells[pc_cols].values\n",
    "                full_target_coords_pc = target_cells[pc_cols].values\n",
    "                \n",
    "                # UMAP coords\n",
    "                full_source_coords_umap = source_cells[['UMAP_2', 'UMAP_1']].values\n",
    "                full_target_coords_umap = target_cells[['UMAP_2', 'UMAP_1']].values\n",
    "                \n",
    "                # Uniform distribution over source / target\n",
    "                a = np.ones((full_source_coords_pc.shape[0],)) / full_source_coords_pc.shape[0]\n",
    "                b = np.ones((full_target_coords_pc.shape[0],)) / full_target_coords_pc.shape[0]\n",
    "                \n",
    "                cost_matrix = ot.dist(full_source_coords_pc, full_target_coords_pc, metric='euclidean')\n",
    "                \n",
    "                try:\n",
    "                    transport_plan = ot.emd(a, b, cost_matrix, numItermax=100000)\n",
    "                except Exception as e:\n",
    "                    print(f\"OT computation failed for {source_tp} to {target_tp}: {e}\")\n",
    "                    timepoint_results[source_tp]['single_cell_displacements'] = np.array([])\n",
    "                    timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "                    timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                    timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "                    continue\n",
    "                \n",
    "                full_target_indices = np.argmax(transport_plan, axis=1)\n",
    "                displacement_vectors_full = full_target_coords_umap[full_target_indices] - full_source_coords_umap\n",
    "                norms_full = np.linalg.norm(displacement_vectors_full, axis=1)\n",
    "                \n",
    "                # Displacements for the downsampled subset\n",
    "                sampled_source_indices = source_cells.index.get_indexer_for(sampled_source_cells.index)\n",
    "                sampled_displacements = displacement_vectors_full[sampled_source_indices, :]\n",
    "                sampled_norms = norms_full[sampled_source_indices]\n",
    "                \n",
    "                timepoint_results[source_tp]['single_cell_displacements'] = sampled_displacements\n",
    "                timepoint_results[source_tp]['single_cell_arrow_lengths'] = sampled_norms\n",
    "                \n",
    "                if 'seurat_clusters' in source_cells.columns and 'seurat_clusters' in target_cells.columns:\n",
    "                    # Lump some clusters if needed\n",
    "                    source_cells['seurat_clusters'] = source_cells['seurat_clusters'].replace({16:14,17:14})\n",
    "                    target_cells['seurat_clusters'] = target_cells['seurat_clusters'].replace({16:14,17:14})\n",
    "                    source_cells['seurat_clusters'] = source_cells['seurat_clusters'].replace({9:6,18:6})\n",
    "                    target_cells['seurat_clusters'] = target_cells['seurat_clusters'].replace({9:6,18:6})\n",
    "                    \n",
    "                    # For cluster-level arrows, compute average displacement per cluster\n",
    "                    source_clusters_full = source_cells['seurat_clusters'].values\n",
    "                    df_cluster_full = pd.DataFrame({\n",
    "                        'cluster': source_clusters_full,\n",
    "                        'sx': full_source_coords_umap[:,0],\n",
    "                        'sy': full_source_coords_umap[:,1],\n",
    "                        'dx': displacement_vectors_full[:,0],\n",
    "                        'dy': displacement_vectors_full[:,1],\n",
    "                        'norm': norms_full\n",
    "                    })\n",
    "                    \n",
    "                    gray_cells_tp = full_data[source_tp]['gray']\n",
    "                    all_cells_combined = pd.concat([source_cells, gray_cells_tp])\n",
    "                    all_cells_combined['seurat_clusters'] = all_cells_combined['seurat_clusters'].replace({16:14,17:14})\n",
    "                    all_cells_combined['seurat_clusters'] = all_cells_combined['seurat_clusters'].replace({9:6,18:6})\n",
    "                    \n",
    "                    all_clusters = all_cells_combined['seurat_clusters'].values\n",
    "                    all_coords_umap = all_cells_combined[['UMAP_2', 'UMAP_1']].values\n",
    "                    \n",
    "                    df_all = pd.DataFrame({\n",
    "                        'cluster': all_clusters,\n",
    "                        'sx': all_coords_umap[:,0],\n",
    "                        'sy': all_coords_umap[:,1]\n",
    "                    })\n",
    "                    \n",
    "                    centroids = df_all.groupby('cluster')[['sx','sy']].median()\n",
    "                    mean_disp = df_cluster_full.groupby('cluster')[['dx','dy']].mean()\n",
    "                    \n",
    "                    cluster_norms = np.sqrt(mean_disp['dx']**2 + mean_disp['dy']**2)\n",
    "                    \n",
    "                    # Build arrow info for each cluster of interest\n",
    "                    cluster_arrows = []\n",
    "                    common_clusters = centroids.index.intersection(mean_disp.index)\n",
    "                    for clust in common_clusters:\n",
    "                        if clust in clusters_of_interest:\n",
    "                            cx, cy = centroids.loc[clust, ['sx','sy']]\n",
    "                            cdx, cdy = mean_disp.loc[clust, ['dx','dy']]\n",
    "                            cnorm = np.sqrt(cdx**2 + cdy**2)\n",
    "                            if cnorm > 0:\n",
    "                                cdx /= cnorm\n",
    "                                cdy /= cnorm\n",
    "                            length = cluster_norms.loc[clust]\n",
    "                            cdx *= length\n",
    "                            cdy *= length\n",
    "                            cluster_arrows.append((clust, cx, cy, cdx, cdy))\n",
    "                    \n",
    "                    timepoint_results[source_tp]['cluster_arrows'] = cluster_arrows\n",
    "    \n",
    "                    if len(cluster_arrows) > 0:\n",
    "                        # \"Aggregated\" arrow by summation\n",
    "                        source_median_x = source_cells['UMAP_2'].median()\n",
    "                        source_median_y = source_cells['UMAP_1'].median()\n",
    "                        total_dx = sum([arrow[3] for arrow in cluster_arrows])\n",
    "                        total_dy = sum([arrow[4] for arrow in cluster_arrows])\n",
    "                        timepoint_results[source_tp]['aggregated_arrow'] = (\n",
    "                            source_median_x, \n",
    "                            source_median_y, \n",
    "                            total_dx, \n",
    "                            total_dy\n",
    "                        )\n",
    "                    else:\n",
    "                        timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "    \n",
    "                    # Compute target cluster distributions for each COI (source cluster)\n",
    "                    for coi in clusters_of_interest:\n",
    "                        coi_mask = (source_cells['seurat_clusters'] == coi)\n",
    "                        if np.any(coi_mask):\n",
    "                            # Which target clusters do these COI cells map to?\n",
    "                            selected_target_indices = full_target_indices[coi_mask]\n",
    "                            selected_target_clusters = target_cells['seurat_clusters'].iloc[selected_target_indices].values\n",
    "                            unique_tclusters, counts = np.unique(selected_target_clusters, return_counts=True)\n",
    "                            total_count = counts.sum()\n",
    "                            if total_count > 0:\n",
    "                                fraction_dict = {int(tc): (ct / total_count) for tc, ct in zip(unique_tclusters, counts)}\n",
    "                            else:\n",
    "                                fraction_dict = {}\n",
    "                            distributions_coi[source_tp][cohort_name][coi] = fraction_dict\n",
    "                        else:\n",
    "                            distributions_coi[source_tp][cohort_name][coi] = {}\n",
    "                else:\n",
    "                    # If cluster info is missing\n",
    "                    timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                    timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "                    for coi in clusters_of_interest:\n",
    "                        distributions_coi[source_tp][cohort_name][coi] = {}\n",
    "        else:\n",
    "            # Last timepoint has no \"next\" timepoint\n",
    "            timepoint_results[source_tp]['single_cell_displacements'] = np.array([])\n",
    "            timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "            timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "            timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "            for coi in clusters_of_interest:\n",
    "                distributions_coi[source_tp][cohort_name][coi] = {}\n",
    "\n",
    "    # ---------------------------\n",
    "    # Generate PDFs of movement\n",
    "    # ---------------------------\n",
    "\n",
    "    # 1) Single-cell arrows\n",
    "    output_file_original = os.path.join(\n",
    "        output_folder, \n",
    "        f\"{subpop_name}_{cohort_name}_movement_plots_differential_arrow_lengths_equal_cells_using_PCs.pdf\"\n",
    "    )\n",
    "    with PdfPages(output_file_original) as pdf_original:\n",
    "        plot_tps = list(timepoint_results.keys())\n",
    "        num_timepoints = len(plot_tps)\n",
    "        fig_orig, axes_orig = plt.subplots(\n",
    "            1, \n",
    "            num_timepoints, \n",
    "            figsize=(4 * num_timepoints, 4), \n",
    "            sharex=True, \n",
    "            sharey=True\n",
    "        )\n",
    "        if num_timepoints == 1:\n",
    "            axes_orig = [axes_orig]\n",
    "        \n",
    "        for i, source_tp in enumerate(plot_tps):\n",
    "            ax = axes_orig[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            sampled_source_cells = res['sampled_source']\n",
    "            sampled_gray_cells = res['sampled_gray']\n",
    "            \n",
    "            gray_x = sampled_gray_cells['UMAP_2'].values\n",
    "            gray_y = sampled_gray_cells['UMAP_1'].values\n",
    "            source_x = sampled_source_cells['UMAP_2'].values\n",
    "            source_y = sampled_source_cells['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "            \n",
    "            single_cell_displacements = res['single_cell_displacements']\n",
    "            single_cell_arrow_lengths = res['single_cell_arrow_lengths']\n",
    "            \n",
    "            if len(single_cell_displacements) > 0:\n",
    "                source_coords_sampled = sampled_source_cells[['UMAP_2','UMAP_1']].values\n",
    "                sampled_norms = np.linalg.norm(single_cell_displacements, axis=1)\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    unit_vectors = single_cell_displacements / sampled_norms[:, np.newaxis]\n",
    "                    unit_vectors[~np.isfinite(unit_vectors)] = 0\n",
    "                scaled_vectors = unit_vectors * single_cell_arrow_lengths[:, np.newaxis]\n",
    "                \n",
    "                for j in range(len(source_coords_sampled)):\n",
    "                    sx, sy = source_coords_sampled[j]\n",
    "                    dx, dy = scaled_vectors[j]\n",
    "                    ax.arrow(\n",
    "                        sx, sy, \n",
    "                        dx, dy,\n",
    "                        color='black', \n",
    "                        alpha=0.7, \n",
    "                        head_width=0.05, \n",
    "                        head_length=0.05,\n",
    "                        length_includes_head=True, \n",
    "                        linewidth=0.5\n",
    "                    )\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_orig.suptitle(f\"{subpop_name} - {cohort_name}\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_original.savefig(fig_orig)\n",
    "        plt.close(fig_orig)\n",
    "    \n",
    "    # 2) Cluster-level arrows\n",
    "    output_file_cluster = os.path.join(\n",
    "        output_folder, \n",
    "        f\"{subpop_name}_{cohort_name}_movement_plots_aggregated_cluster_arrows_equal_cells_using_PCs.pdf\"\n",
    "    )\n",
    "    with PdfPages(output_file_cluster) as pdf_cluster:\n",
    "        plot_tps = list(timepoint_results.keys())\n",
    "        num_timepoints = len(plot_tps)\n",
    "        fig_clust, axes_clust = plt.subplots(\n",
    "            1, \n",
    "            num_timepoints, \n",
    "            figsize=(4 * num_timepoints, 4), \n",
    "            sharex=True, \n",
    "            sharey=True\n",
    "        )\n",
    "        if num_timepoints == 1:\n",
    "            axes_clust = [axes_clust]\n",
    "        \n",
    "        for i, source_tp in enumerate(plot_tps):\n",
    "            ax = axes_clust[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            \n",
    "            gray_x = res['sampled_gray']['UMAP_2'].values\n",
    "            gray_y = res['sampled_gray']['UMAP_1'].values\n",
    "            source_x = res['sampled_source']['UMAP_2'].values\n",
    "            source_y = res['sampled_source']['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "            \n",
    "            cluster_arrows = res.get('cluster_arrows', [])\n",
    "            \n",
    "            source_cells = full_data[source_tp]['source']\n",
    "            if 'seurat_clusters' in source_cells.columns:\n",
    "                source_cluster_counts = source_cells['seurat_clusters'].value_counts()\n",
    "                total_source_cells_current = len(source_cells)\n",
    "            else:\n",
    "                source_cluster_counts = pd.Series()\n",
    "                total_source_cells_current = 1\n",
    "            \n",
    "            for (clust, cx, cy, cdx, cdy) in cluster_arrows:\n",
    "                arrow_color = cluster_colors_map.get(clust, 'black')\n",
    "                proportion = source_cluster_counts.get(clust, 0) / total_source_cells_current\n",
    "                line_width = 1.0 + 8.0 * proportion\n",
    "\n",
    "                # Outline in black for clarity\n",
    "                ax.arrow(\n",
    "                    cx, cy,\n",
    "                    cdx, cdy,\n",
    "                    color='black',\n",
    "                    alpha=1,\n",
    "                    head_width=0.2 + 0.5 * proportion,\n",
    "                    head_length=0.1 + 0.1 * proportion,\n",
    "                    length_includes_head=True,\n",
    "                    linewidth=line_width + 2\n",
    "                )\n",
    "                \n",
    "                # Main arrow in cluster color\n",
    "                ax.arrow(\n",
    "                    cx, cy,\n",
    "                    cdx, cdy,\n",
    "                    # color=arrow_color,\n",
    "                    color='black',\n",
    "                    alpha=1,\n",
    "                    head_width=0.2 + 0.5 * proportion,\n",
    "                    head_length=0.1 + 0.1 * proportion,\n",
    "                    length_includes_head=True,\n",
    "                    linewidth=line_width\n",
    "                )\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_clust.suptitle(f\"{subpop_name} - {cohort_name} (Cluster-Level Arrows)\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_cluster.savefig(fig_clust)\n",
    "        plt.close(fig_clust)\n",
    "\n",
    "    # 3) Single aggregated arrow\n",
    "    output_file_single_agg = os.path.join(\n",
    "        output_folder, \n",
    "        f\"{subpop_name}_{cohort_name}_movement_plots_aggregated_single_arrow_equal_cells_using_PCs.pdf\"\n",
    "    )\n",
    "    with PdfPages(output_file_single_agg) as pdf_agg:\n",
    "        plot_tps = list(timepoint_results.keys())\n",
    "        num_timepoints = len(plot_tps)\n",
    "        fig_agg, axes_agg = plt.subplots(\n",
    "            1,\n",
    "            num_timepoints,\n",
    "            figsize=(4 * num_timepoints, 4),\n",
    "            sharex=True,\n",
    "            sharey=True\n",
    "        )\n",
    "        if num_timepoints == 1:\n",
    "            axes_agg = [axes_agg]\n",
    "\n",
    "        for i, source_tp in enumerate(plot_tps):\n",
    "            ax = axes_agg[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "\n",
    "            gray_x = res['sampled_gray']['UMAP_2'].values\n",
    "            gray_y = res['sampled_gray']['UMAP_1'].values\n",
    "            source_x = res['sampled_source']['UMAP_2'].values\n",
    "            source_y = res['sampled_source']['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "\n",
    "            aggregated_arrow = res.get('aggregated_arrow', None)\n",
    "            if aggregated_arrow is not None:\n",
    "                global_cx, global_cy, total_dx, total_dy = aggregated_arrow\n",
    "                ax.arrow(\n",
    "                    global_cx, global_cy,\n",
    "                    total_dx, total_dy,\n",
    "                    color='black', \n",
    "                    alpha=0.9,\n",
    "                    head_width=0.3, \n",
    "                    head_length=0.3,\n",
    "                    length_includes_head=True, \n",
    "                    linewidth=2.0\n",
    "                )\n",
    "\n",
    "            ax.set_title(f\"Timepoint: {source_tp} (Single Aggregated Arrow)\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        fig_agg.suptitle(f\"{subpop_name} - {cohort_name} (Single Aggregated Arrow)\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_agg.savefig(fig_agg)\n",
    "        plt.close(fig_agg)\n",
    "    \n",
    "    # Return the distributions for stacked-bar plotting\n",
    "    return distributions_coi\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution Loop\n",
    "# ---------------------------\n",
    "for subpop_name in subpopulations:\n",
    "    # Prepare a nested dict so we can collect distributions\n",
    "    distributions_coi_all = {\n",
    "        tp: {\n",
    "            c: {coi: {} for coi in clusters_of_interest} \n",
    "            for c in cohorts\n",
    "        }\n",
    "        for tp in all_timepoints\n",
    "    }\n",
    "    \n",
    "    # Run for each cohort\n",
    "    for cohort_name in cohorts:\n",
    "        print(f\"Processing {subpop_name} - {cohort_name}\")\n",
    "        input_subfolder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "        if not os.path.exists(input_subfolder):\n",
    "            print(f\"Input subfolder does not exist: {input_subfolder}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        dist_coi = optimal_transport_visualization(subpop_name, cohort_name)\n",
    "        if dist_coi is not None:\n",
    "            for tp in dist_coi:\n",
    "                for c in dist_coi[tp]:\n",
    "                    for coi in dist_coi[tp][c]:\n",
    "                        distributions_coi_all[tp][c][coi] = dist_coi[tp][c][coi]\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 1) Group non-COI target clusters into \"Others\"\n",
    "    # --------------------------------------------------\n",
    "    for tp in distributions_coi_all:\n",
    "        for c in cohorts:\n",
    "            for coi in clusters_of_interest:\n",
    "                fraction_dict = distributions_coi_all[tp][c][coi]\n",
    "                if fraction_dict:\n",
    "                    new_dict = {}\n",
    "                    others_sum = 0.0\n",
    "                    for tclust, frac in fraction_dict.items():\n",
    "                        # If the target cluster is not in our \"source\" list, \n",
    "                        # put it in the \"Others\" bin\n",
    "                        if tclust not in clusters_of_interest:\n",
    "                            others_sum += frac\n",
    "                        else:\n",
    "                            new_dict[tclust] = frac\n",
    "                    if others_sum > 0:\n",
    "                        new_dict['Others'] = others_sum\n",
    "                    distributions_coi_all[tp][c][coi] = new_dict\n",
    "\n",
    "    # Gather all target clusters (including \"Others\")\n",
    "    all_target_clusters = []\n",
    "    for tp in distributions_coi_all:\n",
    "        for c in cohorts:\n",
    "            for coi in clusters_of_interest:\n",
    "                all_target_clusters.extend(distributions_coi_all[tp][c][coi].keys())\n",
    "    all_target_clusters = set(all_target_clusters)\n",
    "\n",
    "    # Handle \"Others\" separately for sorting\n",
    "    others_present = (\"Others\" in all_target_clusters)\n",
    "    if others_present:\n",
    "        all_target_clusters.remove(\"Others\")\n",
    "\n",
    "    # Now only numeric cluster IDs remain\n",
    "    # Sort numeric clusters in ascending order\n",
    "    all_target_clusters = sorted(all_target_clusters)\n",
    "\n",
    "    # Put \"Others\" at the end if present\n",
    "    if others_present:\n",
    "        all_target_clusters.append(\"Others\")\n",
    "\n",
    "    def get_celltype_name(clust_id):\n",
    "        if clust_id == 'Others':\n",
    "            return \"Others\"\n",
    "        return cluster_celltype_map.get(clust_id, f\"Cluster {clust_id}\")\n",
    "\n",
    "    def get_cluster_color(clust_id):\n",
    "        if clust_id == 'Others':\n",
    "            return 'gray'\n",
    "        return cluster_colors_map.get(clust_id, 'gray')\n",
    "\n",
    "    # ---------------------------\n",
    "    # Generate the stacked bar chart PDF\n",
    "    # ---------------------------\n",
    "    output_file_stacked = os.path.join(base_output_dir, f\"{subpop_name}_target_cluster_distribution_coi_with_stack_labels.pdf\")\n",
    "    with PdfPages(output_file_stacked) as pdf_dist:\n",
    "        fig, axes = plt.subplots(\n",
    "            len(clusters_of_interest), \n",
    "            len(all_timepoints), \n",
    "            figsize=(4 * len(all_timepoints), 3 * len(clusters_of_interest)), \n",
    "            sharex=False, \n",
    "            sharey=True\n",
    "        )\n",
    "        \n",
    "        # Handle shape for single row/col\n",
    "        if len(clusters_of_interest) == 1 and len(all_timepoints) == 1:\n",
    "            axes = np.array([[axes]])\n",
    "        elif len(clusters_of_interest) == 1:\n",
    "            axes = axes[np.newaxis, :]\n",
    "        elif len(all_timepoints) == 1:\n",
    "            axes = axes[:, np.newaxis]\n",
    "\n",
    "        # Prepare legend patches\n",
    "        legend_patches = []\n",
    "        for tc in all_target_clusters:\n",
    "            legend_patches.append(\n",
    "                plt.Rectangle((0,0),1,1,\n",
    "                              facecolor=get_cluster_color(tc),\n",
    "                              edgecolor='black',\n",
    "                              label=get_celltype_name(tc))\n",
    "            )\n",
    "        \n",
    "        # Plot bars\n",
    "        for row_i, coi in enumerate(clusters_of_interest):\n",
    "            for col_i, tp in enumerate(all_timepoints):\n",
    "                ax = axes[row_i, col_i]\n",
    "                bar_positions = np.arange(len(cohorts))\n",
    "                bottoms = np.zeros(len(cohorts))\n",
    "\n",
    "                # Retrieve distribution data for each cohort at (tp, coi)\n",
    "                cohort_distributions = [distributions_coi_all[tp][c][coi] for c in cohorts]\n",
    "\n",
    "                # Build up data for stacked bars: (target_cluster, [fractions for each cohort])\n",
    "                stack_data = []\n",
    "                for tc in all_target_clusters:\n",
    "                    h = [dist.get(tc, 0.0) for dist in cohort_distributions]\n",
    "                    stack_data.append((tc, h))\n",
    "\n",
    "                # Sort so biggest fraction (summed across cohorts) is at the bottom\n",
    "                stack_data.sort(key=lambda x: sum(x[1]), reverse=True)\n",
    "\n",
    "                # Plot\n",
    "                for (tc, heights) in stack_data:\n",
    "                    color = get_cluster_color(tc)\n",
    "                    ax.bar(\n",
    "                        bar_positions, \n",
    "                        heights, \n",
    "                        bottom=bottoms, \n",
    "                        color=color, \n",
    "                        edgecolor='black'\n",
    "                    )\n",
    "                    # Label each segment if fraction is big enough\n",
    "                    for idx, val in enumerate(heights):\n",
    "                        if val > 0.05:\n",
    "                            mid_y = bottoms[idx] + val / 2\n",
    "                            ax.text(\n",
    "                                bar_positions[idx], \n",
    "                                mid_y, \n",
    "                                get_celltype_name(tc),\n",
    "                                ha='center', \n",
    "                                va='center',\n",
    "                                fontsize=6, \n",
    "                                color='white'\n",
    "                            )\n",
    "                    bottoms += heights\n",
    "\n",
    "                if row_i == 0:\n",
    "                    ax.set_title(f\"{tp}\", fontsize=10)\n",
    "\n",
    "                ax.set_xticks(bar_positions)\n",
    "                ax.set_xticklabels(cohorts, rotation=45, ha='right', fontsize=8)\n",
    "                ax.set_ylim(0, 1)\n",
    "\n",
    "        fig.suptitle(f\"Distribution of Target Clusters per COI - {subpop_name}\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Adjust to make room for row labels & legend\n",
    "        fig.subplots_adjust(left=0.15, top=0.86, right=0.82)  # extra space on the right for legend\n",
    "\n",
    "        # Label rows with the celltype of the COI\n",
    "        n_rows = len(clusters_of_interest)\n",
    "        for row_i, coi in enumerate(clusters_of_interest):\n",
    "            y_pos = 0.86 - (row_i + 0.5)*(0.86-0.1)/n_rows\n",
    "            fig.text(0.05, y_pos, get_celltype_name(coi), va='center', ha='right', fontsize=10, color='black')\n",
    "\n",
    "        # Create a single-column legend on the right\n",
    "        fig.legend(\n",
    "            handles=legend_patches,\n",
    "            loc='upper left',\n",
    "            bbox_to_anchor=(0.84, 0.95),\n",
    "            ncol=1,\n",
    "            fontsize=8,\n",
    "            title=\"Target Celltypes\"\n",
    "        )\n",
    "\n",
    "        pdf_dist.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "print(\"Visualization generation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a1f7014-d4ab-4e27-a097-d4945df7552f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean diversity (Group1): 0.2626\n",
      "Mean diversity (Group2): 0.5767\n",
      "p-value: 0.177660\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ot  # Make sure you have installed POT or similar library for optimal transport\n",
    "\n",
    "def compute_shannon_diversity(counts_dict):\n",
    "    \"\"\"\n",
    "    Compute the Shannon diversity index of a distribution given as a dictionary of counts.\n",
    "    H = - (p_i * log(p_i)).\n",
    "    \"\"\"\n",
    "    import math\n",
    "    total = sum(counts_dict.values())\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    # Convert counts to proportions\n",
    "    proportions = [count / total for count in counts_dict.values()]\n",
    "    # Calculate Shannon entropy\n",
    "    shannon_entropy = -sum([p * math.log(p + 1e-12) for p in proportions])\n",
    "    return shannon_entropy\n",
    "\n",
    "def compute_simpson_diversity(counts_dict):\n",
    "    \"\"\"\n",
    "    Compute Simpsons diversity index of a distribution given as a dictionary of counts.\n",
    "    Simpsons index = 1 - (p_i), where p_i = count_i / total_counts.\n",
    "    \"\"\"\n",
    "    total = sum(counts_dict.values())\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Convert counts to proportions\n",
    "    proportions = [count / total for count in counts_dict.values()]\n",
    "    # Calculate sum of squares of proportions\n",
    "    sum_of_squares = sum(p ** 2 for p in proportions)\n",
    "    # Simpsons index\n",
    "    simpson_index = 1 - sum_of_squares\n",
    "    return simpson_index\n",
    "\n",
    "\n",
    "def compute_target_diversity_for_group_ot(\n",
    "    base_directory,\n",
    "    subpop_name,\n",
    "    group_name,\n",
    "    timepoint,\n",
    "    mapping,\n",
    "    source_subpop\n",
    "):\n",
    "    \"\"\"\n",
    "    For a single group (e.g., \"short_term\"), compute target diversity index *per patient*\n",
    "    using optimal transport.\n",
    "\n",
    "    Steps (per patient):\n",
    "    1) Read source_cells_new.csv (source) and target_cells_new.csv (target).\n",
    "    2) Filter the source cells to include only the cluster IDs belonging to `source_subpop`.\n",
    "    3) Run optimal transport (OT) from those filtered source cells to all target cells \n",
    "       (for that patient). \n",
    "       - We'll use PC columns to compute the cost matrix, as in your snippet.\n",
    "    4) For each source cell, pick the target cell that has the highest transport weight.\n",
    "    5) Look at the distribution of the matched target cells clusters -> compute diversity.\n",
    "    6) Return {patient_id -> diversity_index}.\n",
    "    \"\"\"\n",
    "    source_path = os.path.join(\n",
    "        base_directory, subpop_name, group_name, f\"Timepoint_{timepoint}\", \"source_cells_new.csv\"\n",
    "    )\n",
    "    target_path = os.path.join(\n",
    "        base_directory, subpop_name, group_name, f\"Timepoint_{timepoint}\", \"target_cells_new.csv\"\n",
    "    )\n",
    "    \n",
    "    if not os.path.exists(source_path):\n",
    "        raise FileNotFoundError(f\"File not found: {source_path}\")\n",
    "    if not os.path.exists(target_path):\n",
    "        raise FileNotFoundError(f\"File not found: {target_path}\")\n",
    "    \n",
    "    source_df = pd.read_csv(source_path)\n",
    "    target_df = pd.read_csv(target_path)\n",
    "\n",
    "    # Check that the CSVs contain the necessary columns.\n",
    "    # Adapt these column names to your actual data.\n",
    "    required_cols = {'cell_id', 'Patient', 'seurat_clusters'}\n",
    "    for colset, df in zip(['source', 'target'], [source_df, target_df]):\n",
    "        if not required_cols.issubset(df.columns):\n",
    "            raise ValueError(f\"Dataframe {colset} is missing required columns: {required_cols}\")\n",
    "    \n",
    "    # We also need PC columns (or whatever you plan to use for OT).\n",
    "    # For example, let's look for 'PC_' columns.\n",
    "    pc_cols = [c for c in source_df.columns if c.startswith(\"PC_\")]\n",
    "    if not pc_cols:\n",
    "        raise ValueError(\"No PC_ columns found for computing cost matrix. Check your data.\")\n",
    "    \n",
    "    # Identify which clusters represent `source_subpop`\n",
    "    source_cluster_ids = mapping[source_subpop]\n",
    "    \n",
    "    # We'll compute a dictionary of {patient_id: diversity}\n",
    "    patient_diversity = {}\n",
    "    all_patients = source_df['Patient'].unique()\n",
    "\n",
    "    for patient_id in all_patients:\n",
    "        # Filter source cells for this patient\n",
    "        patient_source = source_df[\n",
    "            (source_df['Patient'] == patient_id) &\n",
    "            (source_df['seurat_clusters'].isin(source_cluster_ids))\n",
    "        ].copy()\n",
    "        \n",
    "        # If there are no source cells of interest, skip or store 0\n",
    "        if patient_source.empty:\n",
    "            patient_diversity[patient_id] = 0.0\n",
    "            continue\n",
    "        \n",
    "        # Filter target cells for this patient\n",
    "        patient_target = target_df[target_df['Patient'] == patient_id].copy()\n",
    "        \n",
    "        # If no target cells, skip\n",
    "        if patient_target.empty:\n",
    "            patient_diversity[patient_id] = 0.0\n",
    "            continue\n",
    "\n",
    "        # Prepare data for OT\n",
    "        # We'll use PC columns for cost\n",
    "        source_coords = patient_source[pc_cols].values\n",
    "        target_coords = patient_target[pc_cols].values\n",
    "        \n",
    "        # Probability distributions (uniform over source and target)\n",
    "        a = np.ones((source_coords.shape[0],)) / source_coords.shape[0]\n",
    "        b = np.ones((target_coords.shape[0],)) / target_coords.shape[0]\n",
    "        \n",
    "        # Euclidean cost matrix\n",
    "        cost_matrix = ot.dist(source_coords, target_coords, metric='euclidean')\n",
    "        \n",
    "        try:\n",
    "            transport_plan = ot.emd(a, b, cost_matrix, numItermax=100000)\n",
    "        except Exception as e:\n",
    "            print(f\"OT computation failed for patient {patient_id}: {e}\")\n",
    "            patient_diversity[patient_id] = 0.0\n",
    "            continue\n",
    "        \n",
    "        # transport_plan has shape (num_source_cells, num_target_cells).\n",
    "        # For each source cell (row), pick the target cell (col) that has max weight.\n",
    "        matched_target_indices = np.argmax(transport_plan, axis=1)\n",
    "        \n",
    "        # Now find the target cell clusters for these matched indices.\n",
    "        # matched_target_indices[i] is the column index in patient_target\n",
    "        target_clusters_matched = patient_target.iloc[matched_target_indices]['seurat_clusters'].values\n",
    "        \n",
    "        # Count how many times each cluster appears\n",
    "        cluster_counts = pd.Series(target_clusters_matched).value_counts().to_dict()\n",
    "        \n",
    "        # Compute diversity\n",
    "        diversity_val = compute_shannon_diversity(cluster_counts)\n",
    "        # diversity_val = compute_simpson_diversity(cluster_counts)\n",
    "        patient_diversity[patient_id] = diversity_val\n",
    "    \n",
    "    return patient_diversity\n",
    "\n",
    "\n",
    "def compute_p_value(diversity_group1, diversity_group2, n_boot=100000):\n",
    "    \"\"\"\n",
    "    Given two dictionaries { patient_id: diversity_val } for group1 and group2,\n",
    "    compute the p-value by permutation test (two-tailed).\n",
    "    \"\"\"\n",
    "    group1_vals = np.array(list(diversity_group1.values()))\n",
    "    group2_vals = np.array(list(diversity_group2.values()))\n",
    "    \n",
    "    # A small constant to prevent division by zero\n",
    "    EPS = 1e-20\n",
    "    true_diff = group2_vals.mean() / (group1_vals.mean() + EPS)\n",
    "    # true_diff = group2_vals.mean() - group1_vals.mean()\n",
    "    # true_diff = group2_vals.mean() / group1_vals.mean()\n",
    "    \n",
    "    combined = np.concatenate([group1_vals, group2_vals])\n",
    "    n1 = len(group1_vals)\n",
    "    n2 = len(group2_vals)\n",
    "    \n",
    "    count_extreme = 0\n",
    "    for _ in range(n_boot):\n",
    "        np.random.shuffle(combined)\n",
    "        perm_g1 = combined[:n1]\n",
    "        perm_g2 = combined[n1:n1 + n2]\n",
    "        # perm_diff = perm_g2.mean() - perm_g1.mean()\n",
    "        perm_diff = perm_g2.mean() / (perm_g1.mean() + EPS)\n",
    "        # perm_diff = perm_g2.mean() / perm_g1.mean()\n",
    "        if abs(perm_diff) >= abs(true_diff):\n",
    "            count_extreme += 1\n",
    "    \n",
    "    pval = count_extreme / n_boot\n",
    "    return pval\n",
    "\n",
    "\n",
    "def compare_groups_diversity_ot(\n",
    "    base_directory,\n",
    "    subpop_name,\n",
    "    mapping,\n",
    "    timepoint,\n",
    "    group1_names,\n",
    "    group2_names,\n",
    "    source_subpop,\n",
    "    n_boot=100000\n",
    "):\n",
    "    \"\"\"\n",
    "    Main function to:\n",
    "    1) Compute target diversity distributions for two sets of groups (e.g., short_term vs long_term),\n",
    "       but using OT-based mapping from source to target cells.\n",
    "    2) Perform a permutation test on the difference of means.\n",
    "    \"\"\"\n",
    "    # For group1 (can have multiple group names combined)\n",
    "    diversity_group1 = {}\n",
    "    for g_name in group1_names:\n",
    "        group_div = compute_target_diversity_for_group_ot(\n",
    "            base_directory=base_directory,\n",
    "            subpop_name=subpop_name,\n",
    "            group_name=g_name,\n",
    "            timepoint=timepoint,\n",
    "            mapping=mapping,\n",
    "            source_subpop=source_subpop\n",
    "        )\n",
    "        # Merge them (disambiguate with the group name + patient ID, or just patient ID if unique)\n",
    "        for pid, val in group_div.items():\n",
    "            # If a patient can appear in multiple subgroups, you might need special logic.\n",
    "            # For now, we assume each patient is unique to that group.\n",
    "            diversity_group1[(g_name, pid)] = val\n",
    "    \n",
    "    # For group2\n",
    "    diversity_group2 = {}\n",
    "    for g_name in group2_names:\n",
    "        group_div = compute_target_diversity_for_group_ot(\n",
    "            base_directory=base_directory,\n",
    "            subpop_name=subpop_name,\n",
    "            group_name=g_name,\n",
    "            timepoint=timepoint,\n",
    "            mapping=mapping,\n",
    "            source_subpop=source_subpop\n",
    "        )\n",
    "        for pid, val in group_div.items():\n",
    "            diversity_group2[(g_name, pid)] = val\n",
    "    \n",
    "    # Compute p-value\n",
    "    pval = compute_p_value(diversity_group1, diversity_group2, n_boot=n_boot)\n",
    "\n",
    "    # Print results\n",
    "    mean1 = np.mean(list(diversity_group1.values()))\n",
    "    mean2 = np.mean(list(diversity_group2.values()))\n",
    "    print(f\"Mean diversity (Group1): {mean1:.4f}\")\n",
    "    print(f\"Mean diversity (Group2): {mean2:.4f}\")\n",
    "    print(f\"p-value: {pval:.6f}\")\n",
    "\n",
    "    return pval, diversity_group1, diversity_group2\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    celltype_to_cluster = {\n",
    "        \"Effector_CD8\": [1],\n",
    "        \"Memory_Precursor_Effector_CD8\": [2],\n",
    "        \"Exhausted_T\": [3],\n",
    "        \"Stem_Like_CD8\": [8],\n",
    "        \"Effector_Memory_CD8\": [10],\n",
    "        \"Central_Memory_CD8\": [12],\n",
    "        \"Proliferating_Effector\": [14, 16, 17]\n",
    "    }\n",
    "    \n",
    "    base_directory = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/final_nomenclature/coi\"\n",
    "    subpop_name = \"Central_Memory_CD8\"\n",
    "    mapping = celltype_to_cluster\n",
    "    timepoint = \"C1\"\n",
    "    group1_names = [\"short_term\"]\n",
    "    group2_names = [\"long_term\"]\n",
    "    source_subpop = \"Central_Memory_CD8\"\n",
    "    n_boot = 100000\n",
    "\n",
    "    p_val, div_g1, div_g2 = compare_groups_diversity_ot(\n",
    "        base_directory=base_directory,\n",
    "        subpop_name=subpop_name,\n",
    "        mapping=mapping,\n",
    "        timepoint=timepoint,\n",
    "        group1_names=group1_names,\n",
    "        group2_names=group2_names,\n",
    "        source_subpop=source_subpop,\n",
    "        n_boot=n_boot\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b838c22f-c945-40a1-86e5-3f762bc76f86",
   "metadata": {},
   "source": [
    "#### Generating plots where cluster of interest is only Central Memory CD8 T Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b161e598-466b-4eb7-bcba-4bb97febb836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Effector_CD8 - control\n",
      "Processing Effector_CD8 - short_term\n",
      "Processing Effector_CD8 - long_term\n",
      "Processing Memory_Precursor_Effector_CD8 - control\n",
      "Processing Memory_Precursor_Effector_CD8 - short_term\n",
      "Processing Memory_Precursor_Effector_CD8 - long_term\n",
      "Processing Exhausted_T - control\n",
      "Processing Exhausted_T - short_term\n",
      "Processing Exhausted_T - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Central_Memory_CD8 - control\n",
      "Processing Central_Memory_CD8 - short_term\n",
      "Processing Central_Memory_CD8 - long_term\n",
      "Processing Stem_Like_CD8 - control\n",
      "Processing Stem_Like_CD8 - short_term\n",
      "Processing Stem_Like_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Effector_Memory_CD8 - control\n",
      "Processing Effector_Memory_CD8 - short_term\n",
      "Processing Effector_Memory_CD8 - long_term\n",
      "Processing Proliferating_Effector - control\n",
      "Processing Proliferating_Effector - short_term\n",
      "Processing Proliferating_Effector - long_term\n",
      "Processing All_CD8 - control\n",
      "Processing All_CD8 - short_term\n",
      "Processing All_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization generation completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ot\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration Parameters\n",
    "# ---------------------------\n",
    "\n",
    "base_input_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/final_nomenclature/coi\"\n",
    "base_output_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/final_nomenclature/coi_central_memory_cd8\"\n",
    "\n",
    "subpopulations = [\n",
    "    \"Effector_CD8\",\n",
    "    \"Memory_Precursor_Effector_CD8\",\n",
    "    \"Exhausted_T\",\n",
    "    \"Central_Memory_CD8\",\n",
    "    \"Stem_Like_CD8\",\n",
    "    \"Effector_Memory_CD8\",\n",
    "    \"Proliferating_Effector\",\n",
    "    \"All_CD8\"\n",
    "]\n",
    "\n",
    "cohorts = [\"control\", \"short_term\", \"long_term\"]\n",
    "\n",
    "all_timepoints = [\"Pre\", \"C1\", \"C2\", \"C4\", \"C6\", \"C9\", \"C18\", \"C36\"]\n",
    "\n",
    "cohort_colors = {\n",
    "    'control': 'yellow',\n",
    "    'short_term': 'blue',\n",
    "    'long_term': 'red'\n",
    "}\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Clusters we are interested in as the \"source\" (COI)\n",
    "# --------------------------------------------------\n",
    "clusters_of_interest = [12]\n",
    "\n",
    "# Provide the path to your CSV that maps each cluster to a color and celltype\n",
    "color_mapping_file = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/Publication_Material/T_Cell_cluster_colors.csv\"\n",
    "color_mapping_df = pd.read_csv(color_mapping_file)\n",
    "cluster_colors_map = dict(zip(color_mapping_df['Cluster'], color_mapping_df['Color']))\n",
    "cluster_celltype_map = dict(zip(color_mapping_df['Cluster'], color_mapping_df['Celltype']))\n",
    "\n",
    "def optimal_transport_visualization(subpop_name, cohort_name):\n",
    "    \"\"\"\n",
    "    For a single subpopulation and cohort, run the full OT pipeline:\n",
    "    1) Load data at each timepoint.\n",
    "    2) Perform OT to map from source to next timepoint.\n",
    "    3) Collect single-cell and cluster-level arrows, plus aggregated arrows.\n",
    "    4) Generate PDFs of the movement plots (single-cell, cluster-level, aggregated).\n",
    "    5) Return distributions_coi for stacked-bar plotting.\n",
    "    \"\"\"\n",
    "    input_folder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "    output_folder = os.path.join(base_output_dir, subpop_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return None\n",
    "    \n",
    "    timepoint_folders = sorted([f for f in os.listdir(input_folder) if f.startswith(\"Timepoint_\")])\n",
    "    available_timepoints = [tp.split(\"_\")[1] for tp in timepoint_folders]\n",
    "    cohort_timepoints = [tp for tp in all_timepoints if tp in available_timepoints]\n",
    "    \n",
    "    if not cohort_timepoints:\n",
    "        print(f\"No timepoints available for {subpop_name} in {cohort_name} cohort.\")\n",
    "        return None\n",
    "\n",
    "    all_x_coords = []\n",
    "    all_y_coords = []\n",
    "    \n",
    "    cell_counts_source = []\n",
    "    cell_counts_gray = []\n",
    "    full_data = {}\n",
    "    \n",
    "    # Gather data across all valid timepoints\n",
    "    for tp in cohort_timepoints:\n",
    "        source_folder = os.path.join(input_folder, f\"Timepoint_{tp}\")\n",
    "        source_cells_file = os.path.join(source_folder, 'source_cells_new.csv')\n",
    "        gray_cells_file = os.path.join(source_folder, 'gray_cells_new.csv')\n",
    "        \n",
    "        if not (os.path.exists(source_cells_file) and os.path.exists(gray_cells_file)):\n",
    "            print(f\"Missing files for {tp}. Skipping this timepoint.\")\n",
    "            continue\n",
    "        \n",
    "        source_cells = pd.read_csv(source_cells_file)\n",
    "        gray_cells = pd.read_csv(gray_cells_file)\n",
    "        \n",
    "        if source_cells.empty:\n",
    "            print(f\"No source cells for {tp}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        full_data[tp] = {\n",
    "            'source': source_cells,\n",
    "            'gray': gray_cells\n",
    "        }\n",
    "        \n",
    "        all_x_coords.extend(gray_cells['UMAP_2'])\n",
    "        all_x_coords.extend(source_cells['UMAP_2'])\n",
    "        all_y_coords.extend(gray_cells['UMAP_1'])\n",
    "        all_y_coords.extend(source_cells['UMAP_1'])\n",
    "        \n",
    "        cell_counts_source.append(len(source_cells))\n",
    "        cell_counts_gray.append(len(gray_cells))\n",
    "    \n",
    "    if not full_data:\n",
    "        print(\"No valid timepoints with data.\")\n",
    "        return None\n",
    "    \n",
    "    x_min, x_max = min(all_x_coords), max(all_x_coords)\n",
    "    y_min, y_max = min(all_y_coords), max(all_y_coords)\n",
    "    \n",
    "    min_source_cells = min(cell_counts_source) if cell_counts_source else 0\n",
    "    min_gray_cells = min(cell_counts_gray) if cell_counts_gray else 0\n",
    "    if min_source_cells == 0 or min_gray_cells == 0:\n",
    "        print(\"Insufficient cells. Skipping visualization.\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare data structures for results\n",
    "    timepoint_results = {}\n",
    "    distributions_coi = {tp: {cohort_name: {c: {} for c in clusters_of_interest}} for tp in cohort_timepoints}\n",
    "    \n",
    "    for i, source_tp in enumerate(cohort_timepoints):\n",
    "        source_data = full_data[source_tp]\n",
    "        source_cells = source_data['source']\n",
    "        \n",
    "        # Downsample so each timepoint has the same # of source & gray cells for plotting\n",
    "        sampled_source_cells = source_cells.sample(n=min_source_cells, random_state=42)\n",
    "        sampled_gray_cells = full_data[source_tp]['gray'].sample(n=min_gray_cells, random_state=42)\n",
    "        \n",
    "        timepoint_results[source_tp] = {\n",
    "            'sampled_source': sampled_source_cells,\n",
    "            'sampled_gray': sampled_gray_cells\n",
    "        }\n",
    "        \n",
    "        # If not the last timepoint, compute OT from source_tp -> target_tp\n",
    "        if i < len(cohort_timepoints) - 1:\n",
    "            target_tp = cohort_timepoints[i+1]\n",
    "            target_data = full_data[target_tp]\n",
    "            target_cells = target_data['source']\n",
    "            \n",
    "            if target_cells.empty:\n",
    "                # If there's no data in the next timepoint, skip\n",
    "                timepoint_results[source_tp]['single_cell_displacements'] = np.array([])\n",
    "                timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "            else:\n",
    "                # Identify columns for PCs\n",
    "                pc_cols = [c for c in source_cells.columns if c.startswith('PC_')]\n",
    "                full_source_coords_pc = source_cells[pc_cols].values\n",
    "                full_target_coords_pc = target_cells[pc_cols].values\n",
    "                \n",
    "                # UMAP coords\n",
    "                full_source_coords_umap = source_cells[['UMAP_2', 'UMAP_1']].values\n",
    "                full_target_coords_umap = target_cells[['UMAP_2', 'UMAP_1']].values\n",
    "                \n",
    "                # Uniform distribution over source / target\n",
    "                a = np.ones((full_source_coords_pc.shape[0],)) / full_source_coords_pc.shape[0]\n",
    "                b = np.ones((full_target_coords_pc.shape[0],)) / full_target_coords_pc.shape[0]\n",
    "                \n",
    "                cost_matrix = ot.dist(full_source_coords_pc, full_target_coords_pc, metric='euclidean')\n",
    "                \n",
    "                try:\n",
    "                    transport_plan = ot.emd(a, b, cost_matrix, numItermax=100000)\n",
    "                except Exception as e:\n",
    "                    print(f\"OT computation failed for {source_tp} to {target_tp}: {e}\")\n",
    "                    timepoint_results[source_tp]['single_cell_displacements'] = np.array([])\n",
    "                    timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "                    timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                    timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "                    continue\n",
    "                \n",
    "                full_target_indices = np.argmax(transport_plan, axis=1)\n",
    "                displacement_vectors_full = full_target_coords_umap[full_target_indices] - full_source_coords_umap\n",
    "                norms_full = np.linalg.norm(displacement_vectors_full, axis=1)\n",
    "                \n",
    "                # Displacements for the downsampled subset\n",
    "                sampled_source_indices = source_cells.index.get_indexer_for(sampled_source_cells.index)\n",
    "                sampled_displacements = displacement_vectors_full[sampled_source_indices, :]\n",
    "                sampled_norms = norms_full[sampled_source_indices]\n",
    "                \n",
    "                timepoint_results[source_tp]['single_cell_displacements'] = sampled_displacements\n",
    "                timepoint_results[source_tp]['single_cell_arrow_lengths'] = sampled_norms\n",
    "                \n",
    "                if 'seurat_clusters' in source_cells.columns and 'seurat_clusters' in target_cells.columns:\n",
    "                    # Lump some clusters if needed\n",
    "                    source_cells['seurat_clusters'] = source_cells['seurat_clusters'].replace({16:14,17:14})\n",
    "                    target_cells['seurat_clusters'] = target_cells['seurat_clusters'].replace({16:14,17:14})\n",
    "                    source_cells['seurat_clusters'] = source_cells['seurat_clusters'].replace({9:6,18:6})\n",
    "                    target_cells['seurat_clusters'] = target_cells['seurat_clusters'].replace({9:6,18:6})\n",
    "                    \n",
    "                    # For cluster-level arrows, compute average displacement per cluster\n",
    "                    source_clusters_full = source_cells['seurat_clusters'].values\n",
    "                    df_cluster_full = pd.DataFrame({\n",
    "                        'cluster': source_clusters_full,\n",
    "                        'sx': full_source_coords_umap[:,0],\n",
    "                        'sy': full_source_coords_umap[:,1],\n",
    "                        'dx': displacement_vectors_full[:,0],\n",
    "                        'dy': displacement_vectors_full[:,1],\n",
    "                        'norm': norms_full\n",
    "                    })\n",
    "                    \n",
    "                    gray_cells_tp = full_data[source_tp]['gray']\n",
    "                    all_cells_combined = pd.concat([source_cells, gray_cells_tp])\n",
    "                    all_cells_combined['seurat_clusters'] = all_cells_combined['seurat_clusters'].replace({16:14,17:14})\n",
    "                    all_cells_combined['seurat_clusters'] = all_cells_combined['seurat_clusters'].replace({9:6,18:6})\n",
    "                    \n",
    "                    all_clusters = all_cells_combined['seurat_clusters'].values\n",
    "                    all_coords_umap = all_cells_combined[['UMAP_2', 'UMAP_1']].values\n",
    "                    \n",
    "                    df_all = pd.DataFrame({\n",
    "                        'cluster': all_clusters,\n",
    "                        'sx': all_coords_umap[:,0],\n",
    "                        'sy': all_coords_umap[:,1]\n",
    "                    })\n",
    "                    \n",
    "                    centroids = df_all.groupby('cluster')[['sx','sy']].median()\n",
    "                    mean_disp = df_cluster_full.groupby('cluster')[['dx','dy']].mean()\n",
    "                    \n",
    "                    cluster_norms = np.sqrt(mean_disp['dx']**2 + mean_disp['dy']**2)\n",
    "                    \n",
    "                    # Build arrow info for each cluster of interest\n",
    "                    cluster_arrows = []\n",
    "                    common_clusters = centroids.index.intersection(mean_disp.index)\n",
    "                    for clust in common_clusters:\n",
    "                        if clust in clusters_of_interest:\n",
    "                            cx, cy = centroids.loc[clust, ['sx','sy']]\n",
    "                            cdx, cdy = mean_disp.loc[clust, ['dx','dy']]\n",
    "                            cnorm = np.sqrt(cdx**2 + cdy**2)\n",
    "                            if cnorm > 0:\n",
    "                                cdx /= cnorm\n",
    "                                cdy /= cnorm\n",
    "                            length = cluster_norms.loc[clust]\n",
    "                            cdx *= length\n",
    "                            cdy *= length\n",
    "                            cluster_arrows.append((clust, cx, cy, cdx, cdy))\n",
    "                    \n",
    "                    timepoint_results[source_tp]['cluster_arrows'] = cluster_arrows\n",
    "    \n",
    "                    if len(cluster_arrows) > 0:\n",
    "                        # \"Aggregated\" arrow by summation\n",
    "                        source_median_x = source_cells['UMAP_2'].median()\n",
    "                        source_median_y = source_cells['UMAP_1'].median()\n",
    "                        total_dx = sum([arrow[3] for arrow in cluster_arrows])\n",
    "                        total_dy = sum([arrow[4] for arrow in cluster_arrows])\n",
    "                        timepoint_results[source_tp]['aggregated_arrow'] = (\n",
    "                            source_median_x, \n",
    "                            source_median_y, \n",
    "                            total_dx, \n",
    "                            total_dy\n",
    "                        )\n",
    "                    else:\n",
    "                        timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "    \n",
    "                    # Compute target cluster distributions for each COI (source cluster)\n",
    "                    for coi in clusters_of_interest:\n",
    "                        coi_mask = (source_cells['seurat_clusters'] == coi)\n",
    "                        if np.any(coi_mask):\n",
    "                            # Which target clusters do these COI cells map to?\n",
    "                            selected_target_indices = full_target_indices[coi_mask]\n",
    "                            selected_target_clusters = target_cells['seurat_clusters'].iloc[selected_target_indices].values\n",
    "                            unique_tclusters, counts = np.unique(selected_target_clusters, return_counts=True)\n",
    "                            total_count = counts.sum()\n",
    "                            if total_count > 0:\n",
    "                                fraction_dict = {int(tc): (ct / total_count) for tc, ct in zip(unique_tclusters, counts)}\n",
    "                            else:\n",
    "                                fraction_dict = {}\n",
    "                            distributions_coi[source_tp][cohort_name][coi] = fraction_dict\n",
    "                        else:\n",
    "                            distributions_coi[source_tp][cohort_name][coi] = {}\n",
    "                else:\n",
    "                    # If cluster info is missing\n",
    "                    timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                    timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "                    for coi in clusters_of_interest:\n",
    "                        distributions_coi[source_tp][cohort_name][coi] = {}\n",
    "        else:\n",
    "            # Last timepoint has no \"next\" timepoint\n",
    "            timepoint_results[source_tp]['single_cell_displacements'] = np.array([])\n",
    "            timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "            timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "            timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "            for coi in clusters_of_interest:\n",
    "                distributions_coi[source_tp][cohort_name][coi] = {}\n",
    "\n",
    "    # ---------------------------\n",
    "    # Generate PDFs of movement\n",
    "    # ---------------------------\n",
    "\n",
    "    # 1) Single-cell arrows\n",
    "    output_file_original = os.path.join(\n",
    "        output_folder, \n",
    "        f\"{subpop_name}_{cohort_name}_movement_plots_differential_arrow_lengths_equal_cells_using_PCs.pdf\"\n",
    "    )\n",
    "    with PdfPages(output_file_original) as pdf_original:\n",
    "        plot_tps = list(timepoint_results.keys())\n",
    "        num_timepoints = len(plot_tps)\n",
    "        fig_orig, axes_orig = plt.subplots(\n",
    "            1, \n",
    "            num_timepoints, \n",
    "            figsize=(4 * num_timepoints, 4), \n",
    "            sharex=True, \n",
    "            sharey=True\n",
    "        )\n",
    "        if num_timepoints == 1:\n",
    "            axes_orig = [axes_orig]\n",
    "        \n",
    "        for i, source_tp in enumerate(plot_tps):\n",
    "            ax = axes_orig[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            sampled_source_cells = res['sampled_source']\n",
    "            sampled_gray_cells = res['sampled_gray']\n",
    "            \n",
    "            gray_x = sampled_gray_cells['UMAP_2'].values\n",
    "            gray_y = sampled_gray_cells['UMAP_1'].values\n",
    "            source_x = sampled_source_cells['UMAP_2'].values\n",
    "            source_y = sampled_source_cells['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "            \n",
    "            single_cell_displacements = res['single_cell_displacements']\n",
    "            single_cell_arrow_lengths = res['single_cell_arrow_lengths']\n",
    "            \n",
    "            # ---------------------------\n",
    "            # Only draw arrows for COI\n",
    "            # ---------------------------\n",
    "            if len(single_cell_displacements) > 0:\n",
    "                # Coordinates for all sampled source cells\n",
    "                source_coords_sampled = sampled_source_cells[['UMAP_2','UMAP_1']].values\n",
    "                \n",
    "                # Compute the unit vectors for each displacement\n",
    "                sampled_norms = np.linalg.norm(single_cell_displacements, axis=1)\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    unit_vectors = single_cell_displacements / sampled_norms[:, np.newaxis]\n",
    "                    unit_vectors[~np.isfinite(unit_vectors)] = 0\n",
    "                scaled_vectors = unit_vectors * single_cell_arrow_lengths[:, np.newaxis]\n",
    "                \n",
    "                # If you have cluster info, filter by clusters_of_interest\n",
    "                if 'seurat_clusters' in sampled_source_cells.columns:\n",
    "                    for j in range(len(source_coords_sampled)):\n",
    "                        # Check if this cell is in one of your clusters of interest\n",
    "                        current_cluster = sampled_source_cells.iloc[j]['seurat_clusters']\n",
    "                        if current_cluster not in clusters_of_interest:\n",
    "                            # Skip drawing arrows if not in COI\n",
    "                            continue\n",
    "                        \n",
    "                        sx, sy = source_coords_sampled[j]\n",
    "                        dx, dy = scaled_vectors[j]\n",
    "                        ax.arrow(\n",
    "                            sx, sy, \n",
    "                            dx, dy,\n",
    "                            color='black', \n",
    "                            alpha=0.7, \n",
    "                            head_width=0.2, \n",
    "                            head_length=0.2,\n",
    "                            length_includes_head=True, \n",
    "                            linewidth=0.5\n",
    "                        )\n",
    "                else:\n",
    "                    # If no cluster column, you can either skip or draw all\n",
    "                    for j in range(len(source_coords_sampled)):\n",
    "                        sx, sy = source_coords_sampled[j]\n",
    "                        dx, dy = scaled_vectors[j]\n",
    "                        ax.arrow(\n",
    "                            sx, sy, \n",
    "                            dx, dy,\n",
    "                            color='black', \n",
    "                            alpha=0.7, \n",
    "                            head_width=0.2, \n",
    "                            head_length=0.2,\n",
    "                            length_includes_head=True, \n",
    "                            linewidth=0.5\n",
    "                        )\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_orig.suptitle(f\"{subpop_name} - {cohort_name}\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_original.savefig(fig_orig)\n",
    "        plt.close(fig_orig)\n",
    "    \n",
    "    # 2) Cluster-level arrows\n",
    "    output_file_cluster = os.path.join(\n",
    "        output_folder, \n",
    "        f\"{subpop_name}_{cohort_name}_movement_plots_aggregated_cluster_arrows_equal_cells_using_PCs.pdf\"\n",
    "    )\n",
    "    with PdfPages(output_file_cluster) as pdf_cluster:\n",
    "        plot_tps = list(timepoint_results.keys())\n",
    "        num_timepoints = len(plot_tps)\n",
    "        fig_clust, axes_clust = plt.subplots(\n",
    "            1, \n",
    "            num_timepoints, \n",
    "            figsize=(4 * num_timepoints, 4), \n",
    "            sharex=True, \n",
    "            sharey=True\n",
    "        )\n",
    "        if num_timepoints == 1:\n",
    "            axes_clust = [axes_clust]\n",
    "        \n",
    "        for i, source_tp in enumerate(plot_tps):\n",
    "            ax = axes_clust[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            \n",
    "            gray_x = res['sampled_gray']['UMAP_2'].values\n",
    "            gray_y = res['sampled_gray']['UMAP_1'].values\n",
    "            source_x = res['sampled_source']['UMAP_2'].values\n",
    "            source_y = res['sampled_source']['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "            \n",
    "            cluster_arrows = res.get('cluster_arrows', [])\n",
    "            \n",
    "            source_cells = full_data[source_tp]['source']\n",
    "            if 'seurat_clusters' in source_cells.columns:\n",
    "                source_cluster_counts = source_cells['seurat_clusters'].value_counts()\n",
    "                total_source_cells_current = len(source_cells)\n",
    "            else:\n",
    "                source_cluster_counts = pd.Series()\n",
    "                total_source_cells_current = 1\n",
    "            \n",
    "            for (clust, cx, cy, cdx, cdy) in cluster_arrows:\n",
    "                arrow_color = cluster_colors_map.get(clust, 'black')\n",
    "                proportion = source_cluster_counts.get(clust, 0) / total_source_cells_current\n",
    "                line_width = 1.0 + 8.0 * proportion\n",
    "\n",
    "                # Outline in black for clarity\n",
    "                ax.arrow(\n",
    "                    cx, cy,\n",
    "                    cdx, cdy,\n",
    "                    color='black',\n",
    "                    alpha=1,\n",
    "                    head_width=0.2 + 0.5 * proportion,\n",
    "                    head_length=0.1 + 0.1 * proportion,\n",
    "                    length_includes_head=True,\n",
    "                    linewidth=line_width + 2\n",
    "                )\n",
    "                \n",
    "                # Main arrow in cluster color\n",
    "                ax.arrow(\n",
    "                    cx, cy,\n",
    "                    cdx, cdy,\n",
    "                    color=arrow_color,\n",
    "                    alpha=1,\n",
    "                    head_width=0.2 + 0.5 * proportion,\n",
    "                    head_length=0.1 + 0.1 * proportion,\n",
    "                    length_includes_head=True,\n",
    "                    linewidth=line_width\n",
    "                )\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_clust.suptitle(f\"{subpop_name} - {cohort_name} (Cluster-Level Arrows)\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_cluster.savefig(fig_clust)\n",
    "        plt.close(fig_clust)\n",
    "\n",
    "    # 3) Single aggregated arrow\n",
    "    output_file_single_agg = os.path.join(\n",
    "        output_folder, \n",
    "        f\"{subpop_name}_{cohort_name}_movement_plots_aggregated_single_arrow_equal_cells_using_PCs.pdf\"\n",
    "    )\n",
    "    with PdfPages(output_file_single_agg) as pdf_agg:\n",
    "        plot_tps = list(timepoint_results.keys())\n",
    "        num_timepoints = len(plot_tps)\n",
    "        fig_agg, axes_agg = plt.subplots(\n",
    "            1,\n",
    "            num_timepoints,\n",
    "            figsize=(4 * num_timepoints, 4),\n",
    "            sharex=True,\n",
    "            sharey=True\n",
    "        )\n",
    "        if num_timepoints == 1:\n",
    "            axes_agg = [axes_agg]\n",
    "\n",
    "        for i, source_tp in enumerate(plot_tps):\n",
    "            ax = axes_agg[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "\n",
    "            gray_x = res['sampled_gray']['UMAP_2'].values\n",
    "            gray_y = res['sampled_gray']['UMAP_1'].values\n",
    "            source_x = res['sampled_source']['UMAP_2'].values\n",
    "            source_y = res['sampled_source']['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "\n",
    "            aggregated_arrow = res.get('aggregated_arrow', None)\n",
    "            if aggregated_arrow is not None:\n",
    "                global_cx, global_cy, total_dx, total_dy = aggregated_arrow\n",
    "                ax.arrow(\n",
    "                    global_cx, global_cy,\n",
    "                    total_dx, total_dy,\n",
    "                    color='black', \n",
    "                    alpha=0.9,\n",
    "                    head_width=0.3, \n",
    "                    head_length=0.3,\n",
    "                    length_includes_head=True, \n",
    "                    linewidth=2.0\n",
    "                )\n",
    "\n",
    "            ax.set_title(f\"Timepoint: {source_tp} (Single Aggregated Arrow)\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        fig_agg.suptitle(f\"{subpop_name} - {cohort_name} (Single Aggregated Arrow)\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_agg.savefig(fig_agg)\n",
    "        plt.close(fig_agg)\n",
    "    \n",
    "    # Return the distributions for stacked-bar plotting\n",
    "    return distributions_coi\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution Loop\n",
    "# ---------------------------\n",
    "for subpop_name in subpopulations:\n",
    "    # Prepare a nested dict so we can collect distributions\n",
    "    distributions_coi_all = {\n",
    "        tp: {\n",
    "            c: {coi: {} for coi in clusters_of_interest} \n",
    "            for c in cohorts\n",
    "        }\n",
    "        for tp in all_timepoints\n",
    "    }\n",
    "    \n",
    "    # Run for each cohort\n",
    "    for cohort_name in cohorts:\n",
    "        print(f\"Processing {subpop_name} - {cohort_name}\")\n",
    "        input_subfolder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "        if not os.path.exists(input_subfolder):\n",
    "            print(f\"Input subfolder does not exist: {input_subfolder}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        dist_coi = optimal_transport_visualization(subpop_name, cohort_name)\n",
    "        if dist_coi is not None:\n",
    "            for tp in dist_coi:\n",
    "                for c in dist_coi[tp]:\n",
    "                    for coi in dist_coi[tp][c]:\n",
    "                        distributions_coi_all[tp][c][coi] = dist_coi[tp][c][coi]\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 1) Group non-COI target clusters into \"Others\"\n",
    "    # --------------------------------------------------\n",
    "    for tp in distributions_coi_all:\n",
    "        for c in cohorts:\n",
    "            for coi in clusters_of_interest:\n",
    "                fraction_dict = distributions_coi_all[tp][c][coi]\n",
    "                if fraction_dict:\n",
    "                    new_dict = {}\n",
    "                    others_sum = 0.0\n",
    "                    for tclust, frac in fraction_dict.items():\n",
    "                        # If the target cluster is not in our \"source\" list, \n",
    "                        # put it in the \"Others\" bin\n",
    "                        if tclust not in clusters_of_interest:\n",
    "                            others_sum += frac\n",
    "                        else:\n",
    "                            new_dict[tclust] = frac\n",
    "                    if others_sum > 0:\n",
    "                        new_dict['Others'] = others_sum\n",
    "                    distributions_coi_all[tp][c][coi] = new_dict\n",
    "\n",
    "    # Gather all target clusters (including \"Others\")\n",
    "    all_target_clusters = []\n",
    "    for tp in distributions_coi_all:\n",
    "        for c in cohorts:\n",
    "            for coi in clusters_of_interest:\n",
    "                all_target_clusters.extend(distributions_coi_all[tp][c][coi].keys())\n",
    "    all_target_clusters = set(all_target_clusters)\n",
    "\n",
    "    # Handle \"Others\" separately for sorting\n",
    "    others_present = (\"Others\" in all_target_clusters)\n",
    "    if others_present:\n",
    "        all_target_clusters.remove(\"Others\")\n",
    "\n",
    "    # Now only numeric cluster IDs remain\n",
    "    # Sort numeric clusters in ascending order\n",
    "    all_target_clusters = sorted(all_target_clusters)\n",
    "\n",
    "    # Put \"Others\" at the end if present\n",
    "    if others_present:\n",
    "        all_target_clusters.append(\"Others\")\n",
    "\n",
    "    def get_celltype_name(clust_id):\n",
    "        if clust_id == 'Others':\n",
    "            return \"Others\"\n",
    "        return cluster_celltype_map.get(clust_id, f\"Cluster {clust_id}\")\n",
    "\n",
    "    def get_cluster_color(clust_id):\n",
    "        if clust_id == 'Others':\n",
    "            return 'gray'\n",
    "        return cluster_colors_map.get(clust_id, 'gray')\n",
    "\n",
    "    # ---------------------------\n",
    "    # Generate the stacked bar chart PDF\n",
    "    # ---------------------------\n",
    "    output_file_stacked = os.path.join(base_output_dir, f\"{subpop_name}_target_cluster_distribution_coi_with_stack_labels.pdf\")\n",
    "    with PdfPages(output_file_stacked) as pdf_dist:\n",
    "        fig, axes = plt.subplots(\n",
    "            len(clusters_of_interest), \n",
    "            len(all_timepoints), \n",
    "            figsize=(4 * len(all_timepoints), 3 * len(clusters_of_interest)), \n",
    "            sharex=False, \n",
    "            sharey=True\n",
    "        )\n",
    "        \n",
    "        # Handle shape for single row/col\n",
    "        if len(clusters_of_interest) == 1 and len(all_timepoints) == 1:\n",
    "            axes = np.array([[axes]])\n",
    "        elif len(clusters_of_interest) == 1:\n",
    "            axes = axes[np.newaxis, :]\n",
    "        elif len(all_timepoints) == 1:\n",
    "            axes = axes[:, np.newaxis]\n",
    "\n",
    "        # Prepare legend patches\n",
    "        legend_patches = []\n",
    "        for tc in all_target_clusters:\n",
    "            legend_patches.append(\n",
    "                plt.Rectangle((0,0),1,1,\n",
    "                              facecolor=get_cluster_color(tc),\n",
    "                              edgecolor='black',\n",
    "                              label=get_celltype_name(tc))\n",
    "            )\n",
    "        \n",
    "        # Plot bars\n",
    "        for row_i, coi in enumerate(clusters_of_interest):\n",
    "            for col_i, tp in enumerate(all_timepoints):\n",
    "                ax = axes[row_i, col_i]\n",
    "                bar_positions = np.arange(len(cohorts))\n",
    "                bottoms = np.zeros(len(cohorts))\n",
    "\n",
    "                # Retrieve distribution data for each cohort at (tp, coi)\n",
    "                cohort_distributions = [distributions_coi_all[tp][c][coi] for c in cohorts]\n",
    "\n",
    "                # Build up data for stacked bars: (target_cluster, [fractions for each cohort])\n",
    "                stack_data = []\n",
    "                for tc in all_target_clusters:\n",
    "                    h = [dist.get(tc, 0.0) for dist in cohort_distributions]\n",
    "                    stack_data.append((tc, h))\n",
    "\n",
    "                # Sort so biggest fraction (summed across cohorts) is at the bottom\n",
    "                stack_data.sort(key=lambda x: sum(x[1]), reverse=True)\n",
    "\n",
    "                # Plot\n",
    "                for (tc, heights) in stack_data:\n",
    "                    color = get_cluster_color(tc)\n",
    "                    ax.bar(\n",
    "                        bar_positions, \n",
    "                        heights, \n",
    "                        bottom=bottoms, \n",
    "                        color=color, \n",
    "                        edgecolor='black'\n",
    "                    )\n",
    "                    # Label each segment if fraction is big enough\n",
    "                    for idx, val in enumerate(heights):\n",
    "                        if val > 0.05:\n",
    "                            mid_y = bottoms[idx] + val / 2\n",
    "                            ax.text(\n",
    "                                bar_positions[idx], \n",
    "                                mid_y, \n",
    "                                get_celltype_name(tc),\n",
    "                                ha='center', \n",
    "                                va='center',\n",
    "                                fontsize=6, \n",
    "                                color='white'\n",
    "                            )\n",
    "                    bottoms += heights\n",
    "\n",
    "                if row_i == 0:\n",
    "                    ax.set_title(f\"{tp}\", fontsize=10)\n",
    "\n",
    "                ax.set_xticks(bar_positions)\n",
    "                ax.set_xticklabels(cohorts, rotation=45, ha='right', fontsize=8)\n",
    "                ax.set_ylim(0, 1)\n",
    "\n",
    "        fig.suptitle(f\"Distribution of Target Clusters per COI - {subpop_name}\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Adjust to make room for row labels & legend\n",
    "        fig.subplots_adjust(left=0.15, top=0.86, right=0.82)  # extra space on the right for legend\n",
    "\n",
    "        # Label rows with the celltype of the COI\n",
    "        n_rows = len(clusters_of_interest)\n",
    "        for row_i, coi in enumerate(clusters_of_interest):\n",
    "            y_pos = 0.86 - (row_i + 0.5)*(0.86-0.1)/n_rows\n",
    "            fig.text(0.05, y_pos, get_celltype_name(coi), va='center', ha='right', fontsize=10, color='black')\n",
    "\n",
    "        # Create a single-column legend on the right\n",
    "        fig.legend(\n",
    "            handles=legend_patches,\n",
    "            loc='upper left',\n",
    "            bbox_to_anchor=(0.84, 0.95),\n",
    "            ncol=1,\n",
    "            fontsize=8,\n",
    "            title=\"Target Celltypes\"\n",
    "        )\n",
    "\n",
    "        pdf_dist.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "print(\"Visualization generation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3454136-1179-4222-bf6e-9aa978a12e26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vef_env",
   "language": "python",
   "name": "vef_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
