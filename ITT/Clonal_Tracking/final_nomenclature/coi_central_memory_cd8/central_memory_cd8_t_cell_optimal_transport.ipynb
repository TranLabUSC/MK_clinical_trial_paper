{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b838c22f-c945-40a1-86e5-3f762bc76f86",
   "metadata": {},
   "source": [
    "#### Generating plots where cluster of interest is only Central Memory CD8 T Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b161e598-466b-4eb7-bcba-4bb97febb836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Effector_CD8 - control\n",
      "Processing Effector_CD8 - short_term\n",
      "Processing Effector_CD8 - long_term\n",
      "Processing Memory_Precursor_Effector_CD8 - control\n",
      "Processing Memory_Precursor_Effector_CD8 - short_term\n",
      "Processing Memory_Precursor_Effector_CD8 - long_term\n",
      "Processing Exhausted_T - control\n",
      "Processing Exhausted_T - short_term\n",
      "Processing Exhausted_T - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Central_Memory_CD8 - control\n",
      "Processing Central_Memory_CD8 - short_term\n",
      "Processing Central_Memory_CD8 - long_term\n",
      "Processing Stem_Like_CD8 - control\n",
      "Processing Stem_Like_CD8 - short_term\n",
      "Processing Stem_Like_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Effector_Memory_CD8 - control\n",
      "Processing Effector_Memory_CD8 - short_term\n",
      "Processing Effector_Memory_CD8 - long_term\n",
      "Processing Proliferating_Effector - control\n",
      "Processing Proliferating_Effector - short_term\n",
      "Processing Proliferating_Effector - long_term\n",
      "Processing All_CD8 - control\n",
      "Processing All_CD8 - short_term\n",
      "Processing All_CD8 - long_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n",
      "/home1/h160983/.conda/envs/moscot_env/lib/python3.9/site-packages/ot/lp/__init__.py:361: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization generation completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ot\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration Parameters\n",
    "# ---------------------------\n",
    "\n",
    "base_input_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/final_nomenclature/coi\"\n",
    "base_output_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/final_nomenclature/coi_central_memory_cd8\"\n",
    "\n",
    "subpopulations = [\n",
    "    \"Effector_CD8\",\n",
    "    \"Memory_Precursor_Effector_CD8\",\n",
    "    \"Exhausted_T\",\n",
    "    \"Central_Memory_CD8\",\n",
    "    \"Stem_Like_CD8\",\n",
    "    \"Effector_Memory_CD8\",\n",
    "    \"Proliferating_Effector\",\n",
    "    \"All_CD8\"\n",
    "]\n",
    "\n",
    "cohorts = [\"control\", \"short_term\", \"long_term\"]\n",
    "\n",
    "all_timepoints = [\"Pre\", \"C1\", \"C2\", \"C4\", \"C6\", \"C9\", \"C18\", \"C36\"]\n",
    "\n",
    "cohort_colors = {\n",
    "    'control': 'yellow',\n",
    "    'short_term': 'blue',\n",
    "    'long_term': 'red'\n",
    "}\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Clusters we are interested in as the \"source\" (COI)\n",
    "# --------------------------------------------------\n",
    "clusters_of_interest = [12]\n",
    "\n",
    "# Provide the path to your CSV that maps each cluster to a color and celltype\n",
    "color_mapping_file = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/Publication_Material/T_Cell_cluster_colors.csv\"\n",
    "color_mapping_df = pd.read_csv(color_mapping_file)\n",
    "cluster_colors_map = dict(zip(color_mapping_df['Cluster'], color_mapping_df['Color']))\n",
    "cluster_celltype_map = dict(zip(color_mapping_df['Cluster'], color_mapping_df['Celltype']))\n",
    "\n",
    "def optimal_transport_visualization(subpop_name, cohort_name):\n",
    "    \"\"\"\n",
    "    For a single subpopulation and cohort, run the full OT pipeline:\n",
    "    1) Load data at each timepoint.\n",
    "    2) Perform OT to map from source to next timepoint.\n",
    "    3) Collect single-cell and cluster-level arrows, plus aggregated arrows.\n",
    "    4) Generate PDFs of the movement plots (single-cell, cluster-level, aggregated).\n",
    "    5) Return distributions_coi for stacked-bar plotting.\n",
    "    \"\"\"\n",
    "    input_folder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "    output_folder = os.path.join(base_output_dir, subpop_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return None\n",
    "    \n",
    "    timepoint_folders = sorted([f for f in os.listdir(input_folder) if f.startswith(\"Timepoint_\")])\n",
    "    available_timepoints = [tp.split(\"_\")[1] for tp in timepoint_folders]\n",
    "    cohort_timepoints = [tp for tp in all_timepoints if tp in available_timepoints]\n",
    "    \n",
    "    if not cohort_timepoints:\n",
    "        print(f\"No timepoints available for {subpop_name} in {cohort_name} cohort.\")\n",
    "        return None\n",
    "\n",
    "    all_x_coords = []\n",
    "    all_y_coords = []\n",
    "    \n",
    "    cell_counts_source = []\n",
    "    cell_counts_gray = []\n",
    "    full_data = {}\n",
    "    \n",
    "    # Gather data across all valid timepoints\n",
    "    for tp in cohort_timepoints:\n",
    "        source_folder = os.path.join(input_folder, f\"Timepoint_{tp}\")\n",
    "        source_cells_file = os.path.join(source_folder, 'source_cells_new.csv')\n",
    "        gray_cells_file = os.path.join(source_folder, 'gray_cells_new.csv')\n",
    "        \n",
    "        if not (os.path.exists(source_cells_file) and os.path.exists(gray_cells_file)):\n",
    "            print(f\"Missing files for {tp}. Skipping this timepoint.\")\n",
    "            continue\n",
    "        \n",
    "        source_cells = pd.read_csv(source_cells_file)\n",
    "        gray_cells = pd.read_csv(gray_cells_file)\n",
    "        \n",
    "        if source_cells.empty:\n",
    "            print(f\"No source cells for {tp}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        full_data[tp] = {\n",
    "            'source': source_cells,\n",
    "            'gray': gray_cells\n",
    "        }\n",
    "        \n",
    "        all_x_coords.extend(gray_cells['UMAP_2'])\n",
    "        all_x_coords.extend(source_cells['UMAP_2'])\n",
    "        all_y_coords.extend(gray_cells['UMAP_1'])\n",
    "        all_y_coords.extend(source_cells['UMAP_1'])\n",
    "        \n",
    "        cell_counts_source.append(len(source_cells))\n",
    "        cell_counts_gray.append(len(gray_cells))\n",
    "    \n",
    "    if not full_data:\n",
    "        print(\"No valid timepoints with data.\")\n",
    "        return None\n",
    "    \n",
    "    x_min, x_max = min(all_x_coords), max(all_x_coords)\n",
    "    y_min, y_max = min(all_y_coords), max(all_y_coords)\n",
    "    \n",
    "    min_source_cells = min(cell_counts_source) if cell_counts_source else 0\n",
    "    min_gray_cells = min(cell_counts_gray) if cell_counts_gray else 0\n",
    "    if min_source_cells == 0 or min_gray_cells == 0:\n",
    "        print(\"Insufficient cells. Skipping visualization.\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare data structures for results\n",
    "    timepoint_results = {}\n",
    "    distributions_coi = {tp: {cohort_name: {c: {} for c in clusters_of_interest}} for tp in cohort_timepoints}\n",
    "    \n",
    "    for i, source_tp in enumerate(cohort_timepoints):\n",
    "        source_data = full_data[source_tp]\n",
    "        source_cells = source_data['source']\n",
    "        \n",
    "        # Downsample so each timepoint has the same # of source & gray cells for plotting\n",
    "        sampled_source_cells = source_cells.sample(n=min_source_cells, random_state=42)\n",
    "        sampled_gray_cells = full_data[source_tp]['gray'].sample(n=min_gray_cells, random_state=42)\n",
    "        \n",
    "        timepoint_results[source_tp] = {\n",
    "            'sampled_source': sampled_source_cells,\n",
    "            'sampled_gray': sampled_gray_cells\n",
    "        }\n",
    "        \n",
    "        # If not the last timepoint, compute OT from source_tp -> target_tp\n",
    "        if i < len(cohort_timepoints) - 1:\n",
    "            target_tp = cohort_timepoints[i+1]\n",
    "            target_data = full_data[target_tp]\n",
    "            target_cells = target_data['source']\n",
    "            \n",
    "            if target_cells.empty:\n",
    "                # If there's no data in the next timepoint, skip\n",
    "                timepoint_results[source_tp]['single_cell_displacements'] = np.array([])\n",
    "                timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "                timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "            else:\n",
    "                # Identify columns for PCs\n",
    "                pc_cols = [c for c in source_cells.columns if c.startswith('PC_')]\n",
    "                full_source_coords_pc = source_cells[pc_cols].values\n",
    "                full_target_coords_pc = target_cells[pc_cols].values\n",
    "                \n",
    "                # UMAP coords\n",
    "                full_source_coords_umap = source_cells[['UMAP_2', 'UMAP_1']].values\n",
    "                full_target_coords_umap = target_cells[['UMAP_2', 'UMAP_1']].values\n",
    "                \n",
    "                # Uniform distribution over source / target\n",
    "                a = np.ones((full_source_coords_pc.shape[0],)) / full_source_coords_pc.shape[0]\n",
    "                b = np.ones((full_target_coords_pc.shape[0],)) / full_target_coords_pc.shape[0]\n",
    "                \n",
    "                cost_matrix = ot.dist(full_source_coords_pc, full_target_coords_pc, metric='euclidean')\n",
    "                \n",
    "                try:\n",
    "                    transport_plan = ot.emd(a, b, cost_matrix, numItermax=100000)\n",
    "                except Exception as e:\n",
    "                    print(f\"OT computation failed for {source_tp} to {target_tp}: {e}\")\n",
    "                    timepoint_results[source_tp]['single_cell_displacements'] = np.array([])\n",
    "                    timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "                    timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                    timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "                    continue\n",
    "                \n",
    "                full_target_indices = np.argmax(transport_plan, axis=1)\n",
    "                displacement_vectors_full = full_target_coords_umap[full_target_indices] - full_source_coords_umap\n",
    "                norms_full = np.linalg.norm(displacement_vectors_full, axis=1)\n",
    "                \n",
    "                # Displacements for the downsampled subset\n",
    "                sampled_source_indices = source_cells.index.get_indexer_for(sampled_source_cells.index)\n",
    "                sampled_displacements = displacement_vectors_full[sampled_source_indices, :]\n",
    "                sampled_norms = norms_full[sampled_source_indices]\n",
    "                \n",
    "                timepoint_results[source_tp]['single_cell_displacements'] = sampled_displacements\n",
    "                timepoint_results[source_tp]['single_cell_arrow_lengths'] = sampled_norms\n",
    "                \n",
    "                if 'seurat_clusters' in source_cells.columns and 'seurat_clusters' in target_cells.columns:\n",
    "                    # Lump some clusters if needed\n",
    "                    source_cells['seurat_clusters'] = source_cells['seurat_clusters'].replace({16:14,17:14})\n",
    "                    target_cells['seurat_clusters'] = target_cells['seurat_clusters'].replace({16:14,17:14})\n",
    "                    source_cells['seurat_clusters'] = source_cells['seurat_clusters'].replace({9:6,18:6})\n",
    "                    target_cells['seurat_clusters'] = target_cells['seurat_clusters'].replace({9:6,18:6})\n",
    "                    \n",
    "                    # For cluster-level arrows, compute average displacement per cluster\n",
    "                    source_clusters_full = source_cells['seurat_clusters'].values\n",
    "                    df_cluster_full = pd.DataFrame({\n",
    "                        'cluster': source_clusters_full,\n",
    "                        'sx': full_source_coords_umap[:,0],\n",
    "                        'sy': full_source_coords_umap[:,1],\n",
    "                        'dx': displacement_vectors_full[:,0],\n",
    "                        'dy': displacement_vectors_full[:,1],\n",
    "                        'norm': norms_full\n",
    "                    })\n",
    "                    \n",
    "                    gray_cells_tp = full_data[source_tp]['gray']\n",
    "                    all_cells_combined = pd.concat([source_cells, gray_cells_tp])\n",
    "                    all_cells_combined['seurat_clusters'] = all_cells_combined['seurat_clusters'].replace({16:14,17:14})\n",
    "                    all_cells_combined['seurat_clusters'] = all_cells_combined['seurat_clusters'].replace({9:6,18:6})\n",
    "                    \n",
    "                    all_clusters = all_cells_combined['seurat_clusters'].values\n",
    "                    all_coords_umap = all_cells_combined[['UMAP_2', 'UMAP_1']].values\n",
    "                    \n",
    "                    df_all = pd.DataFrame({\n",
    "                        'cluster': all_clusters,\n",
    "                        'sx': all_coords_umap[:,0],\n",
    "                        'sy': all_coords_umap[:,1]\n",
    "                    })\n",
    "                    \n",
    "                    centroids = df_all.groupby('cluster')[['sx','sy']].median()\n",
    "                    mean_disp = df_cluster_full.groupby('cluster')[['dx','dy']].mean()\n",
    "                    \n",
    "                    cluster_norms = np.sqrt(mean_disp['dx']**2 + mean_disp['dy']**2)\n",
    "                    \n",
    "                    # Build arrow info for each cluster of interest\n",
    "                    cluster_arrows = []\n",
    "                    common_clusters = centroids.index.intersection(mean_disp.index)\n",
    "                    for clust in common_clusters:\n",
    "                        if clust in clusters_of_interest:\n",
    "                            cx, cy = centroids.loc[clust, ['sx','sy']]\n",
    "                            cdx, cdy = mean_disp.loc[clust, ['dx','dy']]\n",
    "                            cnorm = np.sqrt(cdx**2 + cdy**2)\n",
    "                            if cnorm > 0:\n",
    "                                cdx /= cnorm\n",
    "                                cdy /= cnorm\n",
    "                            length = cluster_norms.loc[clust]\n",
    "                            cdx *= length\n",
    "                            cdy *= length\n",
    "                            cluster_arrows.append((clust, cx, cy, cdx, cdy))\n",
    "                    \n",
    "                    timepoint_results[source_tp]['cluster_arrows'] = cluster_arrows\n",
    "    \n",
    "                    if len(cluster_arrows) > 0:\n",
    "                        # \"Aggregated\" arrow by summation\n",
    "                        source_median_x = source_cells['UMAP_2'].median()\n",
    "                        source_median_y = source_cells['UMAP_1'].median()\n",
    "                        total_dx = sum([arrow[3] for arrow in cluster_arrows])\n",
    "                        total_dy = sum([arrow[4] for arrow in cluster_arrows])\n",
    "                        timepoint_results[source_tp]['aggregated_arrow'] = (\n",
    "                            source_median_x, \n",
    "                            source_median_y, \n",
    "                            total_dx, \n",
    "                            total_dy\n",
    "                        )\n",
    "                    else:\n",
    "                        timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "    \n",
    "                    # Compute target cluster distributions for each COI (source cluster)\n",
    "                    for coi in clusters_of_interest:\n",
    "                        coi_mask = (source_cells['seurat_clusters'] == coi)\n",
    "                        if np.any(coi_mask):\n",
    "                            # Which target clusters do these COI cells map to?\n",
    "                            selected_target_indices = full_target_indices[coi_mask]\n",
    "                            selected_target_clusters = target_cells['seurat_clusters'].iloc[selected_target_indices].values\n",
    "                            unique_tclusters, counts = np.unique(selected_target_clusters, return_counts=True)\n",
    "                            total_count = counts.sum()\n",
    "                            if total_count > 0:\n",
    "                                fraction_dict = {int(tc): (ct / total_count) for tc, ct in zip(unique_tclusters, counts)}\n",
    "                            else:\n",
    "                                fraction_dict = {}\n",
    "                            distributions_coi[source_tp][cohort_name][coi] = fraction_dict\n",
    "                        else:\n",
    "                            distributions_coi[source_tp][cohort_name][coi] = {}\n",
    "                else:\n",
    "                    # If cluster info is missing\n",
    "                    timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "                    timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "                    for coi in clusters_of_interest:\n",
    "                        distributions_coi[source_tp][cohort_name][coi] = {}\n",
    "        else:\n",
    "            # Last timepoint has no \"next\" timepoint\n",
    "            timepoint_results[source_tp]['single_cell_displacements'] = np.array([])\n",
    "            timepoint_results[source_tp]['single_cell_arrow_lengths'] = np.array([])\n",
    "            timepoint_results[source_tp]['cluster_arrows'] = []\n",
    "            timepoint_results[source_tp]['aggregated_arrow'] = None\n",
    "            for coi in clusters_of_interest:\n",
    "                distributions_coi[source_tp][cohort_name][coi] = {}\n",
    "\n",
    "    # ---------------------------\n",
    "    # Generate PDFs of movement\n",
    "    # ---------------------------\n",
    "\n",
    "    # 1) Single-cell arrows\n",
    "    output_file_original = os.path.join(\n",
    "        output_folder, \n",
    "        f\"{subpop_name}_{cohort_name}_movement_plots_differential_arrow_lengths_equal_cells_using_PCs.pdf\"\n",
    "    )\n",
    "    with PdfPages(output_file_original) as pdf_original:\n",
    "        plot_tps = list(timepoint_results.keys())\n",
    "        num_timepoints = len(plot_tps)\n",
    "        fig_orig, axes_orig = plt.subplots(\n",
    "            1, \n",
    "            num_timepoints, \n",
    "            figsize=(4 * num_timepoints, 4), \n",
    "            sharex=True, \n",
    "            sharey=True\n",
    "        )\n",
    "        if num_timepoints == 1:\n",
    "            axes_orig = [axes_orig]\n",
    "        \n",
    "        for i, source_tp in enumerate(plot_tps):\n",
    "            ax = axes_orig[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            sampled_source_cells = res['sampled_source']\n",
    "            sampled_gray_cells = res['sampled_gray']\n",
    "            \n",
    "            gray_x = sampled_gray_cells['UMAP_2'].values\n",
    "            gray_y = sampled_gray_cells['UMAP_1'].values\n",
    "            source_x = sampled_source_cells['UMAP_2'].values\n",
    "            source_y = sampled_source_cells['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "            \n",
    "            single_cell_displacements = res['single_cell_displacements']\n",
    "            single_cell_arrow_lengths = res['single_cell_arrow_lengths']\n",
    "            \n",
    "            # ---------------------------\n",
    "            # Only draw arrows for COI\n",
    "            # ---------------------------\n",
    "            if len(single_cell_displacements) > 0:\n",
    "                # Coordinates for all sampled source cells\n",
    "                source_coords_sampled = sampled_source_cells[['UMAP_2','UMAP_1']].values\n",
    "                \n",
    "                # Compute the unit vectors for each displacement\n",
    "                sampled_norms = np.linalg.norm(single_cell_displacements, axis=1)\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    unit_vectors = single_cell_displacements / sampled_norms[:, np.newaxis]\n",
    "                    unit_vectors[~np.isfinite(unit_vectors)] = 0\n",
    "                scaled_vectors = unit_vectors * single_cell_arrow_lengths[:, np.newaxis]\n",
    "                \n",
    "                # If you have cluster info, filter by clusters_of_interest\n",
    "                if 'seurat_clusters' in sampled_source_cells.columns:\n",
    "                    for j in range(len(source_coords_sampled)):\n",
    "                        # Check if this cell is in one of your clusters of interest\n",
    "                        current_cluster = sampled_source_cells.iloc[j]['seurat_clusters']\n",
    "                        if current_cluster not in clusters_of_interest:\n",
    "                            # Skip drawing arrows if not in COI\n",
    "                            continue\n",
    "                        \n",
    "                        sx, sy = source_coords_sampled[j]\n",
    "                        dx, dy = scaled_vectors[j]\n",
    "                        ax.arrow(\n",
    "                            sx, sy, \n",
    "                            dx, dy,\n",
    "                            color='black', \n",
    "                            alpha=0.7, \n",
    "                            head_width=0.5, \n",
    "                            head_length=0.5,\n",
    "                            length_includes_head=True, \n",
    "                            linewidth=0.5\n",
    "                        )\n",
    "                else:\n",
    "                    # If no cluster column, you can either skip or draw all\n",
    "                    for j in range(len(source_coords_sampled)):\n",
    "                        sx, sy = source_coords_sampled[j]\n",
    "                        dx, dy = scaled_vectors[j]\n",
    "                        ax.arrow(\n",
    "                            sx, sy, \n",
    "                            dx, dy,\n",
    "                            color='black', \n",
    "                            alpha=0.7, \n",
    "                            head_width=0.5, \n",
    "                            head_length=0.5,\n",
    "                            length_includes_head=True, \n",
    "                            linewidth=0.5\n",
    "                        )\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_orig.suptitle(f\"{subpop_name} - {cohort_name}\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_original.savefig(fig_orig)\n",
    "        plt.close(fig_orig)\n",
    "    \n",
    "    # 2) Cluster-level arrows\n",
    "    output_file_cluster = os.path.join(\n",
    "        output_folder, \n",
    "        f\"{subpop_name}_{cohort_name}_movement_plots_aggregated_cluster_arrows_equal_cells_using_PCs.pdf\"\n",
    "    )\n",
    "    with PdfPages(output_file_cluster) as pdf_cluster:\n",
    "        plot_tps = list(timepoint_results.keys())\n",
    "        num_timepoints = len(plot_tps)\n",
    "        fig_clust, axes_clust = plt.subplots(\n",
    "            1, \n",
    "            num_timepoints, \n",
    "            figsize=(4 * num_timepoints, 4), \n",
    "            sharex=True, \n",
    "            sharey=True\n",
    "        )\n",
    "        if num_timepoints == 1:\n",
    "            axes_clust = [axes_clust]\n",
    "        \n",
    "        for i, source_tp in enumerate(plot_tps):\n",
    "            ax = axes_clust[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "            \n",
    "            gray_x = res['sampled_gray']['UMAP_2'].values\n",
    "            gray_y = res['sampled_gray']['UMAP_1'].values\n",
    "            source_x = res['sampled_source']['UMAP_2'].values\n",
    "            source_y = res['sampled_source']['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "            \n",
    "            cluster_arrows = res.get('cluster_arrows', [])\n",
    "            \n",
    "            source_cells = full_data[source_tp]['source']\n",
    "            if 'seurat_clusters' in source_cells.columns:\n",
    "                source_cluster_counts = source_cells['seurat_clusters'].value_counts()\n",
    "                total_source_cells_current = len(source_cells)\n",
    "            else:\n",
    "                source_cluster_counts = pd.Series()\n",
    "                total_source_cells_current = 1\n",
    "            \n",
    "            for (clust, cx, cy, cdx, cdy) in cluster_arrows:\n",
    "                arrow_color = cluster_colors_map.get(clust, 'black')\n",
    "                proportion = source_cluster_counts.get(clust, 0) / total_source_cells_current\n",
    "                line_width = 1.0 + 8.0 * proportion\n",
    "\n",
    "                # Outline in black for clarity\n",
    "                ax.arrow(\n",
    "                    cx, cy,\n",
    "                    cdx, cdy,\n",
    "                    color='black',\n",
    "                    alpha=1,\n",
    "                    head_width=0.2 + 0.5 * proportion,\n",
    "                    head_length=0.1 + 0.1 * proportion,\n",
    "                    length_includes_head=True,\n",
    "                    linewidth=line_width + 2\n",
    "                )\n",
    "                \n",
    "                # Main arrow in cluster color\n",
    "                ax.arrow(\n",
    "                    cx, cy,\n",
    "                    cdx, cdy,\n",
    "                    color=arrow_color,\n",
    "                    alpha=1,\n",
    "                    head_width=0.2 + 0.5 * proportion,\n",
    "                    head_length=0.1 + 0.1 * proportion,\n",
    "                    length_includes_head=True,\n",
    "                    linewidth=line_width\n",
    "                )\n",
    "            \n",
    "            ax.set_title(f\"Timepoint: {source_tp}\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        fig_clust.suptitle(f\"{subpop_name} - {cohort_name} (Cluster-Level Arrows)\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_cluster.savefig(fig_clust)\n",
    "        plt.close(fig_clust)\n",
    "\n",
    "    # 3) Single aggregated arrow\n",
    "    output_file_single_agg = os.path.join(\n",
    "        output_folder, \n",
    "        f\"{subpop_name}_{cohort_name}_movement_plots_aggregated_single_arrow_equal_cells_using_PCs.pdf\"\n",
    "    )\n",
    "    with PdfPages(output_file_single_agg) as pdf_agg:\n",
    "        plot_tps = list(timepoint_results.keys())\n",
    "        num_timepoints = len(plot_tps)\n",
    "        fig_agg, axes_agg = plt.subplots(\n",
    "            1,\n",
    "            num_timepoints,\n",
    "            figsize=(4 * num_timepoints, 4),\n",
    "            sharex=True,\n",
    "            sharey=True\n",
    "        )\n",
    "        if num_timepoints == 1:\n",
    "            axes_agg = [axes_agg]\n",
    "\n",
    "        for i, source_tp in enumerate(plot_tps):\n",
    "            ax = axes_agg[i]\n",
    "            res = timepoint_results[source_tp]\n",
    "\n",
    "            gray_x = res['sampled_gray']['UMAP_2'].values\n",
    "            gray_y = res['sampled_gray']['UMAP_1'].values\n",
    "            source_x = res['sampled_source']['UMAP_2'].values\n",
    "            source_y = res['sampled_source']['UMAP_1'].values\n",
    "            \n",
    "            ax.scatter(gray_x, gray_y, color='gray', s=5, alpha=0.5)\n",
    "            cohort_color = cohort_colors.get(cohort_name, 'red')\n",
    "            ax.scatter(source_x, source_y, color=cohort_color, s=5, alpha=1)\n",
    "\n",
    "            aggregated_arrow = res.get('aggregated_arrow', None)\n",
    "            if aggregated_arrow is not None:\n",
    "                global_cx, global_cy, total_dx, total_dy = aggregated_arrow\n",
    "                ax.arrow(\n",
    "                    global_cx, global_cy,\n",
    "                    total_dx, total_dy,\n",
    "                    color='black', \n",
    "                    alpha=0.9,\n",
    "                    head_width=0.5, \n",
    "                    head_length=0.5,\n",
    "                    length_includes_head=True, \n",
    "                    linewidth=2.0\n",
    "                )\n",
    "\n",
    "            ax.set_title(f\"Timepoint: {source_tp} (Single Aggregated Arrow)\")\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        fig_agg.suptitle(f\"{subpop_name} - {cohort_name} (Single Aggregated Arrow)\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        pdf_agg.savefig(fig_agg)\n",
    "        plt.close(fig_agg)\n",
    "    \n",
    "    # Return the distributions for stacked-bar plotting\n",
    "    return distributions_coi\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution Loop\n",
    "# ---------------------------\n",
    "for subpop_name in subpopulations:\n",
    "    # Prepare a nested dict so we can collect distributions\n",
    "    distributions_coi_all = {\n",
    "        tp: {\n",
    "            c: {coi: {} for coi in clusters_of_interest} \n",
    "            for c in cohorts\n",
    "        }\n",
    "        for tp in all_timepoints\n",
    "    }\n",
    "    \n",
    "    # Run for each cohort\n",
    "    for cohort_name in cohorts:\n",
    "        print(f\"Processing {subpop_name} - {cohort_name}\")\n",
    "        input_subfolder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "        if not os.path.exists(input_subfolder):\n",
    "            print(f\"Input subfolder does not exist: {input_subfolder}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        dist_coi = optimal_transport_visualization(subpop_name, cohort_name)\n",
    "        if dist_coi is not None:\n",
    "            for tp in dist_coi:\n",
    "                for c in dist_coi[tp]:\n",
    "                    for coi in dist_coi[tp][c]:\n",
    "                        distributions_coi_all[tp][c][coi] = dist_coi[tp][c][coi]\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 1) Group non-COI target clusters into \"Others\"\n",
    "    # --------------------------------------------------\n",
    "    for tp in distributions_coi_all:\n",
    "        for c in cohorts:\n",
    "            for coi in clusters_of_interest:\n",
    "                fraction_dict = distributions_coi_all[tp][c][coi]\n",
    "                if fraction_dict:\n",
    "                    new_dict = {}\n",
    "                    others_sum = 0.0\n",
    "                    for tclust, frac in fraction_dict.items():\n",
    "                        # If the target cluster is not in our \"source\" list, \n",
    "                        # put it in the \"Others\" bin\n",
    "                        if tclust not in clusters_of_interest:\n",
    "                            others_sum += frac\n",
    "                        else:\n",
    "                            new_dict[tclust] = frac\n",
    "                    if others_sum > 0:\n",
    "                        new_dict['Others'] = others_sum\n",
    "                    distributions_coi_all[tp][c][coi] = new_dict\n",
    "\n",
    "    # Gather all target clusters (including \"Others\")\n",
    "    all_target_clusters = []\n",
    "    for tp in distributions_coi_all:\n",
    "        for c in cohorts:\n",
    "            for coi in clusters_of_interest:\n",
    "                all_target_clusters.extend(distributions_coi_all[tp][c][coi].keys())\n",
    "    all_target_clusters = set(all_target_clusters)\n",
    "\n",
    "    # Handle \"Others\" separately for sorting\n",
    "    others_present = (\"Others\" in all_target_clusters)\n",
    "    if others_present:\n",
    "        all_target_clusters.remove(\"Others\")\n",
    "\n",
    "    # Now only numeric cluster IDs remain\n",
    "    # Sort numeric clusters in ascending order\n",
    "    all_target_clusters = sorted(all_target_clusters)\n",
    "\n",
    "    # Put \"Others\" at the end if present\n",
    "    if others_present:\n",
    "        all_target_clusters.append(\"Others\")\n",
    "\n",
    "    def get_celltype_name(clust_id):\n",
    "        if clust_id == 'Others':\n",
    "            return \"Others\"\n",
    "        return cluster_celltype_map.get(clust_id, f\"Cluster {clust_id}\")\n",
    "\n",
    "    def get_cluster_color(clust_id):\n",
    "        if clust_id == 'Others':\n",
    "            return 'gray'\n",
    "        return cluster_colors_map.get(clust_id, 'gray')\n",
    "\n",
    "    # ---------------------------\n",
    "    # Generate the stacked bar chart PDF\n",
    "    # ---------------------------\n",
    "    output_file_stacked = os.path.join(base_output_dir, f\"{subpop_name}_target_cluster_distribution_coi_with_stack_labels.pdf\")\n",
    "    with PdfPages(output_file_stacked) as pdf_dist:\n",
    "        fig, axes = plt.subplots(\n",
    "            len(clusters_of_interest), \n",
    "            len(all_timepoints), \n",
    "            figsize=(4 * len(all_timepoints), 3 * len(clusters_of_interest)), \n",
    "            sharex=False, \n",
    "            sharey=True\n",
    "        )\n",
    "        \n",
    "        # Handle shape for single row/col\n",
    "        if len(clusters_of_interest) == 1 and len(all_timepoints) == 1:\n",
    "            axes = np.array([[axes]])\n",
    "        elif len(clusters_of_interest) == 1:\n",
    "            axes = axes[np.newaxis, :]\n",
    "        elif len(all_timepoints) == 1:\n",
    "            axes = axes[:, np.newaxis]\n",
    "\n",
    "        # Prepare legend patches\n",
    "        legend_patches = []\n",
    "        for tc in all_target_clusters:\n",
    "            legend_patches.append(\n",
    "                plt.Rectangle((0,0),1,1,\n",
    "                              facecolor=get_cluster_color(tc),\n",
    "                              edgecolor='black',\n",
    "                              label=get_celltype_name(tc))\n",
    "            )\n",
    "        \n",
    "        # Plot bars\n",
    "        for row_i, coi in enumerate(clusters_of_interest):\n",
    "            for col_i, tp in enumerate(all_timepoints):\n",
    "                ax = axes[row_i, col_i]\n",
    "                bar_positions = np.arange(len(cohorts))\n",
    "                bottoms = np.zeros(len(cohorts))\n",
    "\n",
    "                # Retrieve distribution data for each cohort at (tp, coi)\n",
    "                cohort_distributions = [distributions_coi_all[tp][c][coi] for c in cohorts]\n",
    "\n",
    "                # Build up data for stacked bars: (target_cluster, [fractions for each cohort])\n",
    "                stack_data = []\n",
    "                for tc in all_target_clusters:\n",
    "                    h = [dist.get(tc, 0.0) for dist in cohort_distributions]\n",
    "                    stack_data.append((tc, h))\n",
    "\n",
    "                # Sort so biggest fraction (summed across cohorts) is at the bottom\n",
    "                stack_data.sort(key=lambda x: sum(x[1]), reverse=True)\n",
    "\n",
    "                # Plot\n",
    "                for (tc, heights) in stack_data:\n",
    "                    color = get_cluster_color(tc)\n",
    "                    ax.bar(\n",
    "                        bar_positions, \n",
    "                        heights, \n",
    "                        bottom=bottoms, \n",
    "                        color=color, \n",
    "                        edgecolor='black'\n",
    "                    )\n",
    "                    # Label each segment if fraction is big enough\n",
    "                    for idx, val in enumerate(heights):\n",
    "                        if val > 0.05:\n",
    "                            mid_y = bottoms[idx] + val / 2\n",
    "                            ax.text(\n",
    "                                bar_positions[idx], \n",
    "                                mid_y, \n",
    "                                get_celltype_name(tc),\n",
    "                                ha='center', \n",
    "                                va='center',\n",
    "                                fontsize=6, \n",
    "                                color='white'\n",
    "                            )\n",
    "                    bottoms += heights\n",
    "\n",
    "                if row_i == 0:\n",
    "                    ax.set_title(f\"{tp}\", fontsize=10)\n",
    "\n",
    "                ax.set_xticks(bar_positions)\n",
    "                ax.set_xticklabels(cohorts, rotation=45, ha='right', fontsize=8)\n",
    "                ax.set_ylim(0, 1)\n",
    "\n",
    "        fig.suptitle(f\"Distribution of Target Clusters per COI - {subpop_name}\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Adjust to make room for row labels & legend\n",
    "        fig.subplots_adjust(left=0.15, top=0.86, right=0.82)  # extra space on the right for legend\n",
    "\n",
    "        # Label rows with the celltype of the COI\n",
    "        n_rows = len(clusters_of_interest)\n",
    "        for row_i, coi in enumerate(clusters_of_interest):\n",
    "            y_pos = 0.86 - (row_i + 0.5)*(0.86-0.1)/n_rows\n",
    "            fig.text(0.05, y_pos, get_celltype_name(coi), va='center', ha='right', fontsize=10, color='black')\n",
    "\n",
    "        # Create a single-column legend on the right\n",
    "        fig.legend(\n",
    "            handles=legend_patches,\n",
    "            loc='upper left',\n",
    "            bbox_to_anchor=(0.84, 0.95),\n",
    "            ncol=1,\n",
    "            fontsize=8,\n",
    "            title=\"Target Celltypes\"\n",
    "        )\n",
    "\n",
    "        pdf_dist.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "print(\"Visualization generation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3454136-1179-4222-bf6e-9aa978a12e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Contingency Table of Target Clusters ===\n",
      "    Control  LongTerm\n",
      "1        17       357\n",
      "2         2       250\n",
      "3        23       445\n",
      "8         9       163\n",
      "10        9       160\n",
      "12        2        20\n",
      "14        5        71\n",
      "\n",
      "=== Chi-square Test Results ===\n",
      "Chi-square statistic = 10.807\n",
      "Degrees of freedom   = 6\n",
      "p-value             = 0.0945246\n",
      "Fail to reject H₀ → No evidence of a difference in target cluster distribution.\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ot  # for optimal transport\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration Parameters\n",
    "# ---------------------------\n",
    "\n",
    "base_input_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/final_nomenclature/coi\"\n",
    "base_output_dir = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/new_sequencing_data/ITT/Clonal_Tracking_Results/final_nomenclature/coi_central_memory_cd8\"\n",
    "\n",
    "# For illustration, we focus on a single subpopulation,\n",
    "# but you can loop over more if you wish.\n",
    "subpop_name = \"All_CD8\"\n",
    "\n",
    "# We only compare short_term vs long_term\n",
    "cohorts = [\"control\", \"long_term\"]\n",
    "\n",
    "# We are interested in timepoint C1 -> C2\n",
    "# but feel free to adapt for other timepoints\n",
    "source_tp = \"C1\"\n",
    "target_tp = \"C2\"\n",
    "\n",
    "# Clusters we are interested in as the \"source\" (COI):\n",
    "# For instance, cluster 12 = central memory CD8\n",
    "clusters_of_interest = [12]\n",
    "\n",
    "# This file should map cluster numbers to colors/celltypes (from your snippet).\n",
    "color_mapping_file = \"/project/dtran642_927/Data/Collaborator/DJ/scRNAseq/latest_big_sequencing_projects/MK/Publication_Material/T_Cell_cluster_colors.csv\"\n",
    "color_mapping_df = pd.read_csv(color_mapping_file)\n",
    "cluster_colors_map = dict(zip(color_mapping_df['Cluster'], color_mapping_df['Color']))\n",
    "cluster_celltype_map = dict(zip(color_mapping_df['Cluster'], color_mapping_df['Celltype']))\n",
    "\n",
    "# ---------------\n",
    "# Main function\n",
    "# ---------------\n",
    "def run_optimal_transport_and_get_target_clusters(\n",
    "    subpop_name,\n",
    "    cohort_name,\n",
    "    source_tp=\"C1\",\n",
    "    target_tp=\"C2\",\n",
    "    base_input_dir=base_input_dir\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs OT from timepoint `source_tp` to `target_tp` for the given subpop & cohort.\n",
    "    Returns a Pandas Series (indexed by source cell ID) of the *target cluster*\n",
    "    that each source cell maps to.\n",
    "    \n",
    "    If no data or no target cells exist, returns an empty Series.\n",
    "    \"\"\"\n",
    "    input_folder = os.path.join(base_input_dir, subpop_name, cohort_name)\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return pd.Series(dtype=int)\n",
    "    \n",
    "    # Path for timepoint folders\n",
    "    source_folder = os.path.join(input_folder, f\"Timepoint_{source_tp}\")\n",
    "    target_folder = os.path.join(input_folder, f\"Timepoint_{target_tp}\")\n",
    "\n",
    "    if not (os.path.exists(source_folder) and os.path.exists(target_folder)):\n",
    "        print(f\"Either {source_folder} or {target_folder} not found. Skipping.\")\n",
    "        return pd.Series(dtype=int)\n",
    "    \n",
    "    source_cells_file = os.path.join(source_folder, 'source_cells_new.csv')\n",
    "    gray_cells_file = os.path.join(source_folder, 'gray_cells_new.csv')\n",
    "    \n",
    "    target_cells_file = os.path.join(target_folder, 'source_cells_new.csv')\n",
    "    gray_cells_target_file = os.path.join(target_folder, 'gray_cells_new.csv')\n",
    "    \n",
    "    if not all([os.path.exists(x) for x in [source_cells_file, gray_cells_file, \n",
    "                                            target_cells_file, gray_cells_target_file]]):\n",
    "        print(\"Missing CSV files in the source/target timepoints.\")\n",
    "        return pd.Series(dtype=int)\n",
    "    \n",
    "    # Read data\n",
    "    source_cells = pd.read_csv(source_cells_file)\n",
    "    gray_cells_source = pd.read_csv(gray_cells_file)\n",
    "    target_cells = pd.read_csv(target_cells_file)\n",
    "    gray_cells_target = pd.read_csv(gray_cells_target_file)\n",
    "    \n",
    "    # Filter the source_cells to only keep those in the cluster(s) of interest\n",
    "    if 'seurat_clusters' not in source_cells.columns:\n",
    "        print(\"No 'seurat_clusters' column in source file.\")\n",
    "        return pd.Series(dtype=int)\n",
    "    source_cells = source_cells[source_cells['seurat_clusters'].isin(clusters_of_interest)]\n",
    "    \n",
    "    if source_cells.empty:\n",
    "        print(\"No source cells in clusters_of_interest. Skipping.\")\n",
    "        return pd.Series(dtype=int)\n",
    "    \n",
    "    # If target is empty, return\n",
    "    if target_cells.empty:\n",
    "        print(\"No target cells at timepoint. Skipping.\")\n",
    "        return pd.Series(dtype=int)\n",
    "\n",
    "    # Some lumps (based on your snippet)\n",
    "    # These lumps apparently combine certain cluster IDs\n",
    "    for cfile in [source_cells, target_cells]:\n",
    "        cfile['seurat_clusters'] = cfile['seurat_clusters'].replace({16:14,17:14})\n",
    "        cfile['seurat_clusters'] = cfile['seurat_clusters'].replace({9:6,18:6})\n",
    "    \n",
    "    # Identify columns for PCs\n",
    "    pc_cols = [c for c in source_cells.columns if c.startswith('PC_')]\n",
    "    if not pc_cols:\n",
    "        print(\"No PC_ columns found. Are you sure your data has them?\")\n",
    "        return pd.Series(dtype=int)\n",
    "    \n",
    "    full_source_coords_pc = source_cells[pc_cols].values\n",
    "    full_target_coords_pc = target_cells[pc_cols].values\n",
    "    \n",
    "    # UMAP coords\n",
    "    full_source_coords_umap = source_cells[['UMAP_2', 'UMAP_1']].values\n",
    "    full_target_coords_umap = target_cells[['UMAP_2', 'UMAP_1']].values\n",
    "    \n",
    "    # Prepare uniform distributions for OT\n",
    "    a = np.ones((full_source_coords_pc.shape[0],)) / full_source_coords_pc.shape[0]\n",
    "    b = np.ones((full_target_coords_pc.shape[0],)) / full_target_coords_pc.shape[0]\n",
    "    \n",
    "    cost_matrix = ot.dist(full_source_coords_pc, full_target_coords_pc, metric='euclidean')\n",
    "    \n",
    "    try:\n",
    "        transport_plan = ot.emd(a, b, cost_matrix, numItermax=100000)\n",
    "    except Exception as e:\n",
    "        print(f\"OT computation failed: {e}\")\n",
    "        return pd.Series(dtype=int)\n",
    "    \n",
    "    # For each source cell, find the single best-matching target (argmax in each row)\n",
    "    full_target_indices = np.argmax(transport_plan, axis=1)\n",
    "    \n",
    "    # Convert those indices into target clusters\n",
    "    target_clusters = target_cells['seurat_clusters'].iloc[full_target_indices].values\n",
    "    \n",
    "    # Return as a Pandas Series keyed by the source cell's index\n",
    "    out_series = pd.Series(data=target_clusters, index=source_cells.index)\n",
    "    return out_series\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Collect target‐cluster assignments for short‐term vs. long‐term\n",
    "# ------------------------------------------------------------\n",
    "short_term_targets = run_optimal_transport_and_get_target_clusters(\n",
    "    subpop_name,\n",
    "    \"control\",\n",
    "    source_tp,\n",
    "    target_tp\n",
    ")\n",
    "\n",
    "long_term_targets = run_optimal_transport_and_get_target_clusters(\n",
    "    subpop_name,\n",
    "    \"long_term\",\n",
    "    source_tp,\n",
    "    target_tp\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Build contingency table of target clusters\n",
    "# ------------------------------------------------------------\n",
    "# We just count how many times each target cluster appears in short vs. long\n",
    "short_counts = short_term_targets.value_counts().sort_index()\n",
    "long_counts  = long_term_targets.value_counts().sort_index()\n",
    "\n",
    "# Union of all target clusters that appear in either ST or LT\n",
    "all_clusters = sorted(set(short_counts.index).union(long_counts.index))\n",
    "\n",
    "# Make a table with row = cluster, columns = [ShortTerm, LongTerm]\n",
    "# e.g. in a 2D array\n",
    "rows = []\n",
    "for clust_id in all_clusters:\n",
    "    st_count = short_counts.get(clust_id, 0)\n",
    "    lt_count = long_counts.get(clust_id, 0)\n",
    "    rows.append([st_count, lt_count])\n",
    "\n",
    "contingency = np.array(rows)\n",
    "\n",
    "# Optionally put into a DataFrame with labels\n",
    "contingency_df = pd.DataFrame(\n",
    "    contingency,\n",
    "    index=all_clusters,\n",
    "    # columns=[\"ShortTerm\",\"LongTerm\"]\n",
    "    columns=[\"Control\",\"LongTerm\"]\n",
    ")\n",
    "\n",
    "print(\"\\n=== Contingency Table of Target Clusters ===\")\n",
    "print(contingency_df)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Run Chi-square test\n",
    "# ------------------------------------------------------------\n",
    "if contingency_df.shape[0] <= 1:\n",
    "    print(\"\\nNot enough distinct clusters to do a Chi-square test.\")\n",
    "else:\n",
    "    chi2_stat, p_value, dof, expected = chi2_contingency(contingency_df.values)\n",
    "\n",
    "    print(\"\\n=== Chi-square Test Results ===\")\n",
    "    print(f\"Chi-square statistic = {chi2_stat:.3f}\")\n",
    "    print(f\"Degrees of freedom   = {dof}\")\n",
    "    print(f\"p-value             = {p_value:.6g}\")\n",
    "\n",
    "    alpha = 0.05\n",
    "    if p_value < alpha:\n",
    "        print(\"Reject H₀ → The distribution of target clusters differs between Control and LT.\")\n",
    "    else:\n",
    "        print(\"Fail to reject H₀ → No evidence of a difference in target cluster distribution.\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71e1ae9-f7e5-42ad-9384-30ae42893a46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moscot_env",
   "language": "python",
   "name": "moscot_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
